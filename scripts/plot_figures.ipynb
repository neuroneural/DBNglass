{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb\n",
    "\n",
    "dataset_match = {\n",
    "    \"fbirn\": \"FBIRN ICA\",\n",
    "    \"fbirn_roi\": \"FBIRN Schaefer 200 ROI\",\n",
    "    \"bsnip\": \"BSNIP ICA\",\n",
    "    \"cobre\": \"COBRE ICA\",\n",
    "\n",
    "    \"abide\": \"ABIDE1 ICA 569 subjects\",\n",
    "    \"abide_869\": \"ABIDE1 ICA\",\n",
    "    \"abide_roi\": \"ABIDE1 Schaefer 200 ROI\",\n",
    "\n",
    "    \"oasis\": \"OASIS3 ICA\",\n",
    "    \"adni\": \"ADNI ICA\",\n",
    "\n",
    "    \"hcp\": \"HCP ICA\",\n",
    "    \"hcp_roi\": \"HCP Schaefer 200 ROI\",\n",
    "    \"hcp_non_mni\": \"HCP Deskian/Killiany ROI Old\",\n",
    "    \"hcp_non_mni_2\": \"HCP Deskian/Killiany ROI\",\n",
    "    \"hcp_mni\": \"HCP Deskian/Killiany ROI MNI Oldest\",\n",
    "    \"hcp_mni_2\": \"HCP Deskian/Killiany ROI MNI Old\",\n",
    "    \"hcp_mni_3\": \"HCP Deskian/Killiany ROI MNI\",\n",
    "\n",
    "    \"ukb\": \"UK Biobank (Sex) ICA\",\n",
    "    \"ukb_age_bins\": \"UK Biobank (Age-Sex) ICA\",\n",
    "\n",
    "    \"time_fbirn\": \"FBIRN (Time Direction) ICA\",\n",
    "\n",
    "    \"fbirn_bsnip\": \"FxB\",\n",
    "    \"fbirn_cobre\": \"FxC\",\n",
    "\n",
    "    \"bsnip_fbirn\": \"BxF\",\n",
    "    \"bsnip_cobre\": \"BxC\",\n",
    "\n",
    "    \"cobre_fbirn\": \"CxF\",\n",
    "    \"cobre_bsnip\": \"CxB\",\n",
    "\n",
    "    \"oasis_adni\": \"OxA\",\n",
    "    \"adni_oasis\": \"AxO\",\n",
    "}\n",
    "\n",
    "model_match = {\n",
    "    \"mlp\": \"MLP\",\n",
    "    \"lstm\": \"LSTM\",\n",
    "    \"mean_lstm\": r\"$\\mu$LSTM\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"mean_transformer\": r\"$\\mu$Transformer\",\n",
    "    \"milc\": \"MILC\",\n",
    "    \"stdim\": \"ST-DIM\",\n",
    "    \"dice\": \"DICE\",\n",
    "    \"bnt\": \"BNT\",\n",
    "    \"fbnetgen\": \"FBNetGen\",\n",
    "    \"brainnetcnn\": \"BrainNetCNN\",\n",
    "    \"lr\": \"LR\",\n",
    "}\n",
    "\n",
    "def fix_run(run, dataset):\n",
    "    for _, row in run.history().iterrows():\n",
    "        if f\"{dataset}score\" in row and f\"{dataset}accuracy\" in row and \"training_time\" in row and f\"{dataset}average_time\" in row:\n",
    "            auc = row[f\"{dataset}score\"]\n",
    "            acc = row[f\"{dataset}accuracy\"]\n",
    "            train_t = row[\"training_time\"]\n",
    "            inf_t= row[f\"{dataset}average_time\"]\n",
    "            break\n",
    "    \n",
    "    run.summary[f\"{dataset}score\"] = auc\n",
    "    run.summary[f\"{dataset}accuracy\"] = acc\n",
    "    run.summary[\"training_time\"] = train_t\n",
    "    run.summary[f\"{dataset}average_time\"] = inf_t\n",
    "\n",
    "    run.update()\n",
    "\n",
    "    return auc, acc, train_t, inf_t\n",
    "\n",
    "def load_run(proj_name, dataset):\n",
    "    if dataset is not None:\n",
    "        dataset += \"_\"\n",
    "    else:\n",
    "        dataset += \"test_\"\n",
    "\n",
    "    api = wandb.Api(timeout=19)\n",
    "    # Project is specified by <entity/project-name>\n",
    "    runs = api.runs(f\"pavalipopov/{proj_name}\")\n",
    "\n",
    "    summary_list = []\n",
    "    runs_list = []\n",
    "    config_list = []\n",
    "    for i, run in enumerate(runs): \n",
    "        # print(f\"Run {i}\")\n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files \n",
    "        summary_list.append(run.summary._json_dict)\n",
    "        config_list.append(run.config)\n",
    "        runs_list.append(run)\n",
    "\n",
    "\n",
    "    AUC_score = []\n",
    "    accuracy = []\n",
    "    train_time = []\n",
    "    inference_time = []\n",
    "    params = []\n",
    "    shuffling = []\n",
    "\n",
    "    for i, summary in enumerate(summary_list):\n",
    "        if f\"{dataset}score\" in summary and f\"{dataset}accuracy\" in summary and \"training_time\" in summary and f\"{dataset}average_time\" in summary:\n",
    "            AUC_score.append(summary[f\"{dataset}score\"])\n",
    "            accuracy.append(summary[f\"{dataset}accuracy\"])\n",
    "            train_time.append(summary[\"training_time\"])\n",
    "            inference_time.append(summary[f\"{dataset}average_time\"])\n",
    "        else:\n",
    "            print(f\"Run {i} summary is broken, fixing it\")\n",
    "            auc, acc, train_t, inf_t = fix_run(runs_list[i], dataset)\n",
    "            AUC_score.append(auc)\n",
    "            accuracy.append(acc)\n",
    "            train_time.append(train_t)\n",
    "            inference_time.append(inf_t)\n",
    "\n",
    "        if \"params\" in summary:\n",
    "            params.append(summary[\"params\"])\n",
    "        else:\n",
    "            params.append(None)\n",
    "\n",
    "        if \"permute\" in config_list[i][\"general\"]:\n",
    "            if config_list[i][\"general\"][\"permute\"] == False:\n",
    "                shuffling.append(\"None\")\n",
    "            else:\n",
    "                shuffling.append(config_list[i][\"general\"][\"permute\"])\n",
    "        else:\n",
    "            shuffling.append(\"None\")\n",
    "    \n",
    "    return AUC_score, accuracy, train_time, inference_time, params, shuffling\n",
    "\n",
    "def load_metrics(paths_dict, ds_dict, model_dict):\n",
    "    data_list = []\n",
    "\n",
    "    for model_name in paths_dict:\n",
    "        print(model_name)\n",
    "        for dataset_name in paths_dict[model_name]:\n",
    "            print(\"\\t \", dataset_name)\n",
    "            \n",
    "            if len(paths_dict[model_name][dataset_name]) == 2:\n",
    "                path = paths_dict[model_name][dataset_name][0]\n",
    "                dataset = paths_dict[model_name][dataset_name][1]\n",
    "            elif len(paths_dict[model_name][dataset_name]) == 1:\n",
    "                path = paths_dict[model_name][dataset_name][0]\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            print(\"\\t\\t sub_dataset:\", dataset)\n",
    "\n",
    "            AUC_score, accuracy, train_time, inference_time, params, shuffling = load_run(path, dataset)\n",
    "            \n",
    "            if dataset != \"test\":\n",
    "                dataset = dataset_name + \"_\" + dataset\n",
    "            else:\n",
    "                dataset = dataset_name\n",
    "            length = len(AUC_score)\n",
    "\n",
    "            data_list.append(\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\": [model_dict[model_name]]*length,\n",
    "                        \"Dataset\": [ds_dict[dataset]]*length,\n",
    "                        \"AUROC\": AUC_score,\n",
    "                        \"Accuracy\": accuracy,\n",
    "                        \"Train time\": train_time,\n",
    "                        \"Inference time\": inference_time,\n",
    "                        \"Trainable params\": params,\n",
    "                        \"Shuffling\": shuffling,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = {\n",
    "    \"mlp\": {\n",
    "        \"hcp\": (\"all_mni-exp-mlp_defHP-hcp\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mlp_defHP-hcp_roi\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mlp_defHP-hcp_non_mni_2\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mlp_defHP-hcp_mni_3\", ),\n",
    "    },\n",
    "    \"lstm\": {\n",
    "        \"hcp\": (\"all_mni-exp-lstm_defHP-hcp\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-lstm_defHP-hcp_roi\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-lstm_defHP-hcp_non_mni_2\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-lstm_defHP-hcp_mni_3\", ),\n",
    "    },\n",
    "    \"mean_lstm\": {\n",
    "        \"hcp\": (\"all_mni-exp-mean_lstm_defHP-hcp\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mean_lstm_defHP-hcp_roi\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mean_lstm_defHP-hcp_non_mni_2\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mean_lstm_defHP-hcp_mni_3\", ),\n",
    "    },\n",
    "    \"transformer\": {\n",
    "        \"hcp\": (\"all_mni-exp-transformer_defHP-hcp\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-transformer_defHP-hcp_roi\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-transformer_defHP-hcp_non_mni_2\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-transformer_defHP-hcp_mni_3\", ),\n",
    "    },\n",
    "    \"mean_transformer\": {\n",
    "        \"hcp\": (\"all_mni-exp-mean_transformer_defHP-hcp\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mean_transformer_defHP-hcp_roi\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mean_transformer_defHP-hcp_non_mni_2\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mean_transformer_defHP-hcp_mni_3\", ),\n",
    "    },\n",
    "}\n",
    "\n",
    "projects_single = {\n",
    "    \"mlp\": {\n",
    "        \"hcp\": (\"all_mni-exp-mlp_defHP-hcp-perm_Single\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mlp_defHP-hcp_roi-perm_Single\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mlp_defHP-hcp_non_mni_2-perm_Single\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mlp_defHP-hcp_mni_3-perm_Single\", ),\n",
    "    },\n",
    "    \"lstm\": {\n",
    "        \"hcp\": (\"all_mni-exp-lstm_defHP-hcp-perm_Single\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-lstm_defHP-hcp_roi-perm_Single\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-lstm_defHP-hcp_non_mni_2-perm_Single\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-lstm_defHP-hcp_mni_3-perm_Single\", ),\n",
    "    },\n",
    "    \"mean_lstm\": {\n",
    "        \"hcp\": (\"all_mni-exp-mean_lstm_defHP-hcp-perm_Single\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mean_lstm_defHP-hcp_roi-perm_Single\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mean_lstm_defHP-hcp_non_mni_2-perm_Single\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mean_lstm_defHP-hcp_mni_3-perm_Single\", ),\n",
    "    },\n",
    "    \"transformer\": {\n",
    "        \"hcp\": (\"all_mni-exp-transformer_defHP-hcp-perm_Single\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-transformer_defHP-hcp_roi-perm_Single\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-transformer_defHP-hcp_non_mni_2-perm_Single\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-transformer_defHP-hcp_mni_3-perm_Single\", ),\n",
    "    },\n",
    "    \"mean_transformer\": {\n",
    "        \"hcp\": (\"all_mni-exp-mean_transformer_defHP-hcp-perm_Single\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mean_transformer_defHP-hcp_roi-perm_Single\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mean_transformer_defHP-hcp_non_mni_2-perm_Single\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mean_transformer_defHP-hcp_mni_3-perm_Single\", ),\n",
    "    },\n",
    "}\n",
    "\n",
    "projects_multiple = {\n",
    "    \"mlp\": {\n",
    "        \"hcp\": (\"all_mni-exp-mlp_defHP-hcp-perm_Multiple\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mlp_defHP-hcp_roi-perm_Multiple\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mlp_defHP-hcp_non_mni_2-perm_Multiple\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mlp_defHP-hcp_mni_3-perm_Multiple\", ),\n",
    "    },\n",
    "    \"lstm\": {\n",
    "        \"hcp\": (\"all_mni-exp-lstm_defHP-hcp-perm_Multiple\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-lstm_defHP-hcp_roi-perm_Multiple\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-lstm_defHP-hcp_non_mni_2-perm_Multiple\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-lstm_defHP-hcp_mni_3-perm_Multiple\", ),\n",
    "    },\n",
    "    \"mean_lstm\": {\n",
    "        \"hcp\": (\"all_mni-exp-mean_lstm_defHP-hcp-perm_Multiple\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mean_lstm_defHP-hcp_roi-perm_Multiple\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mean_lstm_defHP-hcp_non_mni_2-perm_Multiple\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mean_lstm_defHP-hcp_mni_3-perm_Multiple\", ),\n",
    "    },\n",
    "    \"transformer\": {\n",
    "        \"hcp\": (\"all_mni-exp-transformer_defHP-hcp-perm_Multiple\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-transformer_defHP-hcp_roi-perm_Multiple\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-transformer_defHP-hcp_non_mni_2-perm_Multiple\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-transformer_defHP-hcp_mni_3-perm_Multiple\", ),\n",
    "    },\n",
    "    \"mean_transformer\": {\n",
    "        \"hcp\": (\"all_mni-exp-mean_transformer_defHP-hcp-perm_Multiple\", ),\n",
    "        \"hcp_roi\": (\"all_mni-exp-mean_transformer_defHP-hcp_roi-perm_Multiple\", ),\n",
    "        \"hcp_non_mni_2\": (\"all_mni-exp-mean_transformer_defHP-hcp_non_mni_2-perm_Multiple\", ),\n",
    "        \"hcp_mni_3\": (\"all_mni-exp-mean_transformer_defHP-hcp_mni_3-perm_Multiple\", ),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = load_metrics(projects, dataset_match, model_match)\n",
    "data_s = load_metrics(projects_single, dataset_match, model_match)\n",
    "data_m = load_metrics(projects_multiple, dataset_match, model_match)\n",
    "\n",
    "data = pd.concat([data_n, data_s, data_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_metrics(projects, dataset_match, model_match)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\", \n",
    "    font_scale = 1,\n",
    ")\n",
    "\n",
    "plot_data = data.query(\n",
    "        \"Shuffling == 'None'\")\n",
    "\n",
    "models = [\"MLP\", \n",
    "           'LSTM', \n",
    "           r'$\\mu$LSTM', \n",
    "           'Transformer', \n",
    "           r'$\\mu$Transformer', \n",
    "           \"MILC\", \n",
    "           \"ST-DIM\", \n",
    "           \"DICE\", \n",
    "           \"BNT\", \n",
    "           \"FBNetGen\", \n",
    "           \"BrainNetCNN\", \n",
    "           \"LR\"\n",
    "           ]\n",
    "datasets = [\"HCP ICA\",\"HCP Schaefer 200 ROI\",\"HCP Deskian/Killiany ROI\",\"HCP Deskian/Killiany ROI MNI\",\n",
    "           ]\n",
    "palette = { item: plt.cm.tab20(i) for i, item in enumerate(datasets)}\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Model\", \n",
    "    y=\"AUROC\",\n",
    "    hue=\"Dataset\",\n",
    "    data=plot_data,\n",
    "    palette=palette,\n",
    "    showfliers = False\n",
    ")\n",
    "\n",
    "ax.set(xlabel=None)\n",
    "ax.axhline(0.5)\n",
    "\n",
    "plt.yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "plt.legend(bbox_to_anchor=(1.01, 1.0), loc='upper left', borderaxespad=0)\n",
    "# plt.title(label=\"ICA - Neuromark\")\n",
    "\n",
    "plt.ylim(0.4, 1.1)\n",
    "# ax.set_xlim(xmin=-0.5, xmax=8.5)\n",
    "\n",
    "# ax.patches[11].set_hatch(\"///\")\n",
    "# ax.patches[13].set_hatch(\"///\")\n",
    "# for i, box in enumerate(ax.patches):\n",
    "#     # Set a different hatch for each bar\n",
    "#     if i > 7:\n",
    "#         if i % 9 == 5 or i % 9 == 6 or i % 9 == 7 or i % 9 == 8:\n",
    "#             box.set_hatch(\"///\")\n",
    "\n",
    "# ax.legend_.legendHandles[5].set_hatch(\"///\")\n",
    "# ax.legend_.legendHandles[6].set_hatch(\"///\")\n",
    "# ax.legend_.legendHandles[7].set_hatch(\"///\")\n",
    "# ax.legend_.legendHandles[8].set_hatch(\"///\")\n",
    "\n",
    "# sns.set_theme(\n",
    "#     font_scale = 1,\n",
    "# )\n",
    "\n",
    "# add_stat_annotation(ax, data=ica_data, x=\"Dataset\", y=\"AUC\", hue=\"Model\",\n",
    "#                     box_pairs=[\n",
    "#                         ((\"FBIRN\", \"MLP\"), (\"FBIRN\", \"BNT\")), ((\"FBIRN\", \"LR\"), (\"FBIRN\", \"BNT\")), ((\"FBIRN\", \"LR\"), (\"FBIRN\", \"MLP\")),\n",
    "#                         ((\"BSNIP\", \"MLP\"), (\"BSNIP\", \"BNT\")), ((\"BSNIP\", \"LR\"), (\"BSNIP\", \"BNT\")), ((\"BSNIP\", \"LR\"), (\"BSNIP\", \"MLP\")),\n",
    "#                         ((\"COBRE\", \"MLP\"), (\"COBRE\", \"BNT\")), ((\"COBRE\", \"LR\"), (\"COBRE\", \"BNT\")), ((\"COBRE\", \"LR\"), (\"COBRE\", \"MLP\")),\n",
    "#                         ((\"ABIDE1\", \"MLP\"), (\"ABIDE1\", \"BNT\")), ((\"ABIDE1\", \"LR\"), (\"ABIDE1\", \"BNT\")), ((\"ABIDE1\", \"LR\"), (\"ABIDE1\", \"MLP\")),\n",
    "#                         ((\"OASIS3\", \"MLP\"), (\"OASIS3\", \"BNT\")), ((\"OASIS3\", \"LR\"), (\"OASIS3\", \"BNT\")), ((\"OASIS3\", \"LR\"), (\"OASIS3\", \"MLP\")),\n",
    "#                         ((\"ADNI\", \"MLP\"), (\"ADNI\", \"BNT\")), ((\"ADNI\", \"LR\"), (\"ADNI\", \"BNT\")), ((\"ADNI\", \"LR\"), (\"ADNI\", \"MLP\")),\n",
    "#                         ((\"HCP\", \"MLP\"), (\"HCP\", \"BNT\")), ((\"HCP\", \"LR\"), (\"HCP\", \"BNT\")), ((\"HCP\", \"LR\"), (\"HCP\", \"MLP\")),\n",
    "#                         ((\"UKB-S\", \"MLP\"), (\"UKB-S\", \"BNT\")), ((\"UKB-S\", \"LR\"), (\"UKB-S\", \"BNT\")), ((\"UKB-S\", \"LR\"), (\"UKB-S\", \"MLP\")),\n",
    "#                         ((\"UKB-SA\", \"MLP\"), (\"UKB-SA\", \"BNT\")), ((\"UKB-SA\", \"LR\"), (\"UKB-SA\", \"BNT\")), ((\"UKB-SA\", \"LR\"), (\"UKB-SA\", \"MLP\")),\n",
    "#                     ],\n",
    "#                     test='Wilcoxon', text_format='star', loc='inside', verbose=1, text_offset=-4, \n",
    "#                     pvalue_thresholds = [[0.05, \"*\"], [1, u'\\N{DEGREE SIGN}']],\n",
    "#                     line_offset=0.01, line_offset_to_box=0.01, line_height=0.01, fontsize=24)\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig(\n",
    "#     \"0.svg\",\n",
    "#     format=\"svg\",\n",
    "#     # dpi=300,\n",
    "#     bbox_inches='tight',\n",
    "# )\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\", \n",
    "    font_scale = 1.5,\n",
    ")\n",
    "\n",
    "# datasets = [\"FBIRN ICA\", \"FBIRN Schaefer 200\", \"BSNIP ICA\", \"COBRE ICA\", \"ABIDE1 ICA\", \"OASIS3 ICA\", \"ADNI ICA\", \"HCP ICA\", \"F-time\"]\n",
    "datasets = [\"HCP ICA\", \"HCP Schaefer 200 ROI\", \"HCP Deskian/Killiany ROI\", \"HCP Deskian/Killiany ROI MNI\"]\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10, 5*len(datasets)))\n",
    "# fig = plt.figure(constrained_layout=True)\n",
    "axs = fig.subplots(len(datasets), 1)\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\", \n",
    "    font_scale = 2, \n",
    "    rc={'figure.figsize':(16,5)}\n",
    ")\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    plot_data = data.query(\n",
    "        \"Dataset == @dataset\")\n",
    "\n",
    "    sns.boxplot(\n",
    "        x=\"Model\", \n",
    "        y=\"AUROC\",\n",
    "        hue=\"Shuffling\",\n",
    "        data=plot_data,\n",
    "        showfliers = False,\n",
    "        ax = axs[i]\n",
    "    )\n",
    "\n",
    "    # subfigs[i].axhline(0.5)\n",
    "    axs[i].set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    axs[i].legend(bbox_to_anchor=(1.01, 1.0), loc='upper left', borderaxespad=0, title=\"Shuffling\")\n",
    "\n",
    "    axs[i].set_title(dataset)\n",
    "\n",
    "    axs[i].set_ylim(0.4, 1.1)\n",
    "\n",
    "    # try:\n",
    "    #     add_stat_annotation(axs[i], data=plot_data, x=\"Model\", y=\"AUC\", hue=\"Permuted\",\n",
    "    #             box_pairs=[\n",
    "    #                 ((\"MLP\", True), (\"MLP\", False)),\n",
    "    #                 ((\"LSTM\", True), (\"LSTM\", False)),\n",
    "    #                 ((\"Mean LSTM\", True), (\"Mean LSTM\", False)),\n",
    "    #                 ((\"TF\", True), (\"TF\", False)),\n",
    "    #                 ((\"Mean TF\", True), (\"Mean TF\", False)),\n",
    "    #             ],\n",
    "    #             test='Wilcoxon', text_format='star', loc='inside', verbose=1, text_offset=-4, \n",
    "    #             pvalue_thresholds = [[0.05, \"*\"], [1, u'\\N{DEGREE SIGN}']],\n",
    "    #             line_offset=0.01, line_offset_to_box=0.02, line_height=0.01, fontsize=13)\n",
    "    # except ValueError:\n",
    "    #     pass\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\n",
    "    \"./shiffling.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches='tight',\n",
    "    transparent=False,\n",
    ")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
