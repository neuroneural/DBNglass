{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel, Saliency, visualization as viz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from src.data import data_factory, data_postfactory\n",
    "from src.dataloader import dataloader_factory\n",
    "from src.model import model_config_factory, model_factory\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# colors = [(0.0, 'darkviolet'), (0.5, 'black'), (0.99, 'darkorange'), (1.0, 'white')]\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "corr_colors = [(0.0, 'blue'), (0.5, 'black'), (0.99, 'red'), (1.0, 'white')]\n",
    "abs_corr_colors = [(0.0, 'black'), (0.99, 'red'), (1.0, 'white')]\n",
    "corr_cmap = LinearSegmentedColormap.from_list('custom_colormap', corr_colors, N=256)\n",
    "abs_corr_cmap = LinearSegmentedColormap.from_list('custom_colormap', abs_corr_colors, N=256)\n",
    "\n",
    "import os, sys\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cmap = \"inferno\"\n",
    "abs_corr_cmap = \"inferno\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_data(path):\n",
    "    cfg = OmegaConf.load(path+'/general_config.yaml')\n",
    "\n",
    "    df = pd.read_csv(path+'/runs.csv')\n",
    "    best_idx = df[\"test_score\"].idxmax()\n",
    "    k = best_idx // cfg.mode.n_trials\n",
    "    trial = best_idx - cfg.mode.n_trials * k\n",
    "    print(\"k \", k)\n",
    "    print(\"trial \", trial)\n",
    "\n",
    "    original_data = data_factory(cfg)\n",
    "    model_cfg = model_config_factory(cfg, k)\n",
    "    data = data_postfactory(\n",
    "            cfg,\n",
    "            model_cfg,\n",
    "            original_data,\n",
    "        )\n",
    "    dataloaders = dataloader_factory(cfg, data, k=k, trial=trial)\n",
    "\n",
    "    model = model_factory(cfg, model_cfg)\n",
    "\n",
    "    model_logpath = path+f\"/k_{k:02d}/trial_{trial:04d}/best_model.pt\"\n",
    "    checkpoint = torch.load(\n",
    "        model_logpath, map_location=lambda storage, loc: storage\n",
    "    )\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    return model, dataloaders, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuromark = pd.read_csv(\"/Users/pavelpopov/mlp_nn/assets/data/neuromark_regions.csv\")\n",
    "neuromark[\"Index\"]=neuromark[\"Index\"]-1\n",
    "neuromark = {a: (b, c, d, e, f) for a, b, c, d, e, f in zip(neuromark[\"Index\"], neuromark[\"Name\"], neuromark[\"Network\"], neuromark[\"X\"], neuromark[\"Y\"], neuromark[\"Z\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrospectorV2:\n",
    "    \"\"\"Basic introspector\"\"\"\n",
    "\n",
    "    def __init__(self, path, methods, save_path) -> None:\n",
    "        self.path = path\n",
    "        self.methods = methods\n",
    "        self.save_path = save_path\n",
    "\n",
    "        if \"saliency\" in self.methods:\n",
    "            os.makedirs(f\"{self.save_path}saliency\", exist_ok=True)\n",
    "        if \"ig\" in self.methods:\n",
    "            os.makedirs(f\"{self.save_path}ig\", exist_ok=True)\n",
    "        if \"ignt\" in self.methods:\n",
    "            os.makedirs(f\"{self.save_path}ignt\", exist_ok=True)\n",
    "        \n",
    "        self.compute_data(path)\n",
    "        for ds_key in self.features:\n",
    "            for method in methods:\n",
    "                for class_idx in range(2):\n",
    "                    np.save(f\"{self.save_path}{method}/{ds_key}_features_{class_idx}.npy\", self.features[ds_key][class_idx].cpu().detach().numpy())\n",
    "                    np.save(f\"{self.save_path}{method}/{ds_key}_grads_{class_idx}.npy\", self.grads[ds_key][method][class_idx])\n",
    "    \n",
    "    def compute_data(self, path, load_only_test = True):\n",
    "        cfg = OmegaConf.load(path+'/general_config.yaml')\n",
    "\n",
    "        n_splits = cfg.mode.n_splits\n",
    "        n_trials = cfg.mode.n_trials\n",
    "\n",
    "        self.grads = {}\n",
    "        self.features, self.timeseries, self.smooth_timeseries, self.predictions = {}, {}, {}, {}\n",
    "\n",
    "        # datasets = [\"test\"]\n",
    "        datasets = [\"test\", \"train\", \"valid\"]\n",
    "        if (\n",
    "            \"compatible_datasets\" in cfg.dataset\n",
    "            and cfg.dataset.compatible_datasets is not None\n",
    "        ):\n",
    "            datasets += cfg.dataset.compatible_datasets\n",
    "\n",
    "        for ds_key in datasets:\n",
    "            self.features[ds_key], self.timeseries[ds_key], self.smooth_timeseries[ds_key], self.predictions[ds_key] = None, None, None, None\n",
    "            self.grads[ds_key] = {key: [] for key in self.methods}\n",
    "            \n",
    "        # for k in range(n_splits):\n",
    "        for k in range(1):\n",
    "            # for trial in range(n_trials):\n",
    "            for trial in range(1):\n",
    "                model, data, _ = self.load_model_and_data(path, cfg, k, trial)\n",
    "\n",
    "                for ds_key in data:\n",
    "                    # if ds_key in [\"train\", \"valid\"]:\n",
    "                    #     continue\n",
    "                    \n",
    "                    dataset = data[ds_key].dataset\n",
    "                    features = [sample[0] for sample in dataset]\n",
    "                    labels = [sample[1] for sample in dataset]\n",
    "\n",
    "                    features = torch.stack(features)\n",
    "                    labels = torch.stack(labels)\n",
    "                    targets = torch.unique(labels)\n",
    "                    self.n_classes = n_classes = targets.shape[0]\n",
    "\n",
    "                    class_features, timeseries, smooth_timeseries, predictions = (\n",
    "                        [None]*n_classes, \n",
    "                        [None]*n_classes, \n",
    "                        [None]*n_classes, \n",
    "                        [None]*n_classes,\n",
    "                    )\n",
    "\n",
    "                    window_size = 10\n",
    "                    for class_idx in range(n_classes):\n",
    "                        filter_array = labels == class_idx\n",
    "                        class_features[class_idx] = features[filter_array]\n",
    "                        class_features[class_idx].requires_grad = False\n",
    "\n",
    "                    #     timeseries_raw, predictions[class_idx] = model(class_features[class_idx], introspection=True)\n",
    "                    #     timeseries_raw = timeseries_raw.cpu().detach().numpy()\n",
    "                    #     predictions[class_idx] = predictions[class_idx].cpu().detach().numpy()\n",
    "                        \n",
    "                    #     # timeseries_raw was calculated for the data related to class_idx;\n",
    "                    #     # yet it has `n_classes` class prediction time series in 3rd dimension\n",
    "                    #     # so timeseries[class_idx] is a list of length n_classes, each element of which\n",
    "                    #     # represents the class prediction time series\n",
    "                    #     timeseries[class_idx] = [timeseries_raw[:, :, j] for j in range(n_classes)]\n",
    "\n",
    "                    #     smooth_timeseries[class_idx] = deepcopy(timeseries[class_idx])\n",
    "                    #     for j in range(n_classes):\n",
    "                    #         for subject in range(smooth_timeseries[class_idx][j].shape[0]):\n",
    "                    #             smooth_timeseries[class_idx][j][subject] = np.convolve(smooth_timeseries[class_idx][j][subject], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "                    if self.features[ds_key] is None:\n",
    "                        self.features[ds_key] = class_features\n",
    "                        # self.timeseries[ds_key] = timeseries\n",
    "                        # self.smooth_timeseries[ds_key] = smooth_timeseries\n",
    "                        # self.predictions[ds_key] = predictions\n",
    "                    else:\n",
    "                        for class_idx in range(n_classes):\n",
    "                            self.features[ds_key][class_idx] = torch.concatenate((self.features[ds_key][class_idx], class_features[class_idx]), axis=0)\n",
    "                            # self.timeseries[ds_key][class_idx] = np.concatenate((self.timeseries[ds_key][class_idx], timeseries[class_idx]), axis=1)\n",
    "                            # self.smooth_timeseries[ds_key][class_idx] = np.concatenate((self.smooth_timeseries[ds_key][class_idx], smooth_timeseries[class_idx]), axis=1)\n",
    "                            # self.predictions[ds_key][class_idx] = np.concatenate((self.predictions[ds_key][class_idx], predictions[class_idx]), axis=0)\n",
    "\n",
    "                    # print(f\"self.features[{ds_key}][0]: {self.features[ds_key][0].shape}\")\n",
    "\n",
    "                    grads = {key: [] for key in self.methods}\n",
    "                    for method in self.methods:\n",
    "                        for j in range(n_classes):\n",
    "                            class_features[j].requires_grad = True\n",
    "                            grads[method].append(self.get_grads(model, method, class_features[j], j).cpu().detach().numpy())\n",
    "\n",
    "                        if len(self.grads[ds_key][method]) == 0:\n",
    "                            self.grads[ds_key][method] += grads[method]\n",
    "                        else:\n",
    "                            for j in range(n_classes):\n",
    "                                self.grads[ds_key][method][j] = np.concatenate((self.grads[ds_key][method][j], grads[method][j]), axis=0)\n",
    "                    \n",
    "                    # print(f\"self.grads[{ds_key}][0]: {self.grads[ds_key]['saliency'][0].shape}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def load_model_and_data(self, path, cfg, k, trial):\n",
    "        with HiddenPrints():\n",
    "            original_data = data_factory(cfg)\n",
    "            model_cfg = model_config_factory(cfg, k)\n",
    "            data = data_postfactory(\n",
    "                    cfg,\n",
    "                    model_cfg,\n",
    "                    original_data,\n",
    "                )\n",
    "            dataloaders = dataloader_factory(cfg, data, k=k, trial=trial)\n",
    "\n",
    "            model = model_factory(cfg, model_cfg)\n",
    "\n",
    "            model_logpath = path+f\"/k_{k:02d}/trial_{trial:04d}/best_model.pt\"\n",
    "            checkpoint = torch.load(\n",
    "                model_logpath, map_location=lambda storage, loc: storage\n",
    "            )\n",
    "            model.load_state_dict(checkpoint)\n",
    "\n",
    "        return model, dataloaders, checkpoint\n",
    "\n",
    "    def run(self, cutoff=1, percentile=0.1):\n",
    "        \"\"\"Run introspection, save results\"\"\"\n",
    "\n",
    "        # plot everything\n",
    "        for method in self.methods:\n",
    "            print(f\"Plotting {method}\")\n",
    "\n",
    "            # for key in self.grads:\n",
    "            #     print(f\"Plotting {key} data\")\n",
    "            #     for i in range(self.n_classes):\n",
    "            #         # print(f\"Plotting time series for true target {i}\")\n",
    "            #         # self.plot_timeseries(\n",
    "            #         #     timeseries=self.timeseries[key][i], \n",
    "            #         #     smooth_timeseries=self.smooth_timeseries[key][i], \n",
    "            #         #     grads=self.grads[key][method][i],\n",
    "            #         #     features=self.features[key][i],\n",
    "            #         #     predictions=self.predictions[key][i],\n",
    "            #         #     target=i,\n",
    "            #         #     cutoff=cutoff, \n",
    "            #         #     filepath=f\"{self.save_path}{method}/{key}_timeseries_target_{i}\"\n",
    "            #         # )\n",
    "\n",
    "            #     print(f\"Plotting spatial attention\")\n",
    "            #     self.plot_histograms(\n",
    "            #         grads=self.grads[key][method],\n",
    "            #         filepath=f\"{self.save_path}{method}/{key}_spatial\",\n",
    "            #         method=method,\n",
    "            #         percentile=percentile\n",
    "            #     )\n",
    "\n",
    "    def plot_timeseries(self, timeseries, smooth_timeseries, grads, features, predictions, target, filepath, cutoff, use_log=False):\n",
    "        fig, ax = plt.subplots(2, cutoff,figsize=(3*cutoff, 4))\n",
    "\n",
    "        x = np.arange(timeseries[0].shape[1])\n",
    "        for i in range(cutoff):\n",
    "            smooth_line_0 = ax[0][i].plot(x[:140], smooth_timeseries[0][i][:140], label='0', color='blue')\n",
    "            smooth_line_1 = ax[0][i].plot(x[:140], smooth_timeseries[1][i][:140], label='1', color='red')\n",
    "            line_0 = ax[0][i].plot(x[:140], timeseries[0][i][:140], '#00009922')\n",
    "            line_1 = ax[0][i].plot(x[:140], timeseries[1][i][:140], '#99000022')\n",
    "\n",
    "            # ax[i].set_title(f\"True target = {target}, Predicted target = {predictions[i]}\")\n",
    "            ax[0][i].legend(title=\"Prediction\")\n",
    "            \n",
    "            ax[0][i].set_xlim(0, 140)\n",
    "            ax[0][i].set_xticklabels([])\n",
    "            ax[0][i].set_xticks([])\n",
    "            ax[0][i].set_yticklabels([])\n",
    "            ax[0][i].set_yticks([])\n",
    "            ax[0][i].set_xlabel(\"Time\")\n",
    "            ax[0][i].set_ylabel(\"Prediction strength\" if i == 0 else \"\")\n",
    "\n",
    "            # grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            feat = features[i].cpu().detach().numpy()[np.newaxis, :, :]\n",
    "            _, _ = viz.visualize_image_attr(\n",
    "                np.transpose(grad, (2, 1, 0))[:,:140, :],\n",
    "                np.transpose(feat, (2, 1, 0))[:,:140, :],\n",
    "                method=\"heat_map\",\n",
    "                cmap=\"inferno\",\n",
    "                show_colorbar=False,\n",
    "                plt_fig_axis=(fig, ax[1][i]),\n",
    "                use_pyplot=False,\n",
    "            )\n",
    "            ax[0][i].grid()\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "  \n",
    "    def plot_histograms(self, grads, filepath, method, percentile=0.1):\n",
    "        sns.reset_defaults()\n",
    "\n",
    "        data_0 = grads[0]\n",
    "        data_1 = grads[1]\n",
    "\n",
    "        corr_0 = np.corrcoef(grads[0].reshape(-1, grads[0].shape[2]).T)\n",
    "        corr_1 = np.corrcoef(grads[1].reshape(-1, grads[1].shape[2]).T)\n",
    "        difcor = np.abs(corr_0-corr_1)\n",
    "\n",
    "\n",
    "\n",
    "        pearson_0 = np.zeros((data_0.shape[0], data_0.shape[2], data_0.shape[2]))\n",
    "        pearson_1 = np.zeros((data_1.shape[0], data_1.shape[2], data_1.shape[2]))\n",
    "        for i in range(pearson_0.shape[0]):\n",
    "            pearson_0[i, :, :] = np.corrcoef(data_0[i, :, :], rowvar=False)\n",
    "        for i in range(pearson_1.shape[0]):\n",
    "            pearson_1[i, :, :] = np.corrcoef(data_1[i, :, :], rowvar=False)\n",
    "\n",
    "        np.save(f\"{filepath}_fnc_grads_0.npy\", pearson_0)\n",
    "        np.save(f\"{filepath}_fnc_grads_1.npy\", pearson_1)\n",
    "        \n",
    "        # mannwhitney U rank test\n",
    "        pvals_mw = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_mw.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.mannwhitneyu(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                )\n",
    "                    \n",
    "                pvals_mw[-1].append(test.pvalue)\n",
    "\n",
    "            pvals_mw[-1] = np.array(pvals_mw[-1])\n",
    "        pvals_mw = np.array(pvals_mw)\n",
    "\n",
    "        # ttest_ind test\n",
    "        pvals_ttest = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_ttest.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.ttest_ind(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                    equal_var=False\n",
    "                )\n",
    "                pvals_ttest[-1].append(test.pvalue)\n",
    "            pvals_ttest[-1] = np.array(pvals_ttest[-1])\n",
    "        pvals_ttest = np.array(pvals_ttest)\n",
    "        \n",
    "        # kstest\n",
    "        pvals_ks = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_ks.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.kstest(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                )\n",
    "                pvals_ks[-1].append(test.pvalue)\n",
    "            pvals_ks[-1] = np.array(pvals_ks[-1])\n",
    "        pvals_ks = np.array(pvals_ks)\n",
    "\n",
    "        # brunnermunzel\n",
    "        pvals_bm = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_bm.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.kstest(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                )\n",
    "                pvals_bm[-1].append(test.pvalue)\n",
    "            pvals_bm[-1] = np.array(pvals_bm[-1])\n",
    "        pvals_bm = np.array(pvals_bm)\n",
    "\n",
    "        tests = [\n",
    "            (\"Mann-Whitney U rank test\", pvals_mw), \n",
    "            (\"T-test\", pvals_ttest),\n",
    "            (\"Kolmogorov-Smirnov test\", pvals_ks),\n",
    "            (\"Brunner-Munzel test\", pvals_bm),\n",
    "        ]\n",
    "\n",
    "        pearson_0 = pearson_0.mean(axis=0)\n",
    "        pearson_1 = pearson_1.mean(axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(5, 3, figsize=(15, 25))\n",
    "        ims = []\n",
    "\n",
    "        ims.append(ax[0][0].imshow(pearson_0, cmap=corr_cmap))\n",
    "        ims[-1].set(clim=(-1.0, 1.0))\n",
    "        ax[0][0].set_xticks([], [])\n",
    "        ax[0][0].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[0][0].axis(\"off\")\n",
    "        ax[0][0].set_title(\"FNC class 0\")\n",
    "        \n",
    "        ims.append(ax[0][1].imshow(pearson_0, cmap=corr_cmap))\n",
    "        ims[-1].set(clim=(-1.0, 1.0))\n",
    "        ax[0][1].set_xticks([], [])\n",
    "        ax[0][1].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[0][1].axis(\"off\")\n",
    "        ax[0][1].set_title(\"FNC class 1\")\n",
    "\n",
    "        for test_idx in range(4):\n",
    "            # correction\n",
    "            pvals = tests[test_idx][1]\n",
    "            # pvals = pvals.reshape(corr_shape, corr_shape)\n",
    "\n",
    "            # setting to 1 manually, tests are sensitive to the rounding error variance\n",
    "            pvals[np.diag_indices(pvals.shape[0])] = 1\n",
    "            pvals[np.triu_indices_from(pvals, k=1)] = stats.false_discovery_control(pvals[np.triu_indices_from(pvals, k=1)], method='by')\n",
    "            pvals_triu = np.triu(pvals, k=1)\n",
    "            pvals[np.tril_indices_from(pvals, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "            np.save(f\"{filepath}_fnc_{tests[test_idx][0]}.npy\", pvals)\n",
    "\n",
    "            ims.append(ax[1+test_idx][0].matshow(pvals, cmap=corr_cmap))\n",
    "            ax[1+test_idx][0].set_xticks([], [])\n",
    "            ax[1+test_idx][0].set_yticks([], [])\n",
    "            fig.colorbar(ims[-1])\n",
    "            ax[1+test_idx][0].axis(\"off\")\n",
    "            ax[1+test_idx][0].set_title(f\"{tests[test_idx][0]}\")\n",
    "\n",
    "            ims.append(ax[1+test_idx][2].matshow(difcor, cmap=corr_cmap))\n",
    "            ax[1+test_idx][2].set_xticks([], [])\n",
    "            ax[1+test_idx][2].set_yticks([], [])\n",
    "            fig.colorbar(ims[-1])\n",
    "            ax[1+test_idx][2].axis(\"off\")\n",
    "            ax[1+test_idx][2].set_title(f\"Abs covar diff showed before\")\n",
    "\n",
    "            \n",
    "            pvals[pvals > 0.05] = 1\n",
    "            ims.append(ax[1+test_idx][1].matshow(pvals, cmap=corr_cmap))\n",
    "            ims[-1].set(clim=(0, 0.05))\n",
    "            ax[1+test_idx][1].set_xticks([], [])\n",
    "            ax[1+test_idx][1].set_yticks([], [])\n",
    "            fig.colorbar(ims[-1])\n",
    "            ax[1+test_idx][1].axis(\"off\")\n",
    "            ax[1+test_idx][1].set_title(f\"{tests[test_idx][0]} significant\")\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_fnc.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        # CONCAT FNC    \n",
    "        if len(grads[0].shape) == 3:\n",
    "            data_0 = grads[0].reshape(-1, grads[0].shape[2])\n",
    "            data_1 = grads[1].reshape(-1, grads[1].shape[2])\n",
    "            x = np.arange(grads[0].shape[2])\n",
    "        else:\n",
    "            data_0 = grads[0]\n",
    "            data_1 = grads[1]\n",
    "            x = np.arange(grads[0].shape[1])\n",
    "        \n",
    "        corr_0 = np.corrcoef(data_0.T)\n",
    "        corr_1 = np.corrcoef(data_1.T)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(corr_0, cmap=corr_cmap)\n",
    "        cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_cocat_fnc_0.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(corr_1, cmap=corr_cmap)\n",
    "        cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_cocat_fnc_1.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(np.abs(corr_0-corr_1), cmap=abs_corr_cmap)\n",
    "        # cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_cocat_fnc_diff.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        # fig, ax = plt.subplots(1, 4, figsize=(cutoff+3, 7))\n",
    "        n_panels = 2+4\n",
    "        fig, ax = plt.subplots(1, n_panels, figsize=(6*n_panels, 14))\n",
    "        # fig, ax = plt.subplots(1, 2, figsize=(12, 14))\n",
    "\n",
    "        if method != \"ignt\":\n",
    "            data_1 = data_1 * -1.0\n",
    "\n",
    "        df_0 = pd.DataFrame(data_0, columns=[i for i in range(data_0.shape[1])])\n",
    "        df_0['Class'] = 0\n",
    "        df_1 = pd.DataFrame(data_1, columns=[i for i in range(data_1.shape[1])])\n",
    "        df_1['Class'] = 1\n",
    "        df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "        df = pd.melt(df, id_vars=['Class'], var_name='Component', value_name='Data')\n",
    "        sns.boxplot(df, y=\"Component\", x=\"Data\", hue=\"Class\", ax=ax[0], palette=[\"blue\", \"red\"], showfliers = False, orient='h', linewidth=0.3)\n",
    "        ax[0].grid(axis='y', linestyle='--', alpha=0.3, linewidth=0.3)\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[0].set_yticks(x, x, fontsize=6)\n",
    "        ax[0].set_ylim(-3, 55)\n",
    "        ax[0].set_title(\"Gradients\")\n",
    "\n",
    "        # data has shape [observations, components]\n",
    "\n",
    "        # mannwhitney U rank test\n",
    "        test = stats.mannwhitneyu(data_0, data_1)\n",
    "        pvals_mw = test.pvalue\n",
    "\n",
    "        # ttest_ind test\n",
    "        test = stats.ttest_ind(\n",
    "            data_0, \n",
    "            data_1,\n",
    "            equal_var=False\n",
    "        )\n",
    "        pvals_ttest = test.pvalue\n",
    "        \n",
    "        # kstest\n",
    "        pvals_ks = []\n",
    "        for i in range(data_0.shape[1]):\n",
    "            test = stats.kstest(data_0[:, i], data_1[:, i])\n",
    "            pvals_ks.append(test.pvalue)\n",
    "        pvals_ks = np.array(pvals_ks)\n",
    "\n",
    "        # brunnermunzel\n",
    "        pvals_bm = []\n",
    "        for i in range(data_0.shape[1]):\n",
    "            test = stats.brunnermunzel(data_0[:, i], data_1[:, i])\n",
    "            pvals_bm.append(test.pvalue)\n",
    "        pvals_bm = np.array(pvals_bm)\n",
    "\n",
    "        tests = [\n",
    "            (\"Mann-Whitney U rank test\", pvals_mw), \n",
    "            (\"T-test\", pvals_ttest),\n",
    "            (\"Kolmogorov-Smirnov test\", pvals_ks),\n",
    "            (\"Brunner-Munzel test\", pvals_bm),\n",
    "        \n",
    "        ]\n",
    "        for test_idx in range(4):\n",
    "            # correction\n",
    "            pvals = tests[test_idx][1]\n",
    "            pvals = stats.false_discovery_control(pvals, method='by')\n",
    "            significant_p_vals = np.argwhere(pvals < 0.05)\n",
    "            # pvals = pvals / pvals.shape[0]\n",
    "\n",
    "            bars = ax[test_idx+1].barh(\n",
    "                x,\n",
    "                pvals,\n",
    "                align=\"center\",\n",
    "                color='blue',\n",
    "            )\n",
    "            ax[test_idx+1].set_yticks(x, [neuromark[k][0] if k in significant_p_vals else k for k in x], fontsize=6)\n",
    "            ax[test_idx+1].set_xlabel(\"P-value\")\n",
    "            ax[test_idx+1].set_xscale(\"log\")\n",
    "            ax[test_idx+1].set_title(tests[test_idx][0])\n",
    "\n",
    "\n",
    "            for i, bar in enumerate(bars):\n",
    "                if pvals[i] < 0.05:\n",
    "                    bar.set_color('red')\n",
    "            # bar charts: summarizes gradients at each component\n",
    "\n",
    "        data_0 = np.median(data_0, axis=0)\n",
    "        data_1 = np.median(data_1, axis=0)\n",
    "        \n",
    "        min_0, max_0 = np.min(data_0), np.max(data_0)\n",
    "        min_1, max_1 = np.min(data_1), np.max(data_1)\n",
    "        min_v, max_v = min(min_0, min_1), max(max_0, max_1)\n",
    "        if min_v <= 0:\n",
    "            max_v = max(abs(min_v), abs(max_v))\n",
    "            min_v = -1.0*max_v\n",
    "        else:\n",
    "            min_v = 0\n",
    "                \n",
    "        sym_data_0 = np.zeros(x.shape)\n",
    "        sym_data_1 = np.zeros(x.shape)\n",
    "        asym_data_0 = np.zeros(x.shape)\n",
    "        asym_data_1 = np.zeros(x.shape)\n",
    "\n",
    "        for j in x:\n",
    "            sym_data_0[j] = np.sign(data_0[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            sym_data_1[j] = np.sign(data_1[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            asym_data_0[j] = data_0[j] - sym_data_0[j]\n",
    "            asym_data_1[j] = data_1[j] - sym_data_1[j]\n",
    "\n",
    "        abs_grad = np.abs(data_0) + np.abs(data_1)\n",
    "        abs_sym_grad = np.abs(sym_data_0) + np.abs(sym_data_1)\n",
    "        abs_asym_grad = np.abs(asym_data_0) + np.abs(asym_data_1)\n",
    "        significant_comp = np.argsort(abs_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_sym_comp = np.argsort(abs_sym_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_asym_comp = np.argsort(abs_asym_grad)[-int(percentile*x.shape[0]):]\n",
    "\n",
    "        all_significant = np.intersect1d(significant_comp, np.intersect1d(significant_sym_comp, significant_asym_comp))\n",
    "        all_significant = {\n",
    "            \"idx\": all_significant,\n",
    "            \"name\": [neuromark[k][0] for k in all_significant],\n",
    "            \"network\": [neuromark[k][1] for k in all_significant],\n",
    "            \"x\": [neuromark[k][2] for k in all_significant],\n",
    "            \"y\": [neuromark[k][3] for k in all_significant],\n",
    "            \"z\": [neuromark[k][4] for k in all_significant],\n",
    "        }\n",
    "        pd.DataFrame(all_significant).to_csv(f\"{filepath}_significant.csv\")\n",
    "\n",
    "\n",
    "        ax[-1].barh(\n",
    "            x,\n",
    "            data_0,\n",
    "            align=\"center\",\n",
    "            color='blue',\n",
    "            label='0', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax[-1].barh(\n",
    "            x,\n",
    "            data_1,\n",
    "            align=\"center\",\n",
    "            color='red',\n",
    "            label='1', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax[-1].set_xlim(min_v, max_v)\n",
    "        ax[-1].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax[-1].set_yticks(x, [neuromark[k][0] if k in significant_comp else k for k in x], fontsize=6)\n",
    "        ax[-1].set_title(\"Grad medians\")\n",
    "\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    \n",
    "    def get_grads(self, model, method, features, target):\n",
    "        \"\"\"Returns gradients according to method\"\"\"\n",
    "        model.zero_grad()\n",
    "\n",
    "        if method == \"saliency\":\n",
    "            saliency = Saliency(model)\n",
    "            grads = saliency.attribute(features, target=target, abs=False)\n",
    "        elif method == \"ig\":\n",
    "            # ig = IntegratedGradients(self.model, multiply_by_inputs=False)\n",
    "            ig = IntegratedGradients(model, multiply_by_inputs=True)\n",
    "            grads, _ = ig.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                baselines=torch.zeros_like(features),\n",
    "                return_convergence_delta=True,\n",
    "            )\n",
    "        elif method == \"ignt\":\n",
    "            ig = Saliency(model)\n",
    "            nt = NoiseTunnel(ig)\n",
    "            grads = nt.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                abs=False,\n",
    "                nt_type=\"vargrad\",\n",
    "                nt_samples=10,\n",
    "                stdevs=0.3,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"'{method}' methods is not recognized\")\n",
    "\n",
    "        return grads\n",
    "\n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "# paths = [\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\"]\n",
    "    \n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# paths = [\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-bsnip\"]\n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\", \n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-bsnip\", \n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-cobre\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-adni\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-abide_869\"\n",
    "#     ]\n",
    "paths = [\n",
    "    \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-bsnip\",\n",
    "    # \"./assets/logs/rerun_all-exp-dice_defHP-bsnip\",\n",
    "    ]\n",
    "# paths = [\n",
    "#     \"./assets/logs/AAA-exp-rearranged_mlp_defHP-bsnip\",\n",
    "#     # \"./assets/logs/rerun_all-exp-milc_defHP-bsnip\",\n",
    "#     # \"./assets/logs/AAA-exp-dice_defHP-bsnip\",\n",
    "#     ]\n",
    "# paths = [\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\", \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-bsnip\"]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "# methods = [\"saliency\", \"ignt\"]\n",
    "# methods = [\"ig\"]\n",
    "# methods = [\"ignt\"]\n",
    "methods = [\"saliency\"]\n",
    "\n",
    "for path in paths:\n",
    "    introspector = IntrospectorV2(\n",
    "        path=path,\n",
    "        methods=methods, \n",
    "        save_path=f\"{path.replace('/logs/','/introspection/')}/\"\n",
    "    )\n",
    "    introspector.run(cutoff=10, percentile=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BSNIP T-test maksing experiments\n",
    "def compute_fnc(data):\n",
    "    new_data = np.zeros((data.shape[0], data.shape[2], data.shape[2]))\n",
    "    for i in range(new_data.shape[0]):\n",
    "        new_data[i, :, :] = np.corrcoef(data[i, :, :], rowvar=False)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def run_ttest(data1, data2):\n",
    "    pvals = []\n",
    "    tvals = []\n",
    "    for i in range(data1.shape[1]):\n",
    "        pvals.append([])\n",
    "        tvals.append([])\n",
    "        for j in range(data1.shape[1]):\n",
    "            test = stats.ttest_ind(\n",
    "                data1[:, i, j], \n",
    "                data2[:, i, j],\n",
    "                equal_var=False\n",
    "            )\n",
    "            pvals[-1].append(test.pvalue)\n",
    "            tvals[-1].append(test.statistic)\n",
    "        pvals[-1] = np.array(pvals[-1])\n",
    "        tvals[-1] = np.array(tvals[-1])\n",
    "    pvals = np.array(pvals)\n",
    "    tvals = np.array(tvals)\n",
    "\n",
    "    #run FDR correction\n",
    "    pvals[np.diag_indices_from(pvals)] = 1\n",
    "    pvals[np.triu_indices_from(pvals, k=1)] = stats.false_discovery_control(pvals[np.triu_indices_from(pvals, k=1)], method='by')\n",
    "    pvals_triu = np.triu(pvals, k=1)\n",
    "    pvals[np.tril_indices_from(pvals, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "\n",
    "    #compute mask from pvals\n",
    "    mask = np.ones_like(pvals)\n",
    "    mask[pvals < 0.05] = 1\n",
    "    mask[pvals >= 0.05] = 0\n",
    "\n",
    "    return pvals, tvals, mask\n",
    "\n",
    "method = \"saliency\"\n",
    "\n",
    "# plots = {\n",
    "#     \"mlp-20_to_80\": {\n",
    "#         \"bsnip_trained\": (f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip copy/{method}/fbirn\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip copy/{method}/train\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip copy/{method}/test\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip copy/{method}/cobre\"),\n",
    "#         # \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/train\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/cobre\"),\n",
    "#     },\n",
    "#     \"mlp-80_to-20\": {\n",
    "#         # \"bsnip_trained\": (f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/train\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/test\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/cobre\"),\n",
    "#         \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip copy/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip copy/{method}/train\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip copy/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip copy/{method}/cobre\"),\n",
    "#     },\n",
    "# }\n",
    "plots = {\n",
    "    \"mlp-20_to_80\": {\n",
    "        \"bsnip_trained\": (f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/train\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/test\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/cobre\"),\n",
    "        # \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/train\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/cobre\"),\n",
    "    },\n",
    "    \"mlp-80_to-20\": {\n",
    "        # \"bsnip_trained\": (f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/train\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/test\", f\"./assets/introspection/AAA-exp-rearranged_mlp_defHP-bsnip/{method}/cobre\"),\n",
    "        \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/train\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/cobre\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "data_tested = \"grads\"\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "ims = []\n",
    "for index, (model, model_paths) in enumerate(plots.items()):\n",
    "    for (_, paths) in model_paths.items():\n",
    "        \n",
    "        if len(ax) == 4:\n",
    "            ax = [ax]\n",
    "\n",
    "        fbirn_grads_0 = np.load(f\"{paths[0]}_grads_0.npy\")\n",
    "        fbirn_grads_1 = np.load(f\"{paths[0]}_grads_1.npy\")\n",
    "        bsnip_grads_0 = np.load(f\"{paths[1]}_grads_0.npy\")\n",
    "        bsnip_grads_1 = np.load(f\"{paths[1]}_grads_1.npy\")\n",
    "        bsnip_grads_test_0 = np.load(f\"{paths[2]}_grads_0.npy\")\n",
    "        bsnip_grads_test_1 = np.load(f\"{paths[2]}_grads_1.npy\")\n",
    "        cobre_grads_0 = np.load(f\"{paths[3]}_grads_0.npy\")\n",
    "        cobre_grads_1 = np.load(f\"{paths[3]}_grads_1.npy\")\n",
    "\n",
    "        data = {\n",
    "            \"bsnip_train\": {\n",
    "                \"raw_grads\": (bsnip_grads_0, bsnip_grads_1),\n",
    "            },\n",
    "            \"bsnip_test\": {\n",
    "                \"raw_grads\": (bsnip_grads_test_0, bsnip_grads_test_1),\n",
    "            },\n",
    "            \"fbirn\": {\n",
    "                \"raw_grads\": (fbirn_grads_0, fbirn_grads_1),\n",
    "            },\n",
    "            \"cobre\": {\n",
    "                \"raw_grads\": (cobre_grads_0, cobre_grads_1),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for dataset in data:\n",
    "            data[dataset][\"grads\"] = (compute_fnc(data[dataset][\"raw_grads\"][0]), compute_fnc(data[dataset][\"raw_grads\"][1]))\n",
    "            for data_type in [\"grads\"]:\n",
    "                pvals, tvals, mask = run_ttest(data[dataset][data_type][0], data[dataset][data_type][1])\n",
    "                data[dataset][data_type+\"_pvals\"] = pvals\n",
    "                data[dataset][data_type+\"_tvals\"] = tvals\n",
    "                data[dataset][data_type+\"_mask\"] = mask\n",
    "\n",
    "                data[dataset][data_type+\"_mean\"] = np.mean(np.concatenate((data[dataset][data_type][0], data[dataset][data_type][0]), axis=0), axis=0)\n",
    "        \n",
    "        # primary_dataset = \"bsnip_train\"\n",
    "        # secondary_dataset = \"bsnip_test\"\n",
    "        secondary_dataset = \"bsnip_train\"\n",
    "        primary_dataset = \"bsnip_test\"\n",
    "        tertiary_dataset = \"fbirn\"\n",
    "        dataset_4 = \"cobre\"\n",
    "\n",
    "        plot_index = 0\n",
    "\n",
    "        plot_data = data[primary_dataset][data_tested+\"_tvals\"] * data[primary_dataset][data_type+\"_mask\"] \n",
    "        data_max = np.max(np.abs(plot_data))\n",
    "        ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-data_max, data_max))\n",
    "        ax[index][plot_index].set_xticks([], [])\n",
    "        ax[index][plot_index].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4) \n",
    "        ax[index][plot_index].axis(\"off\")\n",
    "        ax[index][plot_index].set_title(f\"{model}: {primary_dataset}\", fontsize = 6)\n",
    "        plot_index += 1\n",
    "\n",
    "        # plot_data_and = data[secondary_dataset][data_tested+\"_tvals\"] * np.logical_and(data[primary_dataset][data_type+\"_mask\"], data[secondary_dataset][data_type+\"_mask\"])\n",
    "        # plot_data_andnot = data[secondary_dataset][data_tested+\"_tvals\"] * np.logical_and(np.logical_not(data[primary_dataset][data_type+\"_mask\"]), data[secondary_dataset][data_type+\"_mask\"])\n",
    "        # plot_data = np.tril(plot_data_and) + np.triu(plot_data_andnot)\n",
    "        # data_max = np.max(np.abs(plot_data))\n",
    "        # ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "        # ims[-1].set(clim=(-data_max, data_max))\n",
    "        # ax[index][plot_index].set_xticks([], [])\n",
    "        # ax[index][plot_index].set_yticks([], [])\n",
    "        # fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "        # ax[index][plot_index].axis(\"off\")\n",
    "        # ax[index][plot_index].set_title(f\"{secondary_dataset}\", fontsize = 6)\n",
    "        # plot_index += 1\n",
    "\n",
    "        plot_data_and = data[tertiary_dataset][data_tested+\"_tvals\"] * np.logical_and(data[primary_dataset][data_type+\"_mask\"], data[tertiary_dataset][data_type+\"_mask\"])\n",
    "        plot_data_andnot = data[tertiary_dataset][data_tested+\"_tvals\"] * np.logical_and(np.logical_not(data[primary_dataset][data_type+\"_mask\"]), data[tertiary_dataset][data_type+\"_mask\"])\n",
    "        plot_data = np.tril(plot_data_and) + np.triu(plot_data_andnot)\n",
    "        data_max = np.max(np.abs(plot_data))\n",
    "        ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-data_max, data_max))\n",
    "        ax[index][plot_index].set_xticks([], [])\n",
    "        ax[index][plot_index].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "        ax[index][plot_index].axis(\"off\")\n",
    "        ax[index][plot_index].set_title(f\"{tertiary_dataset}\", fontsize = 6)\n",
    "        plot_index += 1\n",
    "\n",
    "        plot_data_and = data[dataset_4][data_tested+\"_tvals\"] * np.logical_and(data[primary_dataset][data_type+\"_mask\"], data[dataset_4][data_type+\"_mask\"])\n",
    "        plot_data_andnot = data[dataset_4][data_tested+\"_tvals\"] * np.logical_and(np.logical_not(data[primary_dataset][data_type+\"_mask\"]), data[dataset_4][data_type+\"_mask\"])\n",
    "        plot_data = np.tril(plot_data_and) + np.triu(plot_data_andnot)\n",
    "        data_max = np.max(np.abs(plot_data))\n",
    "        ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-data_max, data_max))\n",
    "        ax[index][plot_index].set_xticks([], [])\n",
    "        ax[index][plot_index].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "        ax[index][plot_index].axis(\"off\")\n",
    "        ax[index][plot_index].set_title(f\"{dataset_4}\", fontsize = 6)\n",
    "        plot_index += 1\n",
    "\n",
    "fig.savefig(f\"test_masks_{primary_dataset}.png\", dpi=300, bbox_inches='tight')\n",
    "fig.savefig(f\"test_masks_{primary_dataset}.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plots\n",
    "\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "\n",
    "data_0 = np.load(\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/saliency/test_grads_0.npy\")\n",
    "data_1 = np.load(\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/saliency/test_grads_1.npy\")\n",
    "x = np.arange(data_0.shape[2])\n",
    "data_0 = data_0.reshape(-1, data_0.shape[2])\n",
    "data_1 = data_1.reshape(-1, data_1.shape[2]) * -1.0\n",
    "\n",
    "df_0 = pd.DataFrame(data_0, columns=[i for i in range(data_0.shape[1])])\n",
    "df_0['Class'] = 0\n",
    "df_1 = pd.DataFrame(data_1, columns=[i for i in range(data_1.shape[1])])\n",
    "df_1['Class'] = 1\n",
    "df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "df = pd.melt(df, id_vars=['Class'], var_name='Component', value_name='Saliency gradient')\n",
    "sns.boxplot(df, y=\"Saliency gradient\", x=\"Component\", hue=\"Class\", ax=ax, palette=[\"blue\", \"red\"], showfliers = False, linewidth=0.3)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3, linewidth=0.3)\n",
    "# ax.invert_yaxis()\n",
    "ax.set_xticks(x, x, fontsize=6)\n",
    "# ax.set_ylim(-3, 55)\n",
    "ax.set_title(\"Gradients\")\n",
    "\n",
    "\n",
    "fig.savefig(\n",
    "    f\"barplots1.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fig.savefig(\n",
    "    f\"barplots1.svg\",\n",
    "    format=\"svg\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plots\n",
    "\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "\n",
    "data_0 = np.load(\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/saliency/test_grads_0.npy\")\n",
    "data_1 = np.load(\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/saliency/test_grads_1.npy\")\n",
    "x = np.arange(data_0.shape[2])\n",
    "data_0 = data_0.reshape(-1, data_0.shape[2])\n",
    "data_1 = data_1.reshape(-1, data_1.shape[2]) * -1.0\n",
    "\n",
    "# # data has shape [observations, components]\n",
    "\n",
    "# ttest_ind test\n",
    "test = stats.ttest_ind(\n",
    "    data_0, \n",
    "    data_1,\n",
    "    # equal_var=False\n",
    ")\n",
    "pvals_ttest = test.pvalue\n",
    "tvals_ttest = test.statistic\n",
    "\n",
    "np.save(\"pvals.npy\", pvals_ttest)\n",
    "np.save(\"tvals.npy\", tvals_ttest)\n",
    "\n",
    "pvals_ttest = np.load(\"pvals.npy\")\n",
    "tvals_ttest = np.load(\"tvals.npy\")\n",
    "\n",
    "tests = (\"Welch's T-test\", pvals_ttest, tvals_ttest)\n",
    "\n",
    "pvals = tests[1]\n",
    "pvals = stats.false_discovery_control(pvals, method='by')\n",
    "significant_p_vals = np.argwhere(pvals < 0.05)\n",
    "# pvals = pvals / pvals.shape[0]\n",
    "\n",
    "bars = ax.bar(\n",
    "    x,\n",
    "    pvals,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    ")\n",
    "ax.set_xticks(x, x, fontsize=6)\n",
    "ax.set_ylabel(\"P-value\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(tests[0])\n",
    "ax.set_xlim(-0.5, 52.5)\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    if pvals[i] < 0.05:\n",
    "        bar.set_color('red')\n",
    "# bar charts: summarizes gradients at each component\n",
    "\n",
    "\n",
    "fig.savefig(\n",
    "    f\"barplots2.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fig.savefig(\n",
    "    f\"barplots2.svg\",\n",
    "    format=\"svg\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.standard_normal(50)\n",
    "data_2 = np.random.standard_normal(50)\n",
    "\n",
    "i = 10\n",
    "while i < 1e10:\n",
    "    data_1 = np.random.standard_normal((i, 50))\n",
    "    # data_2 = np.random.standard_normal((i, 50))\n",
    "    data_2 = 0.001 * np.ones((i, 50)) + np.random.standard_normal((i, 50))\n",
    "\n",
    "    test = stats.ttest_ind(\n",
    "        data_1, \n",
    "        data_2,\n",
    "        # equal_var=False\n",
    "    )\n",
    "    pvals = test.pvalue\n",
    "    \n",
    "    pvals = stats.false_discovery_control(pvals, method='by')\n",
    "\n",
    "    significant_p_vals = np.argwhere(pvals < 0.05)\n",
    "\n",
    "    print(i)\n",
    "    print(len(significant_p_vals))\n",
    "\n",
    "    print()\n",
    "\n",
    "    i *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"Mann-Whitney U rank test\", \"T-test\",\"Kolmogorov-Smirnov test\",\"Brunner-Munzel test\"]\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"cobre_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/fbirn_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/bsnip_spatial_fnc_\"),\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip_spatial_fnc_\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test_spatial_fnc_\"),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(3, 8, figsize=(24, 9))\n",
    "ims = []\n",
    "for index, (figure, paths) in enumerate(plots.items()):\n",
    "    \n",
    "    fbirn_grads_0 = np.load(f\"{paths[0]}grads_0.npy\")\n",
    "    fbirn_grads_1 = np.load(f\"{paths[0]}grads_1.npy\")\n",
    "    bsnip_grads_0 = np.load(f\"{paths[1]}grads_0.npy\")\n",
    "    bsnip_grads_1 = np.load(f\"{paths[1]}grads_1.npy\")\n",
    "\n",
    "    fbirn_pvals_ttest = []\n",
    "    fbirn_tvals_ttest = []\n",
    "    bsnip_pvals_ttest = []\n",
    "    bsnip_tvals_ttest = []\n",
    "    for i in range(fbirn_grads_0.shape[1]):\n",
    "        fbirn_pvals_ttest.append([])\n",
    "        fbirn_tvals_ttest.append([])\n",
    "        bsnip_pvals_ttest.append([])\n",
    "        bsnip_tvals_ttest.append([])\n",
    "        for j in range(fbirn_grads_0.shape[1]):\n",
    "            fbirn_test = stats.ttest_ind(\n",
    "                fbirn_grads_0[:, i, j], \n",
    "                fbirn_grads_1[:, i, j],\n",
    "                equal_var=False\n",
    "            )\n",
    "            bsnip_test = stats.ttest_ind(\n",
    "                bsnip_grads_0[:, i, j], \n",
    "                bsnip_grads_1[:, i, j],\n",
    "                equal_var=False\n",
    "            )\n",
    "            fbirn_pvals_ttest[-1].append(fbirn_test.pvalue)\n",
    "            fbirn_tvals_ttest[-1].append(fbirn_test.statistic)\n",
    "            bsnip_pvals_ttest[-1].append(bsnip_test.pvalue)\n",
    "            bsnip_tvals_ttest[-1].append(bsnip_test.statistic)\n",
    "        fbirn_pvals_ttest[-1] = np.array(fbirn_pvals_ttest[-1])\n",
    "        fbirn_tvals_ttest[-1] = np.array(fbirn_tvals_ttest[-1])\n",
    "        bsnip_pvals_ttest[-1] = np.array(bsnip_pvals_ttest[-1])\n",
    "        bsnip_tvals_ttest[-1] = np.array(bsnip_tvals_ttest[-1])\n",
    "    fbirn_pvals_ttest = np.array(fbirn_pvals_ttest)\n",
    "    fbirn_tvals_ttest = np.array(fbirn_tvals_ttest)\n",
    "    bsnip_pvals_ttest = np.array(bsnip_pvals_ttest)\n",
    "    bsnip_tvals_ttest = np.array(bsnip_tvals_ttest)\n",
    "    \n",
    "    fbirn_pvals_ttest[np.diag_indices(fbirn_pvals_ttest.shape[0])] = 1\n",
    "    fbirn_pvals_ttest[np.triu_indices_from(fbirn_pvals_ttest, k=1)] = stats.false_discovery_control(fbirn_pvals_ttest[np.triu_indices_from(fbirn_pvals_ttest, k=1)], method='by')\n",
    "    pvals_triu = np.triu(fbirn_pvals_ttest, k=1)\n",
    "    fbirn_pvals_ttest[np.tril_indices_from(fbirn_pvals_ttest, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "    bsnip_pvals_ttest[np.diag_indices(bsnip_pvals_ttest.shape[0])] = 1\n",
    "    bsnip_pvals_ttest[np.triu_indices_from(bsnip_pvals_ttest, k=1)] = stats.false_discovery_control(bsnip_pvals_ttest[np.triu_indices_from(bsnip_pvals_ttest, k=1)], method='by')\n",
    "    pvals_triu = np.triu(bsnip_pvals_ttest, k=1)\n",
    "    bsnip_pvals_ttest[np.tril_indices_from(bsnip_pvals_ttest, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "    \n",
    "    fbirn_mask = np.ones_like(fbirn_pvals_ttest)\n",
    "    bsnip_mask = np.ones_like(bsnip_pvals_ttest)\n",
    "    fbirn_mask[fbirn_pvals_ttest < 0.05] = 1\n",
    "    bsnip_mask[bsnip_pvals_ttest < 0.05] = 1\n",
    "    fbirn_mask[fbirn_pvals_ttest >= 0.05] = 0\n",
    "    bsnip_mask[bsnip_pvals_ttest >= 0.05] = 0\n",
    "\n",
    "    fbirn_grads_mean = np.mean(np.concatenate((fbirn_grads_0, fbirn_grads_1), axis=0), axis=0)\n",
    "    bsnip_grads_mean = np.mean(np.concatenate((bsnip_grads_0, bsnip_grads_1), axis=0), axis=0)\n",
    "\n",
    "    ims.append(ax[index][0].imshow(fbirn_grads_mean, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][0].set_xticks([], [])\n",
    "    ax[index][0].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][0].axis(\"off\")\n",
    "    ax[index][0].set_title(f\"{figure} FBIRN corr mean\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][1].imshow(bsnip_grads_mean, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][1].set_xticks([], [])\n",
    "    ax[index][1].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][1].axis(\"off\")\n",
    "    ax[index][1].set_title(f\"{figure} BSNIP corr mean\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][2].imshow(fbirn_pvals_ttest, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][2].set_xticks([], [])\n",
    "    ax[index][2].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][2].axis(\"off\")\n",
    "    ax[index][2].set_title(f\"{figure} FBIRN P-vals\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][3].imshow(bsnip_pvals_ttest, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][3].set_xticks([], [])\n",
    "    ax[index][3].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][3].axis(\"off\")\n",
    "    ax[index][3].set_title(f\"{figure} BSNIP P-vals\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][4].imshow(fbirn_mask, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][4].set_xticks([], [])\n",
    "    ax[index][4].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][4].axis(\"off\")\n",
    "    ax[index][4].set_title(f\"{figure} FBIRN mask\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][5].imshow(bsnip_mask, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][5].set_xticks([], [])\n",
    "    ax[index][5].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][5].axis(\"off\")\n",
    "    ax[index][5].set_title(f\"{figure} BSNIP mask\", fontsize = 8)\n",
    "\n",
    "\n",
    "    fbirn_filtered_max_val = np.max(np.abs(fbirn_tvals_ttest*fbirn_mask))\n",
    "    ims.append(ax[index][6].imshow(fbirn_tvals_ttest*fbirn_mask, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-fbirn_filtered_max_val, fbirn_filtered_max_val))\n",
    "    ax[index][6].set_xticks([], [])\n",
    "    ax[index][6].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][6].axis(\"off\")\n",
    "    ax[index][6].set_title(f\"{figure} FBIRN T-vals masked\", fontsize = 8)\n",
    "\n",
    "    bsnip_filtered_max_val = np.max(np.abs(bsnip_tvals_ttest*bsnip_mask))\n",
    "    ims.append(ax[index][7].imshow(bsnip_tvals_ttest*bsnip_mask, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-bsnip_filtered_max_val, bsnip_filtered_max_val))\n",
    "    ax[index][7].set_xticks([], [])\n",
    "    ax[index][7].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][7].axis(\"off\")\n",
    "    ax[index][7].set_title(f\"{figure} BSNIP T-vals maksed\", fontsize = 8)\n",
    "\n",
    "\n",
    "fig.savefig(f\"test.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test Only\n",
    "def compute_fnc(data):\n",
    "    new_data = np.zeros((data.shape[0], data.shape[2], data.shape[2]))\n",
    "    for i in range(new_data.shape[0]):\n",
    "        new_data[i, :, :] = np.corrcoef(data[i, :, :], rowvar=False)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def run_ttest(data1, data2):\n",
    "    pvals = []\n",
    "    tvals = []\n",
    "    for i in range(data1.shape[1]):\n",
    "        pvals.append([])\n",
    "        tvals.append([])\n",
    "        for j in range(data1.shape[1]):\n",
    "            test = stats.ttest_ind(\n",
    "                data1[:, i, j], \n",
    "                data2[:, i, j],\n",
    "                equal_var=False\n",
    "            )\n",
    "            pvals[-1].append(test.pvalue)\n",
    "            tvals[-1].append(test.statistic)\n",
    "        pvals[-1] = np.array(pvals[-1])\n",
    "        tvals[-1] = np.array(tvals[-1])\n",
    "    pvals = np.array(pvals)\n",
    "    tvals = np.array(tvals)\n",
    "\n",
    "    #run FDR correction\n",
    "    pvals[np.diag_indices_from(pvals)] = 1\n",
    "    pvals[np.triu_indices_from(pvals, k=1)] = stats.false_discovery_control(pvals[np.triu_indices_from(pvals, k=1)], method='by')\n",
    "    pvals_triu = np.triu(pvals, k=1)\n",
    "    pvals[np.tril_indices_from(pvals, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "\n",
    "    #compute mask from pvals\n",
    "    mask = np.ones_like(pvals)\n",
    "    mask[pvals < 0.05] = 1\n",
    "    mask[pvals >= 0.05] = 0\n",
    "\n",
    "    return pvals, tvals, mask\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"cobre_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/bsnip\"),\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\"),\n",
    "}\n",
    "\n",
    "data_tested = \"features\"\n",
    "# data_tested = \"grads\"\n",
    "\n",
    "fig, ax = plt.subplots(3, 8, figsize=(24, 9))\n",
    "ims = []\n",
    "for index, (figure, paths) in enumerate(plots.items()):\n",
    "    \n",
    "    fbirn_grads_0 = np.load(f\"{paths[0]}_grads_0.npy\")\n",
    "    fbirn_grads_1 = np.load(f\"{paths[0]}_grads_1.npy\")\n",
    "    bsnip_grads_0 = np.load(f\"{paths[1]}_grads_0.npy\")\n",
    "    bsnip_grads_1 = np.load(f\"{paths[1]}_grads_1.npy\")\n",
    "    fbirn_features_0 = np.load(f\"{paths[0]}_features_0.npy\")\n",
    "    fbirn_features_1 = np.load(f\"{paths[0]}_features_1.npy\")\n",
    "    bsnip_features_0 = np.load(f\"{paths[1]}_features_0.npy\")\n",
    "    bsnip_features_1 = np.load(f\"{paths[1]}_features_1.npy\")\n",
    "\n",
    "    data = {\n",
    "        \"fbirn\": {\n",
    "            \"raw_grads\": (fbirn_grads_0, fbirn_grads_1),\n",
    "            \"raw_features\":  (fbirn_features_0, fbirn_features_1)\n",
    "        },\n",
    "        \"bsnip\": {\n",
    "            \"raw_grads\": (bsnip_grads_0, bsnip_grads_1),\n",
    "            \"raw_features\":  (bsnip_features_0, bsnip_features_1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for dataset in data:\n",
    "        data[dataset][\"grads\"] = (compute_fnc(data[dataset][\"raw_grads\"][0]), compute_fnc(data[dataset][\"raw_grads\"][1]))\n",
    "        data[dataset][\"features\"] = (compute_fnc(data[dataset][\"raw_features\"][0]), compute_fnc(data[dataset][\"raw_features\"][1]))\n",
    "        for data_type in [\"grads\", \"features\"]:\n",
    "            pvals, tvals, mask = run_ttest(data[dataset][data_type][0], data[dataset][data_type][1])\n",
    "            data[dataset][data_type+\"_pvals\"] = pvals\n",
    "            data[dataset][data_type+\"_tvals\"] = tvals\n",
    "            data[dataset][data_type+\"_mask\"] = mask\n",
    "\n",
    "            data[dataset][data_type+\"_mean\"] = np.mean(np.concatenate((data[dataset][data_type][0], data[dataset][data_type][0]), axis=0), axis=0)\n",
    "    \n",
    "    print(figure + \"data shape\")\n",
    "    for dataset in data:\n",
    "        for key in data[dataset]:\n",
    "            if key in [\"raw_grads\", \"raw_features\", \"grads\", \"features\"]:\n",
    "                print(f\"{dataset} {key}: {data[dataset][key][0].shape}, {data[dataset][key][1].shape}\")\n",
    "            else:\n",
    "                print(f\"{dataset} {key}: {data[dataset][key].shape}\")\n",
    "    print()\n",
    "\n",
    "    ims.append(ax[index][0].imshow(data[\"fbirn\"][data_tested+\"_mean\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][0].set_xticks([], [])\n",
    "    ax[index][0].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][0].axis(\"off\")\n",
    "    ax[index][0].set_title(f\"{figure} FBIRN corr mean\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][1].imshow(data[\"bsnip\"][data_tested+\"_mean\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][1].set_xticks([], [])\n",
    "    ax[index][1].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][1].axis(\"off\")\n",
    "    ax[index][1].set_title(f\"{figure} BSNIP corr mean\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][2].imshow(data[\"fbirn\"][data_tested+\"_pvals\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][2].set_xticks([], [])\n",
    "    ax[index][2].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][2].axis(\"off\")\n",
    "    ax[index][2].set_title(f\"{figure} FBIRN P-vals\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][3].imshow(data[\"bsnip\"][data_tested+\"_pvals\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][3].set_xticks([], [])\n",
    "    ax[index][3].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][3].axis(\"off\")\n",
    "    ax[index][3].set_title(f\"{figure} BSNIP P-vals\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][4].imshow(data[\"fbirn\"][data_tested+\"_mask\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][4].set_xticks([], [])\n",
    "    ax[index][4].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][4].axis(\"off\")\n",
    "    ax[index][4].set_title(f\"{figure} FBIRN mask\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][5].imshow(data[\"bsnip\"][data_tested+\"_mask\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][5].set_xticks([], [])\n",
    "    ax[index][5].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][5].axis(\"off\")\n",
    "    ax[index][5].set_title(f\"{figure} BSNIP mask\", fontsize = 8)\n",
    "\n",
    "\n",
    "    fbirn_filtered_max_val = np.max(np.abs(data[\"fbirn\"][data_tested+\"_tvals\"]*data[\"fbirn\"][data_tested+\"_mask\"]))\n",
    "    ims.append(ax[index][6].imshow(data[\"fbirn\"][data_tested+\"_tvals\"]*data[\"fbirn\"][data_tested+\"_mask\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-fbirn_filtered_max_val, fbirn_filtered_max_val))\n",
    "    ax[index][6].set_xticks([], [])\n",
    "    ax[index][6].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][6].axis(\"off\")\n",
    "    ax[index][6].set_title(f\"{figure} FBIRN T-vals masked\", fontsize = 8)\n",
    "\n",
    "    bsnip_filtered_max_val = np.max(np.abs(data[\"bsnip\"][data_tested+\"_tvals\"]*data[\"bsnip\"][data_tested+\"_mask\"]))\n",
    "    ims.append(ax[index][7].imshow(data[\"bsnip\"][data_tested+\"_tvals\"]*data[\"bsnip\"][data_tested+\"_mask\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-bsnip_filtered_max_val, bsnip_filtered_max_val))\n",
    "    ax[index][7].set_xticks([], [])\n",
    "    ax[index][7].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][7].axis(\"off\")\n",
    "    ax[index][7].set_title(f\"{figure} BSNIP T-vals maksed\", fontsize = 8)\n",
    "\n",
    "\n",
    "fig.savefig(f\"test_{data_tested}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test maksing experiments\n",
    "def compute_fnc(data):\n",
    "    new_data = np.zeros((data.shape[0], data.shape[2], data.shape[2]))\n",
    "    for i in range(new_data.shape[0]):\n",
    "        new_data[i, :, :] = np.corrcoef(data[i, :, :], rowvar=False)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def run_ttest(data1, data2):\n",
    "    pvals = []\n",
    "    tvals = []\n",
    "    for i in range(data1.shape[1]):\n",
    "        pvals.append([])\n",
    "        tvals.append([])\n",
    "        for j in range(data1.shape[1]):\n",
    "            test = stats.ttest_ind(\n",
    "                data1[:, i, j], \n",
    "                data2[:, i, j],\n",
    "                equal_var=False\n",
    "            )\n",
    "            pvals[-1].append(test.pvalue)\n",
    "            tvals[-1].append(test.statistic)\n",
    "        pvals[-1] = np.array(pvals[-1])\n",
    "        tvals[-1] = np.array(tvals[-1])\n",
    "    pvals = np.array(pvals)\n",
    "    tvals = np.array(tvals)\n",
    "\n",
    "    #run FDR correction\n",
    "    pvals[np.diag_indices_from(pvals)] = 1\n",
    "    pvals[np.triu_indices_from(pvals, k=1)] = stats.false_discovery_control(pvals[np.triu_indices_from(pvals, k=1)], method='by')\n",
    "    pvals_triu = np.triu(pvals, k=1)\n",
    "    pvals[np.tril_indices_from(pvals, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "\n",
    "    #compute mask from pvals\n",
    "    mask = np.ones_like(pvals)\n",
    "    mask[pvals < 0.05] = 1\n",
    "    mask[pvals >= 0.05] = 0\n",
    "\n",
    "    return pvals, tvals, mask\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\"),\n",
    "}\n",
    "\n",
    "data_tested = \"grads\"\n",
    "\n",
    "fig, ax = plt.subplots(2, 7, figsize=(18, 6))\n",
    "ims = []\n",
    "for index, (figure, paths) in enumerate(plots.items()):\n",
    "    \n",
    "    fbirn_grads_0 = np.load(f\"{paths[0]}_grads_0.npy\")\n",
    "    fbirn_grads_1 = np.load(f\"{paths[0]}_grads_1.npy\")\n",
    "    bsnip_grads_0 = np.load(f\"{paths[1]}_grads_0.npy\")\n",
    "    bsnip_grads_1 = np.load(f\"{paths[1]}_grads_1.npy\")\n",
    "\n",
    "    data = {\n",
    "        \"fbirn\": {\n",
    "            \"raw_grads\": (fbirn_grads_0, fbirn_grads_1),\n",
    "        },\n",
    "        \"bsnip\": {\n",
    "            \"raw_grads\": (bsnip_grads_0, bsnip_grads_1),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for dataset in data:\n",
    "        data[dataset][\"grads\"] = (compute_fnc(data[dataset][\"raw_grads\"][0]), compute_fnc(data[dataset][\"raw_grads\"][1]))\n",
    "        for data_type in [\"grads\"]:\n",
    "            pvals, tvals, mask = run_ttest(data[dataset][data_type][0], data[dataset][data_type][1])\n",
    "            data[dataset][data_type+\"_pvals\"] = pvals\n",
    "            data[dataset][data_type+\"_tvals\"] = tvals\n",
    "            data[dataset][data_type+\"_mask\"] = mask\n",
    "\n",
    "            data[dataset][data_type+\"_mean\"] = np.mean(np.concatenate((data[dataset][data_type][0], data[dataset][data_type][0]), axis=0), axis=0)\n",
    "    \n",
    "    primary_dataset = \"fbirn\" if \"fbirn\" in figure else \"bsnip\"\n",
    "    secondary_dataset = \"fbirn\" if primary_dataset == \"bsnip\" else \"bsnip\"\n",
    "\n",
    "    primary_mask = data[primary_dataset][data_type+\"_mask\"]\n",
    "    secondary_mask = data[secondary_dataset][data_type+\"_mask\"]\n",
    "\n",
    "    plot_index = 0\n",
    "\n",
    "    ims.append(ax[index][plot_index].imshow(primary_mask, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"{figure} {primary_dataset} mask\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    ims.append(ax[index][plot_index].imshow(secondary_mask, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4) \n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"{figure} {secondary_dataset} mask\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    plot_data = data[primary_dataset][data_tested+\"_tvals\"] * data[primary_dataset][data_type+\"_mask\"] \n",
    "    data_max = np.max(np.abs(plot_data))\n",
    "    ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-data_max, data_max))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4) \n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"{primary_dataset} T-vals x {primary_dataset} mask\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    plot_data = data[secondary_dataset][data_tested+\"_tvals\"] * data[secondary_dataset][data_type+\"_mask\"] \n",
    "    data_max = np.max(np.abs(plot_data))\n",
    "    ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-data_max, data_max))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"{secondary_dataset} T-vals x {secondary_dataset} mask\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    plot_data = np.logical_and(data[primary_dataset][data_type+\"_mask\"], data[secondary_dataset][data_type+\"_mask\"])\n",
    "    data_max = np.max(np.abs(plot_data))\n",
    "    ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"AND({primary_dataset} mask, {secondary_dataset} mask)\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    plot_data = np.logical_and(np.logical_not(data[primary_dataset][data_type+\"_mask\"]), data[secondary_dataset][data_type+\"_mask\"])\n",
    "    data_max = np.max(np.abs(plot_data))\n",
    "    ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"AND(NOT({primary_dataset} mask), {secondary_dataset} mask)\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    plot_data_and = data[secondary_dataset][data_tested+\"_tvals\"] * np.logical_and(data[primary_dataset][data_type+\"_mask\"], data[secondary_dataset][data_type+\"_mask\"])\n",
    "    plot_data_andnot = data[secondary_dataset][data_tested+\"_tvals\"] * np.logical_and(np.logical_not(data[primary_dataset][data_type+\"_mask\"]), data[secondary_dataset][data_type+\"_mask\"])\n",
    "    plot_data = np.tril(plot_data_and) + np.triu(plot_data_andnot)\n",
    "    data_max = np.max(np.abs(plot_data))\n",
    "    ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-data_max, data_max))\n",
    "    ax[index][plot_index].set_xticks([], [])\n",
    "    ax[index][plot_index].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "    ax[index][plot_index].axis(\"off\")\n",
    "    ax[index][plot_index].set_title(f\"TRI_L: {secondary_dataset} T-vals x AND({primary_dataset} mask, {secondary_dataset} mask)\\n TRI_U: {secondary_dataset} T-vals x AND(NOT({primary_dataset} mask), {secondary_dataset} mask)\", fontsize = 6)\n",
    "    plot_index += 1\n",
    "\n",
    "    # plot_data = data[secondary_dataset][data_tested+\"_tvals\"] * np.logical_and(np.logical_not(data[primary_dataset][data_type+\"_mask\"]), data[secondary_dataset][data_type+\"_mask\"])\n",
    "    # data_max = np.max(np.abs(plot_data))\n",
    "    # ims.append(ax[index][plot_index].imshow(plot_data, cmap=\"seismic\"))\n",
    "    # ims[-1].set(clim=(-data_max, data_max))\n",
    "    # ax[index][plot_index].set_xticks([], [])\n",
    "    # ax[index][plot_index].set_yticks([], [])\n",
    "    # fig.colorbar(ims[-1],fraction=0.046, pad=0.04).ax.tick_params(labelsize=4)\n",
    "    # ax[index][plot_index].axis(\"off\")\n",
    "    # ax[index][plot_index].set_title(f\"{secondary_dataset} T-vals x AND(NOT({primary_dataset} mask), {secondary_dataset} mask)\", fontsize = 6)\n",
    "    # plot_index += 1\n",
    "\n",
    "fig.savefig(f\"test_masks.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fnc(data):\n",
    "    new_data = np.zeros((data.shape[0], data.shape[2], data.shape[2]))\n",
    "    for i in range(new_data.shape[0]):\n",
    "        new_data[i, :, :] = np.corrcoef(data[i, :, :], rowvar=False)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def run_ttest(data1, data2):\n",
    "    pvals = []\n",
    "    tvals = []\n",
    "    for i in range(data1.shape[1]):\n",
    "        pvals.append([])\n",
    "        tvals.append([])\n",
    "        for j in range(data1.shape[1]):\n",
    "            test = stats.ttest_ind(\n",
    "                data1[:, i, j], \n",
    "                data2[:, i, j],\n",
    "                equal_var=False\n",
    "            )\n",
    "            pvals[-1].append(test.pvalue)\n",
    "            tvals[-1].append(test.statistic)\n",
    "        pvals[-1] = np.array(pvals[-1])\n",
    "        tvals[-1] = np.array(tvals[-1])\n",
    "    pvals = np.array(pvals)\n",
    "    tvals = np.array(tvals)\n",
    "\n",
    "    #run FDR correction\n",
    "    pvals[np.diag_indices_from(pvals)] = 1\n",
    "    pvals[np.triu_indices_from(pvals, k=1)] = stats.false_discovery_control(pvals[np.triu_indices_from(pvals, k=1)], method='by')\n",
    "    pvals_triu = np.triu(pvals, k=1)\n",
    "    pvals[np.tril_indices_from(pvals, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "\n",
    "    #compute mask from pvals\n",
    "    mask = np.ones_like(pvals)\n",
    "    mask[pvals < 0.05] = 1\n",
    "    mask[pvals >= 0.05] = 0\n",
    "\n",
    "    return pvals, tvals, mask\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"cobre_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/bsnip\"),\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\"),\n",
    "}\n",
    "\n",
    "data_tested = \"features\"\n",
    "# data_tested = \"grads\"\n",
    "\n",
    "fig, ax = plt.subplots(3, 8, figsize=(24, 9))\n",
    "ims = []\n",
    "for index, (figure, paths) in enumerate(plots.items()):\n",
    "    \n",
    "    fbirn_grads_0 = np.load(f\"{paths[0]}_grads_0.npy\")\n",
    "    fbirn_grads_1 = np.load(f\"{paths[0]}_grads_1.npy\")\n",
    "    bsnip_grads_0 = np.load(f\"{paths[1]}_grads_0.npy\")\n",
    "    bsnip_grads_1 = np.load(f\"{paths[1]}_grads_1.npy\")\n",
    "    fbirn_features_0 = np.load(f\"{paths[0]}_features_0.npy\")\n",
    "    fbirn_features_1 = np.load(f\"{paths[0]}_features_1.npy\")\n",
    "    bsnip_features_0 = np.load(f\"{paths[1]}_features_0.npy\")\n",
    "    bsnip_features_1 = np.load(f\"{paths[1]}_features_1.npy\")\n",
    "\n",
    "    data = {\n",
    "        \"fbirn\": {\n",
    "            \"raw_grads\": (fbirn_grads_0, fbirn_grads_1),\n",
    "            \"raw_features\":  (fbirn_features_0, fbirn_features_1)\n",
    "        },\n",
    "        \"bsnip\": {\n",
    "            \"raw_grads\": (bsnip_grads_0, bsnip_grads_1),\n",
    "            \"raw_features\":  (bsnip_features_0, bsnip_features_1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for dataset in data:\n",
    "        data[dataset][\"grads\"] = (compute_fnc(data[dataset][\"raw_grads\"][0]), compute_fnc(data[dataset][\"raw_grads\"][1]))\n",
    "        data[dataset][\"features\"] = (compute_fnc(data[dataset][\"raw_features\"][0]), compute_fnc(data[dataset][\"raw_features\"][1]))\n",
    "        for data_type in [\"grads\", \"features\"]:\n",
    "            pvals, tvals, mask = run_ttest(data[dataset][data_type][0], data[dataset][data_type][1])\n",
    "            data[dataset][data_type+\"_pvals\"] = pvals\n",
    "            data[dataset][data_type+\"_tvals\"] = tvals\n",
    "            data[dataset][data_type+\"_mask\"] = mask\n",
    "\n",
    "            data[dataset][data_type+\"_mean\"] = np.mean(np.concatenate((data[dataset][data_type][0], data[dataset][data_type][0]), axis=0), axis=0)\n",
    "    \n",
    "    print(figure + \"data shape\")\n",
    "    for dataset in data:\n",
    "        for key in data[dataset]:\n",
    "            if key in [\"raw_grads\", \"raw_features\", \"grads\", \"features\"]:\n",
    "                print(f\"{dataset} {key}: {data[dataset][key][0].shape}, {data[dataset][key][1].shape}\")\n",
    "            else:\n",
    "                print(f\"{dataset} {key}: {data[dataset][key].shape}\")\n",
    "    print()\n",
    "\n",
    "    ims.append(ax[index][0].imshow(data[\"fbirn\"][data_tested+\"_mean\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][0].set_xticks([], [])\n",
    "    ax[index][0].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][0].axis(\"off\")\n",
    "    ax[index][0].set_title(f\"{figure} FBIRN corr mean\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][1].imshow(data[\"bsnip\"][data_tested+\"_mean\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][1].set_xticks([], [])\n",
    "    ax[index][1].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][1].axis(\"off\")\n",
    "    ax[index][1].set_title(f\"{figure} BSNIP corr mean\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][2].imshow(data[\"fbirn\"][data_tested+\"_pvals\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][2].set_xticks([], [])\n",
    "    ax[index][2].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][2].axis(\"off\")\n",
    "    ax[index][2].set_title(f\"{figure} FBIRN P-vals\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][3].imshow(data[\"bsnip\"][data_tested+\"_pvals\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][3].set_xticks([], [])\n",
    "    ax[index][3].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][3].axis(\"off\")\n",
    "    ax[index][3].set_title(f\"{figure} BSNIP P-vals\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][4].imshow(data[\"fbirn\"][data_tested+\"_mask\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][4].set_xticks([], [])\n",
    "    ax[index][4].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][4].axis(\"off\")\n",
    "    ax[index][4].set_title(f\"{figure} FBIRN mask\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][5].imshow(data[\"bsnip\"][data_tested+\"_mask\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][5].set_xticks([], [])\n",
    "    ax[index][5].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][5].axis(\"off\")\n",
    "    ax[index][5].set_title(f\"{figure} BSNIP mask\", fontsize = 8)\n",
    "\n",
    "\n",
    "    fbirn_filtered_max_val = np.max(np.abs(data[\"fbirn\"][data_tested+\"_tvals\"]*data[\"fbirn\"][data_tested+\"_mask\"]))\n",
    "    ims.append(ax[index][6].imshow(data[\"fbirn\"][data_tested+\"_tvals\"]*data[\"fbirn\"][data_tested+\"_mask\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-fbirn_filtered_max_val, fbirn_filtered_max_val))\n",
    "    ax[index][6].set_xticks([], [])\n",
    "    ax[index][6].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][6].axis(\"off\")\n",
    "    ax[index][6].set_title(f\"{figure} FBIRN T-vals masked\", fontsize = 8)\n",
    "\n",
    "    bsnip_filtered_max_val = np.max(np.abs(data[\"bsnip\"][data_tested+\"_tvals\"]*data[\"bsnip\"][data_tested+\"_mask\"]))\n",
    "    ims.append(ax[index][7].imshow(data[\"bsnip\"][data_tested+\"_tvals\"]*data[\"bsnip\"][data_tested+\"_mask\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-bsnip_filtered_max_val, bsnip_filtered_max_val))\n",
    "    ax[index][7].set_xticks([], [])\n",
    "    ax[index][7].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][7].axis(\"off\")\n",
    "    ax[index][7].set_title(f\"{figure} BSNIP T-vals maksed\", fontsize = 8)\n",
    "\n",
    "\n",
    "fig.savefig(f\"test_{data_tested}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.fbirn import load_data as load_data_fbirn\n",
    "from src.datasets.bsnip import load_data as load_data_bsnip\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"cobre_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/bsnip\"),\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test\"),\n",
    "}\n",
    "\n",
    "data_tested = \"features\"\n",
    "# data_tested = \"grads\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 8, figsize=(24, 3))\n",
    "ims = []\n",
    "for index in range(1):\n",
    "    \n",
    "    figure = \"Loaded features\"\n",
    "    cfg = DictConfig({\"dataset\": {\"filter_indices\": True}})\n",
    "    fbirn_features, fbirn_labels = load_data_fbirn(cfg)\n",
    "    bsnip_features, bsnip_labels = load_data_bsnip(cfg)\n",
    "\n",
    "\n",
    "    fbirn_features_0 = fbirn_features[fbirn_labels == 0]\n",
    "    fbirn_features_1 = fbirn_features[fbirn_labels == 1]\n",
    "    bsnip_features_0 = bsnip_features[bsnip_labels == 0]\n",
    "    bsnip_features_1 = bsnip_features[bsnip_labels == 1]\n",
    "\n",
    "    data = {\n",
    "        \"fbirn\": {\n",
    "            # \"raw_grads\": (fbirn_grads_0, fbirn_grads_1),\n",
    "            \"raw_features\":  (fbirn_features_0, fbirn_features_1)\n",
    "        },\n",
    "        \"bsnip\": {\n",
    "            # \"raw_grads\": (bsnip_grads_0, bsnip_grads_1),\n",
    "            \"raw_features\":  (bsnip_features_0, bsnip_features_1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for dataset in data:\n",
    "        # data[dataset][\"grads\"] = (compute_fnc(data[dataset][\"raw_grads\"][0]), compute_fnc(data[dataset][\"raw_grads\"][1]))\n",
    "        data[dataset][\"features\"] = (compute_fnc(data[dataset][\"raw_features\"][0]), compute_fnc(data[dataset][\"raw_features\"][1]))\n",
    "        for data_type in [\"features\"]:\n",
    "            pvals, tvals, mask = run_ttest(data[dataset][data_type][0], data[dataset][data_type][1])\n",
    "            data[dataset][data_type+\"_pvals\"] = pvals\n",
    "            data[dataset][data_type+\"_tvals\"] = tvals\n",
    "            data[dataset][data_type+\"_mask\"] = mask\n",
    "\n",
    "            data[dataset][data_type+\"_mean\"] = np.mean(np.concatenate((data[dataset][data_type][0], data[dataset][data_type][0]), axis=0), axis=0)\n",
    "    \n",
    "    print(figure + \"data shape\")\n",
    "    for dataset in data:\n",
    "        for key in data[dataset]:\n",
    "            if key in [\"raw_grads\", \"raw_features\", \"grads\", \"features\"]:\n",
    "                print(f\"{dataset} {key}: {data[dataset][key][0].shape}, {data[dataset][key][1].shape}\")\n",
    "            else:\n",
    "                print(f\"{dataset} {key}: {data[dataset][key].shape}\")\n",
    "    print()\n",
    "\n",
    "    ax = [ax]\n",
    "    ims.append(ax[index][0].imshow(data[\"fbirn\"][data_tested+\"_mean\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][0].set_xticks([], [])\n",
    "    ax[index][0].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][0].axis(\"off\")\n",
    "    ax[index][0].set_title(f\"{figure} FBIRN corr mean\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][1].imshow(data[\"bsnip\"][data_tested+\"_mean\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-1.0, 1.0))\n",
    "    ax[index][1].set_xticks([], [])\n",
    "    ax[index][1].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][1].axis(\"off\")\n",
    "    ax[index][1].set_title(f\"{figure} BSNIP corr mean\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][2].imshow(data[\"fbirn\"][data_tested+\"_pvals\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][2].set_xticks([], [])\n",
    "    ax[index][2].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][2].axis(\"off\")\n",
    "    ax[index][2].set_title(f\"{figure} FBIRN P-vals\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][3].imshow(data[\"bsnip\"][data_tested+\"_pvals\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 0.05))\n",
    "    ax[index][3].set_xticks([], [])\n",
    "    ax[index][3].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][3].axis(\"off\")\n",
    "    ax[index][3].set_title(f\"{figure} BSNIP P-vals\", fontsize = 8)\n",
    "\n",
    "\n",
    "    ims.append(ax[index][4].imshow(data[\"fbirn\"][data_tested+\"_mask\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][4].set_xticks([], [])\n",
    "    ax[index][4].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][4].axis(\"off\")\n",
    "    ax[index][4].set_title(f\"{figure} FBIRN mask\", fontsize = 8)\n",
    "\n",
    "    ims.append(ax[index][5].imshow(data[\"bsnip\"][data_tested+\"_mask\"], cmap=\"inferno\"))\n",
    "    ims[-1].set(clim=(0, 1))\n",
    "    ax[index][5].set_xticks([], [])\n",
    "    ax[index][5].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][5].axis(\"off\")\n",
    "    ax[index][5].set_title(f\"{figure} BSNIP mask\", fontsize = 8)\n",
    "\n",
    "\n",
    "    fbirn_filtered_max_val = np.max(np.abs(data[\"fbirn\"][data_tested+\"_tvals\"]*data[\"fbirn\"][data_tested+\"_mask\"]))\n",
    "    ims.append(ax[index][6].imshow(data[\"fbirn\"][data_tested+\"_tvals\"]*data[\"fbirn\"][data_tested+\"_mask\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-fbirn_filtered_max_val, fbirn_filtered_max_val))\n",
    "    ax[index][6].set_xticks([], [])\n",
    "    ax[index][6].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][6].axis(\"off\")\n",
    "    ax[index][6].set_title(f\"{figure} FBIRN T-vals masked\", fontsize = 8)\n",
    "\n",
    "    bsnip_filtered_max_val = np.max(np.abs(data[\"bsnip\"][data_tested+\"_tvals\"]*data[\"bsnip\"][data_tested+\"_mask\"]))\n",
    "    ims.append(ax[index][7].imshow(data[\"bsnip\"][data_tested+\"_tvals\"]*data[\"bsnip\"][data_tested+\"_mask\"], cmap=\"seismic\"))\n",
    "    ims[-1].set(clim=(-bsnip_filtered_max_val, bsnip_filtered_max_val))\n",
    "    ax[index][7].set_xticks([], [])\n",
    "    ax[index][7].set_yticks([], [])\n",
    "    fig.colorbar(ims[-1])\n",
    "    ax[index][7].axis(\"off\")\n",
    "    ax[index][7].set_title(f\"{figure} BSNIP T-vals maksed\", fontsize = 8)\n",
    "\n",
    "\n",
    "fig.savefig(f\"test_loaded_features.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"Mann-Whitney U rank test\", \"T-test\",\"Kolmogorov-Smirnov test\",\"Brunner-Munzel test\"]\n",
    "\n",
    "method = \"ignt\"\n",
    "plots = {\n",
    "    \"cobre_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/fbirn_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/bsnip_spatial_fnc_\"),\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip_spatial_fnc_\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test_spatial_fnc_\"),\n",
    "}\n",
    "\n",
    "for figure, paths in plots.items():\n",
    "    fig, ax = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    ims = []\n",
    "\n",
    "    for i, test in enumerate(tests):\n",
    "        fbirn = np.load(f\"{paths[0]}{test}.npy\")\n",
    "        bsnip = np.load(f\"{paths[1]}{test}.npy\")\n",
    "\n",
    "        fbirn[fbirn > 0.05] = 1\n",
    "        bsnip[bsnip > 0.05] = 1\n",
    "\n",
    "        ims.append(ax[i][0].imshow(fbirn, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][0].set_xticks([], [])\n",
    "        ax[i][0].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][0].axis(\"off\")\n",
    "        ax[i][0].set_title(f\"{test} FBIRN\")\n",
    "        \n",
    "        ims.append(ax[i][1].imshow(bsnip, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][1].set_xticks([], [])\n",
    "        ax[i][1].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][1].axis(\"off\")\n",
    "        ax[i][1].set_title(f\"{test} BSNIP\")\n",
    "\n",
    "        # fbirn[fbirn > 0.99] = 0\n",
    "        # bsnip[bsnip > 0.99] = 0\n",
    "\n",
    "        ims.append(ax[i][2].imshow(fbirn-bsnip, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][2].set_xticks([], [])\n",
    "        ax[i][2].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][2].axis(\"off\")\n",
    "        ax[i][2].set_title(f\"{test} diff\")\n",
    "\n",
    "    fig.savefig(f\"{figure}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"Mann-Whitney U rank test\", \"T-test\",\"Kolmogorov-Smirnov test\",\"Brunner-Munzel test\"]\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"fbirn\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn_spatial_fnc_\"),\n",
    "    \"bsnip\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip_spatial_fnc_\"),\n",
    "}\n",
    "\n",
    "for figure, paths in plots.items():\n",
    "    fig, ax = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    ims = []\n",
    "\n",
    "    for i, test in enumerate(tests):\n",
    "        orig = np.load(f\"{paths[0]}{test}.npy\")\n",
    "        transfer = np.load(f\"{paths[1]}{test}.npy\")\n",
    "\n",
    "        orig[orig > 0.05] = 1\n",
    "        transfer[transfer > 0.05] = 1\n",
    "\n",
    "        ims.append(ax[i][0].imshow(orig, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][0].set_xticks([], [])\n",
    "        ax[i][0].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][0].axis(\"off\")\n",
    "        ax[i][0].set_title(f\"{test} {figure} Orig\")\n",
    "        \n",
    "        ims.append(ax[i][1].imshow(transfer, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][1].set_xticks([], [])\n",
    "        ax[i][1].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][1].axis(\"off\")\n",
    "        ax[i][1].set_title(f\"{test} {figure} Transfer\")\n",
    "\n",
    "        # orig[orig > 0.99] = 0\n",
    "        # transfer[transfer > 0.99] = 0\n",
    "\n",
    "        ims.append(ax[i][2].imshow(orig-transfer, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][2].set_xticks([], [])\n",
    "        ax[i][2].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][2].axis(\"off\")\n",
    "        ax[i][2].set_title(f\"{test} diff\")\n",
    "\n",
    "    fig.savefig(f\"{figure}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"Mann-Whitney U rank test\", \"T-test\",\"Kolmogorov-Smirnov test\",\"Brunner-Munzel test\"]\n",
    "\n",
    "method = \"saliency\"\n",
    "plots = {\n",
    "    \"cobre_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/fbirn_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-cobre/{method}/bsnip_spatial_fnc_\"),\n",
    "    \"fbirn_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/test_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-fbirn/{method}/bsnip_spatial_fnc_\"),\n",
    "    \"bsnip_trained\": (f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/fbirn_spatial_fnc_\", f\"./assets/introspection/rerun_all2-exp-rearranged_mlp_defHP-bsnip/{method}/test_spatial_fnc_\"),\n",
    "}\n",
    "\n",
    "for figure, paths in plots.items():\n",
    "    fig, ax = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    ims = []\n",
    "\n",
    "    for i, test in enumerate(tests):\n",
    "        fbirn = np.load(f\"{paths[0]}{test}.npy\")\n",
    "        bsnip = np.load(f\"{paths[1]}{test}.npy\")\n",
    "\n",
    "        fbirn = np.log(0.05 / fbirn)\n",
    "        bsnip = np.log(0.05 / bsnip)\n",
    "        fbirn[fbirn < 0] = 0\n",
    "        bsnip[bsnip < 0] = 0\n",
    "        fbirn /= np.max(fbirn)\n",
    "        bsnip /= np.max(bsnip)\n",
    "\n",
    "        ims.append(ax[i][0].imshow(fbirn, cmap=\"inferno\"))\n",
    "        # ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][0].set_xticks([], [])\n",
    "        ax[i][0].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][0].axis(\"off\")\n",
    "        ax[i][0].set_title(f\"{test} FBIRN\")\n",
    "        \n",
    "        ims.append(ax[i][1].imshow(bsnip, cmap=\"inferno\"))\n",
    "        # ims[-1].set(clim=(-0.05, 0.05))\n",
    "        ax[i][1].set_xticks([], [])\n",
    "        ax[i][1].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][1].axis(\"off\")\n",
    "        ax[i][1].set_title(f\"{test} BSNIP\")\n",
    "\n",
    "        # fbirn[fbirn > 0.99] = 0\n",
    "        # bsnip[bsnip > 0.99] = 0\n",
    "\n",
    "        diff = fbirn-bsnip\n",
    "        max_abs_diff = np.max(np.abs(diff))\n",
    "        ims.append(ax[i][2].imshow(diff, cmap=\"seismic\"))\n",
    "        ims[-1].set(clim=(-max_abs_diff, max_abs_diff))\n",
    "        ax[i][2].set_xticks([], [])\n",
    "        ax[i][2].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[i][2].axis(\"off\")\n",
    "        ax[i][2].set_title(f\"{test} diff\")\n",
    "\n",
    "    fig.savefig(f\"{figure}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Introspector:\n",
    "    \"\"\"Basic introspector\"\"\"\n",
    "\n",
    "    def __init__(self, model, features, labels, methods, save_path) -> None:\n",
    "        self.methods = methods\n",
    "        self.save_path = save_path\n",
    "        self.model = model\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        # os.makedirs(f\"{self.save_path}timeseries\", exist_ok=True)\n",
    "        if \"saliency\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}saliency/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}saliency/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}saliency\", exist_ok=True)\n",
    "        if \"ig\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}ig/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}ig/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}ig\", exist_ok=True)\n",
    "        if \"ignt\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}ignt/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}ignt/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}ignt\", exist_ok=True)\n",
    "\n",
    "    def run(self, cutoff=1, percentile=0.1):\n",
    "        \"\"\"Run introspection, save results\"\"\"\n",
    "        targets = torch.unique(self.labels)\n",
    "        n_classes = targets.shape[0]\n",
    "\n",
    "        print(f\"Getting MLP predictions for {self.save_path}\")\n",
    "        features, timeseries, smooth_timeseries, predictions = (\n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes,\n",
    "        )\n",
    "\n",
    "        window_size = 10\n",
    "        for class_idx in range(n_classes):\n",
    "            filter_array = self.labels == class_idx\n",
    "            features[class_idx] = self.features[filter_array]\n",
    "            features[class_idx].requires_grad = False\n",
    "\n",
    "            timeseries_raw, predictions[class_idx] = self.model(features[class_idx], introspection=True)\n",
    "            timeseries_raw = timeseries_raw.cpu().detach().numpy()\n",
    "            predictions[class_idx] = predictions[class_idx].cpu().detach().numpy()\n",
    "            \n",
    "            # timeseries_raw was calculated for the data related to class_idx;\n",
    "            # yet it has `n_classes` class prediction time series in 3rd dimension\n",
    "            # so timeseries[class_idx] is a list of length n_classes, each element of which\n",
    "            # represents the class prediction time series\n",
    "            timeseries[class_idx] = [timeseries_raw[:, :, j] for j in range(n_classes)]\n",
    "\n",
    "            smooth_timeseries[class_idx] = deepcopy(timeseries[class_idx])\n",
    "            for j in range(n_classes):\n",
    "                for subject in range(smooth_timeseries[class_idx][j].shape[0]):\n",
    "                    smooth_timeseries[class_idx][j][subject] = np.convolve(smooth_timeseries[class_idx][j][subject], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "        # all_features = torch.concat(features)\n",
    "        # features = [all_features, all_features]\n",
    "\n",
    "        grads = {key: None for key in self.methods}\n",
    "        specific_grads = {key: None for key in self.methods}\n",
    "        for method in self.methods:\n",
    "            print(f\"Processing {method}\")\n",
    "\n",
    "            sizes = [features[j].shape for j in range(n_classes)]\n",
    "            # print(features[0].shape)\n",
    "            # print(features[0].reshape(-1, 1, 53).shape)\n",
    "            # grads[method] = [self.get_grads(method, features[j].reshape(-1, 1, 53), j).cpu().detach().numpy().reshape(*sizes[j]) for j in range(n_classes)]\n",
    "            grads[method] = [self.get_grads(method, features[j], j).cpu().detach().numpy() for j in range(n_classes)]\n",
    "            print(grads[method][0].shape)\n",
    "\n",
    "            np.save(f\"{self.save_path}{method}/grads_0.npy\", grads[method][0])\n",
    "            np.save(f\"{self.save_path}{method}/grads_1.npy\", grads[method][1])\n",
    "\n",
    "            specific_grads[method] = []\n",
    "            for i in range(n_classes):\n",
    "                if i == 0:\n",
    "                    filter_array = np.argwhere(timeseries[i][0] > timeseries[i][1])\n",
    "                else:\n",
    "                    filter_array = np.argwhere(timeseries[i][0] < timeseries[i][1])\n",
    "                \n",
    "                specific_grads[method].append(np.array([grads[method][i][x, y, :] for (x, y) in filter_array]))\n",
    "\n",
    "            # print(specific_grads[method][0].shape)\n",
    "            # print(grads[method][0].shape)\n",
    "            np.save(f\"{self.save_path}{method}/filtered_grads_0.npy\", specific_grads[method][0])\n",
    "            np.save(f\"{self.save_path}{method}/filtered_grads_1.npy\", specific_grads[method][1])\n",
    "\n",
    "        # plot everything\n",
    "        for method in self.methods:\n",
    "            print(f\"Plotting {method}\")\n",
    "\n",
    "            # for i in range(n_classes):\n",
    "            #     print(f\"Plotting time series for true target {i}\")\n",
    "            #     self.plot_timeseries(\n",
    "            #         timeseries=timeseries[i], \n",
    "            #         smooth_timeseries=smooth_timeseries[i], \n",
    "            #         grads=grads[method][i],\n",
    "            #         features=features[i],\n",
    "            #         predictions=predictions[i],\n",
    "            #         target=i,\n",
    "            #         cutoff=cutoff, \n",
    "            #         filepath=f\"{self.save_path}{method}/timeseries_target_{i}\"\n",
    "            #     )\n",
    "\n",
    "            print(f\"Plotting spatial attention\")\n",
    "            self.plot_histograms(\n",
    "                grads=grads[method],\n",
    "                # grads=specific_grads[method],\n",
    "                features=features,\n",
    "                cutoff=cutoff, \n",
    "                filepath=f\"{self.save_path}{method}/spatial\",\n",
    "                method=method,\n",
    "                percentile=percentile\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def plot_timeseries(self, timeseries, smooth_timeseries, grads, features, predictions, target, filepath, cutoff, use_log=False):\n",
    "        fig, ax = plt.subplots(2, cutoff,figsize=(3*cutoff, 4))\n",
    "\n",
    "        x = np.arange(timeseries[0].shape[1])\n",
    "        for i in range(cutoff):\n",
    "            smooth_line_0 = ax[0][i].plot(x[:140], smooth_timeseries[0][i][:140], label='0', color='blue')\n",
    "            smooth_line_1 = ax[0][i].plot(x[:140], smooth_timeseries[1][i][:140], label='1', color='red')\n",
    "            line_0 = ax[0][i].plot(x[:140], timeseries[0][i][:140], '#00009922')\n",
    "            line_1 = ax[0][i].plot(x[:140], timeseries[1][i][:140], '#99000022')\n",
    "\n",
    "            # ax[i].set_title(f\"True target = {target}, Predicted target = {predictions[i]}\")\n",
    "            ax[0][i].legend(title=\"Prediction\")\n",
    "            \n",
    "            ax[0][i].set_xlim(0, 140)\n",
    "            ax[0][i].set_xticklabels([])\n",
    "            ax[0][i].set_xticks([])\n",
    "            ax[0][i].set_yticklabels([])\n",
    "            ax[0][i].set_yticks([])\n",
    "            ax[0][i].set_xlabel(\"Time\")\n",
    "            ax[0][i].set_ylabel(\"Prediction strength\" if i == 0 else \"\")\n",
    "\n",
    "            # grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            feat = features[i].cpu().detach().numpy()[np.newaxis, :, :]\n",
    "            _, _ = viz.visualize_image_attr(\n",
    "                np.transpose(grad, (2, 1, 0))[:,:140, :],\n",
    "                np.transpose(feat, (2, 1, 0))[:,:140, :],\n",
    "                method=\"heat_map\",\n",
    "                cmap=\"inferno\",\n",
    "                show_colorbar=False,\n",
    "                plt_fig_axis=(fig, ax[1][i]),\n",
    "                use_pyplot=False,\n",
    "            )\n",
    "            ax[0][i].grid()\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        \n",
    "    def plot_histograms(self, grads, features, cutoff, filepath, method, percentile=0.1):\n",
    "        sns.reset_defaults()\n",
    "\n",
    "        data_0 = grads[0]\n",
    "        data_1 = grads[1]\n",
    "\n",
    "        corr_0 = np.corrcoef(grads[0].reshape(-1, grads[0].shape[2]).T)\n",
    "        corr_1 = np.corrcoef(grads[1].reshape(-1, grads[1].shape[2]).T)\n",
    "        difcor = np.abs(corr_0-corr_1)\n",
    "\n",
    "        print(\"raw data shape\")\n",
    "        print(data_0.shape)\n",
    "\n",
    "        pearson_0 = np.zeros((data_0.shape[0], data_0.shape[2], data_0.shape[2]))\n",
    "        pearson_1 = np.zeros((data_1.shape[0], data_1.shape[2], data_1.shape[2]))\n",
    "        for i in range(pearson_0.shape[0]):\n",
    "            pearson_0[i, :, :] = np.corrcoef(data_0[i, :, :], rowvar=False)\n",
    "        for i in range(pearson_1.shape[0]):\n",
    "            pearson_1[i, :, :] = np.corrcoef(data_1[i, :, :], rowvar=False)\n",
    "\n",
    "        # print(\"pearson_0 shape\")\n",
    "        # print(pearson_0.shape)\n",
    "        # print(\"pearson_1 shape\")\n",
    "        # print(pearson_1.shape)\n",
    "        \n",
    "        # mannwhitney U rank test\n",
    "        pvals_mw = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_mw.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.mannwhitneyu(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                )\n",
    "                    \n",
    "                pvals_mw[-1].append(test.pvalue)\n",
    "\n",
    "            pvals_mw[-1] = np.array(pvals_mw[-1])\n",
    "        pvals_mw = np.array(pvals_mw)\n",
    "\n",
    "        # ttest_ind test\n",
    "        pvals_ttest = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_ttest.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.ttest_ind(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                    equal_var=False\n",
    "                )\n",
    "                pvals_ttest[-1].append(test.pvalue)\n",
    "            pvals_ttest[-1] = np.array(pvals_ttest[-1])\n",
    "        pvals_ttest = np.array(pvals_ttest)\n",
    "        \n",
    "        # kstest\n",
    "        pvals_ks = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_ks.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.kstest(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                )\n",
    "                pvals_ks[-1].append(test.pvalue)\n",
    "            pvals_ks[-1] = np.array(pvals_ks[-1])\n",
    "        pvals_ks = np.array(pvals_ks)\n",
    "\n",
    "        # brunnermunzel\n",
    "        pvals_bm = []\n",
    "        for i in range(pearson_0.shape[1]):\n",
    "            pvals_bm.append([])\n",
    "            for j in range(pearson_0.shape[1]):\n",
    "                test = stats.kstest(\n",
    "                    pearson_0[:, i, j], \n",
    "                    pearson_1[:, i, j],\n",
    "                )\n",
    "                pvals_bm[-1].append(test.pvalue)\n",
    "            pvals_bm[-1] = np.array(pvals_bm[-1])\n",
    "        pvals_bm = np.array(pvals_bm)\n",
    "\n",
    "        tests = [\n",
    "            (\"Mann-Whitney U rank test\", pvals_mw), \n",
    "            (\"T-test\", pvals_ttest),\n",
    "            (\"Kolmogorov-Smirnov test\", pvals_ks),\n",
    "            (\"Brunner-Munzel test\", pvals_bm),\n",
    "        ]\n",
    "\n",
    "        pearson_0 = pearson_0.mean(axis=0)\n",
    "        pearson_1 = pearson_1.mean(axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(5, 3, figsize=(15, 25))\n",
    "        ims = []\n",
    "\n",
    "        ims.append(ax[0][0].imshow(pearson_0, cmap=corr_cmap))\n",
    "        ims[-1].set(clim=(-1.0, 1.0))\n",
    "        ax[0][0].set_xticks([], [])\n",
    "        ax[0][0].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[0][0].axis(\"off\")\n",
    "        ax[0][0].set_title(\"FNC class 0\")\n",
    "        \n",
    "        ims.append(ax[0][1].imshow(pearson_0, cmap=corr_cmap))\n",
    "        ims[-1].set(clim=(-1.0, 1.0))\n",
    "        ax[0][1].set_xticks([], [])\n",
    "        ax[0][1].set_yticks([], [])\n",
    "        fig.colorbar(ims[-1])\n",
    "        ax[0][1].axis(\"off\")\n",
    "        ax[0][1].set_title(\"FNC class 1\")\n",
    "\n",
    "        for test_idx in range(4):\n",
    "            # correction\n",
    "            pvals = tests[test_idx][1]\n",
    "            # pvals = pvals.reshape(corr_shape, corr_shape)\n",
    "\n",
    "            # setting to 1 manually, tests are sensitive to the rounding error variance\n",
    "            pvals[np.diag_indices(pvals.shape[0])] = 1\n",
    "            pvals[np.triu_indices_from(pvals, k=1)] = stats.false_discovery_control(pvals[np.triu_indices_from(pvals, k=1)], method='by')\n",
    "            pvals_triu = np.triu(pvals, k=1)\n",
    "            pvals[np.tril_indices_from(pvals, k=-1)] = pvals_triu.T[np.tril_indices_from(pvals_triu, k=-1)]\n",
    "\n",
    "            ims.append(ax[1+test_idx][0].matshow(pvals, cmap=corr_cmap))\n",
    "            ax[1+test_idx][0].set_xticks([], [])\n",
    "            ax[1+test_idx][0].set_yticks([], [])\n",
    "            fig.colorbar(ims[-1])\n",
    "            ax[1+test_idx][0].axis(\"off\")\n",
    "            ax[1+test_idx][0].set_title(f\"{tests[test_idx][0]}\")\n",
    "\n",
    "            ims.append(ax[1+test_idx][2].matshow(difcor, cmap=corr_cmap))\n",
    "            ax[1+test_idx][2].set_xticks([], [])\n",
    "            ax[1+test_idx][2].set_yticks([], [])\n",
    "            fig.colorbar(ims[-1])\n",
    "            ax[1+test_idx][2].axis(\"off\")\n",
    "            ax[1+test_idx][2].set_title(f\"Abs covar diff showed before\")\n",
    "\n",
    "            \n",
    "            pvals[pvals > 0.05] = 1\n",
    "            ims.append(ax[1+test_idx][1].matshow(pvals, cmap=corr_cmap))\n",
    "            ims[-1].set(clim=(0, 0.05))\n",
    "            ax[1+test_idx][1].set_xticks([], [])\n",
    "            ax[1+test_idx][1].set_yticks([], [])\n",
    "            fig.colorbar(ims[-1])\n",
    "            ax[1+test_idx][1].axis(\"off\")\n",
    "            ax[1+test_idx][1].set_title(f\"{tests[test_idx][0]} significant\")\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_fnc.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        # CONCAT FNC    \n",
    "        if len(grads[0].shape) == 3:\n",
    "            data_0 = grads[0].reshape(-1, grads[0].shape[2])\n",
    "            data_1 = grads[1].reshape(-1, grads[1].shape[2])\n",
    "            x = np.arange(grads[0].shape[2])\n",
    "        else:\n",
    "            data_0 = grads[0]\n",
    "            data_1 = grads[1]\n",
    "            x = np.arange(grads[0].shape[1])\n",
    "        \n",
    "        corr_0 = np.corrcoef(data_0.T)\n",
    "        corr_1 = np.corrcoef(data_1.T)\n",
    "\n",
    "        # print(\"data shape\")\n",
    "        # print(data_0.shape)\n",
    "        # print(\"corr shape\")\n",
    "        # print(corr_0.shape)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(corr_0, cmap=corr_cmap)\n",
    "        cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_cocat_fnc_0.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(corr_1, cmap=corr_cmap)\n",
    "        cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_cocat_fnc_1.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(np.abs(corr_0-corr_1), cmap=abs_corr_cmap)\n",
    "        # cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_cocat_fnc_diff.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        # fig, ax = plt.subplots(1, 4, figsize=(cutoff+3, 7))\n",
    "        n_panels = 2+4\n",
    "        fig, ax = plt.subplots(1, n_panels, figsize=(6*n_panels, 14))\n",
    "        # fig, ax = plt.subplots(1, 2, figsize=(12, 14))\n",
    "\n",
    "        if method != \"ignt\":\n",
    "            data_1 = data_1 * -1.0\n",
    "\n",
    "        df_0 = pd.DataFrame(data_0, columns=[i for i in range(data_0.shape[1])])\n",
    "        df_0['Class'] = 0\n",
    "        df_1 = pd.DataFrame(data_1, columns=[i for i in range(data_1.shape[1])])\n",
    "        df_1['Class'] = 1\n",
    "        df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "        df = pd.melt(df, id_vars=['Class'], var_name='Component', value_name='Data')\n",
    "        sns.boxplot(df, y=\"Component\", x=\"Data\", hue=\"Class\", ax=ax[0], palette=[\"blue\", \"red\"], showfliers = False, orient='h', linewidth=0.3)\n",
    "        ax[0].grid(axis='y', linestyle='--', alpha=0.3, linewidth=0.3)\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[0].set_yticks(x, x, fontsize=6)\n",
    "        ax[0].set_ylim(-3, 55)\n",
    "        ax[0].set_title(\"Gradients\")\n",
    "\n",
    "        # data has shape [observations, components]\n",
    "\n",
    "        # mannwhitney U rank test\n",
    "        test = stats.mannwhitneyu(data_0, data_1)\n",
    "        pvals_mw = test.pvalue\n",
    "\n",
    "        # ttest_ind test\n",
    "        test = stats.ttest_ind(\n",
    "            data_0, \n",
    "            data_1,\n",
    "            equal_var=False\n",
    "        )\n",
    "        pvals_ttest = test.pvalue\n",
    "        \n",
    "        # kstest\n",
    "        pvals_ks = []\n",
    "        for i in range(data_0.shape[1]):\n",
    "            test = stats.kstest(data_0[:, i], data_1[:, i])\n",
    "            pvals_ks.append(test.pvalue)\n",
    "        pvals_ks = np.array(pvals_ks)\n",
    "\n",
    "        # brunnermunzel\n",
    "        pvals_bm = []\n",
    "        for i in range(data_0.shape[1]):\n",
    "            test = stats.brunnermunzel(data_0[:, i], data_1[:, i])\n",
    "            pvals_bm.append(test.pvalue)\n",
    "        pvals_bm = np.array(pvals_bm)\n",
    "\n",
    "        tests = [\n",
    "            (\"Mann-Whitney U rank test\", pvals_mw), \n",
    "            (\"T-test\", pvals_ttest),\n",
    "            (\"Kolmogorov-Smirnov test\", pvals_ks),\n",
    "            (\"Brunner-Munzel test\", pvals_bm),\n",
    "        \n",
    "        ]\n",
    "        for test_idx in range(4):\n",
    "            # correction\n",
    "            pvals = tests[test_idx][1]\n",
    "            pvals = stats.false_discovery_control(pvals, method='by')\n",
    "            significant_p_vals = np.argwhere(pvals < 0.05)\n",
    "            # pvals = pvals / pvals.shape[0]\n",
    "\n",
    "            bars = ax[test_idx+1].barh(\n",
    "                x,\n",
    "                pvals,\n",
    "                align=\"center\",\n",
    "                color='blue',\n",
    "            )\n",
    "            ax[test_idx+1].set_yticks(x, [neuromark[k][0] if k in significant_p_vals else k for k in x], fontsize=6)\n",
    "            ax[test_idx+1].set_xlabel(\"P-value\")\n",
    "            ax[test_idx+1].set_xscale(\"log\")\n",
    "            ax[test_idx+1].set_title(tests[test_idx][0])\n",
    "\n",
    "\n",
    "            for i, bar in enumerate(bars):\n",
    "                if pvals[i] < 0.05:\n",
    "                    bar.set_color('red')\n",
    "            # bar charts: summarizes gradients at each component\n",
    "\n",
    "        data_0 = np.median(data_0, axis=0)\n",
    "        data_1 = np.median(data_1, axis=0)\n",
    "        \n",
    "        min_0, max_0 = np.min(data_0), np.max(data_0)\n",
    "        min_1, max_1 = np.min(data_1), np.max(data_1)\n",
    "        min_v, max_v = min(min_0, min_1), max(max_0, max_1)\n",
    "        if min_v <= 0:\n",
    "            max_v = max(abs(min_v), abs(max_v))\n",
    "            min_v = -1.0*max_v\n",
    "        else:\n",
    "            min_v = 0\n",
    "                \n",
    "        sym_data_0 = np.zeros(x.shape)\n",
    "        sym_data_1 = np.zeros(x.shape)\n",
    "        asym_data_0 = np.zeros(x.shape)\n",
    "        asym_data_1 = np.zeros(x.shape)\n",
    "\n",
    "        for j in x:\n",
    "            sym_data_0[j] = np.sign(data_0[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            sym_data_1[j] = np.sign(data_1[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            asym_data_0[j] = data_0[j] - sym_data_0[j]\n",
    "            asym_data_1[j] = data_1[j] - sym_data_1[j]\n",
    "\n",
    "        abs_grad = np.abs(data_0) + np.abs(data_1)\n",
    "        abs_sym_grad = np.abs(sym_data_0) + np.abs(sym_data_1)\n",
    "        abs_asym_grad = np.abs(asym_data_0) + np.abs(asym_data_1)\n",
    "        significant_comp = np.argsort(abs_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_sym_comp = np.argsort(abs_sym_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_asym_comp = np.argsort(abs_asym_grad)[-int(percentile*x.shape[0]):]\n",
    "\n",
    "        all_significant = np.intersect1d(significant_comp, np.intersect1d(significant_sym_comp, significant_asym_comp))\n",
    "        all_significant = {\n",
    "            \"idx\": all_significant,\n",
    "            \"name\": [neuromark[k][0] for k in all_significant],\n",
    "            \"network\": [neuromark[k][1] for k in all_significant],\n",
    "            \"x\": [neuromark[k][2] for k in all_significant],\n",
    "            \"y\": [neuromark[k][3] for k in all_significant],\n",
    "            \"z\": [neuromark[k][4] for k in all_significant],\n",
    "        }\n",
    "        pd.DataFrame(all_significant).to_csv(f\"{filepath}_significant.csv\")\n",
    "\n",
    "\n",
    "        ax[-1].barh(\n",
    "            x,\n",
    "            data_0,\n",
    "            align=\"center\",\n",
    "            color='blue',\n",
    "            label='0', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax[-1].barh(\n",
    "            x,\n",
    "            data_1,\n",
    "            align=\"center\",\n",
    "            color='red',\n",
    "            label='1', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax[-1].set_xlim(min_v, max_v)\n",
    "        ax[-1].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax[-1].set_yticks(x, [neuromark[k][0] if k in significant_comp else k for k in x], fontsize=6)\n",
    "        ax[-1].set_title(\"Grad medians\")\n",
    "\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    \n",
    "    def get_grads(self, method, features, target):\n",
    "        \"\"\"Returns gradients according to method\"\"\"\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        if method == \"saliency\":\n",
    "            saliency = Saliency(self.model)\n",
    "            grads = saliency.attribute(features, target=target, abs=False)\n",
    "        elif method == \"ig\":\n",
    "            # ig = IntegratedGradients(self.model, multiply_by_inputs=False)\n",
    "            ig = IntegratedGradients(self.model, multiply_by_inputs=True)\n",
    "            grads, _ = ig.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                baselines=torch.zeros_like(features),\n",
    "                return_convergence_delta=True,\n",
    "            )\n",
    "        elif method == \"ignt\":\n",
    "            ig = Saliency(self.model)\n",
    "            nt = NoiseTunnel(ig)\n",
    "            grads = nt.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                abs=False,\n",
    "                nt_type=\"vargrad\",\n",
    "                nt_samples=10,\n",
    "                stdevs=0.5,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"'{method}' methods is not recognized\")\n",
    "\n",
    "        return grads\n",
    "\n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "# paths = [\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\"]\n",
    "    \n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# paths = [\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\"]\n",
    "paths = [\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-bsnip\", \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-cobre\"]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "# methods = [\"ig\"]\n",
    "# methods = [\"ignt\"]\n",
    "methods = [\"saliency\"]\n",
    "\n",
    "for path in paths:\n",
    "    model, data, _ = load_model_and_data(path)\n",
    "\n",
    "    dataset = data[\"test\"].dataset\n",
    "    features = [sample[0] for sample in dataset]\n",
    "    labels = [sample[1] for sample in dataset]\n",
    "\n",
    "    load_only_test = True\n",
    "    if not load_only_test:\n",
    "        dataset = data[\"test\"].dataset\n",
    "        for sample in dataset:\n",
    "            features.append(sample[0])\n",
    "            labels.append(sample[1])\n",
    "\n",
    "    features = torch.stack(features)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    introspector = Introspector(\n",
    "        model=model, \n",
    "        features=features, \n",
    "        labels=labels, \n",
    "        methods=methods, \n",
    "        save_path=f\"{path.replace('/logs/','/introspection/')}/\"\n",
    "    )\n",
    "    introspector.run(cutoff=10, percentile=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, checkpoint = load_model_and_data(\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\")\n",
    "\n",
    "dataset = data[\"test\"].dataset\n",
    "all_features = [sample[0] for sample in dataset]\n",
    "all_labels = [sample[1] for sample in dataset]\n",
    "all_features = torch.stack(all_features)\n",
    "all_labels = torch.stack(all_labels)\n",
    "\n",
    "final_weight = checkpoint[\"fc.4.weight\"].numpy()\n",
    "final_bias = checkpoint[\"fc.4.bias\"].numpy()\n",
    "starting_weight = checkpoint[\"fc.0.weight\"].numpy()\n",
    "starting_bias = checkpoint[\"fc.0.bias\"].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_array = all_labels == 0\n",
    "features = all_features[filter_array]\n",
    "features.requires_grad = False\n",
    "model.zero_grad()\n",
    "\n",
    "prediction = model(features[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = prediction[0][0]\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grad = model.fc[4].weight.grad.numpy()\n",
    "init_grad = model.fc[0].weight.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "cutoff = 10\n",
    "\n",
    "for target in range(2):\n",
    "    G = nx.Graph()\n",
    "    plt.figure(figsize=(20,4))\n",
    "    G.add_node(f\"Prediction {target}\", name=f\"Prediction {target}\", layer=2)\n",
    "\n",
    "    important_final = np.argsort(np.abs(final_grad[target]))[-cutoff:]\n",
    "    # print(final_weight.shape)\n",
    "    important_starting = []\n",
    "    for index in important_final:\n",
    "        G.add_node(f\"int {index}\", name=f\"int {index}\", layer=1)\n",
    "        G.add_edge(f\"Prediction {target}\", f\"int {index}\", weight=final_grad[target, index])\n",
    "\n",
    "        important_st = np.argsort(np.abs(init_grad[index]))[-cutoff:]\n",
    "        # for idx in important_st:\n",
    "        #     G.add_node(f\"IC {idx}\", name=f\"IC {idx}\", layer=0)\n",
    "        #     G.add_edge(f\"int {index}\", f\"IC {idx}\", weight=init_grad[index, idx])\n",
    "            \n",
    "        important_starting += [important_st]\n",
    "    \n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\", align='horizontal')\n",
    "    elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0]\n",
    "    esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=elarge, width=1, edge_color=\"r\")\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=esmall, width=1, edge_color=\"b\")\n",
    "    # node labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
    "    # edge weight labels\n",
    "    edge_labels = dict([((u,v,), f\"{d['weight']:.3f}\") for u,v,d in G.edges(data=True)])\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "    ax = plt.gca()\n",
    "    ax.margins(0.08)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.abs(final_weight[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "\n",
    "cutoff = 5\n",
    "\n",
    "for target in range(2):\n",
    "    G = nx.Graph()\n",
    "    plt.figure(figsize=(40,4))\n",
    "    G.add_node(f\"Prediction {target}\", name=f\"Prediction {target}\", layer=2)\n",
    "\n",
    "    important_final = np.argsort(np.abs(final_weight[target]))[-cutoff:]\n",
    "    # important_final = np.argsort(np.abs(final_weight[target]))\n",
    "    # print(final_weight.shape)\n",
    "    important_starting = []\n",
    "    for index in important_final:\n",
    "        G.add_node(f\"int {index}\", name=f\"int {index}\", layer=1)\n",
    "        G.add_edge(f\"Prediction {target}\", f\"int {index}\", weight=final_weight[target, index])\n",
    "\n",
    "        important_st = np.argsort(np.abs(starting_weight[index]))[-cutoff:]\n",
    "        for idx in important_st:\n",
    "            G.add_node(f\"IC {idx}\", name=f\"IC {idx}\", layer=0)\n",
    "            G.add_edge(f\"int {index}\", f\"IC {idx}\", weight=starting_weight[index, idx])\n",
    "            \n",
    "        important_starting += [important_st]\n",
    "    \n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\", align='horizontal')\n",
    "    elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0]\n",
    "    esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=elarge, width=1, edge_color=\"r\")\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=esmall, width=1, edge_color=\"b\")\n",
    "    # node labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
    "    # edge weight labels\n",
    "    edge_labels = dict([((u,v,), f\"{d['weight']:.3f}\") for u,v,d in G.edges(data=True)])\n",
    "    # nx.draw_networkx_edge_labels(G, pos, edge_labels, alpha=0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.margins(0.08)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.abs(starting_weight[0, important_starting[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(final_weight))\n",
    "np.min(np.abs(final_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "paths = [\n",
    "    \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "]\n",
    "methods = [\"ig\"]\n",
    "\n",
    "for path in paths:\n",
    "    model, data, _ = load_model_and_data(path)\n",
    "\n",
    "    dataset = data[\"test\"].dataset\n",
    "    features = [sample[0] for sample in dataset]\n",
    "    labels = [sample[1] for sample in dataset]\n",
    "\n",
    "    load_only_test = True\n",
    "    if not load_only_test:\n",
    "        dataset = data[\"test\"].dataset\n",
    "        for sample in dataset:\n",
    "            features.append(sample[0])\n",
    "            labels.append(sample[1])\n",
    "\n",
    "    features = torch.stack(features)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    introspector = Introspector(\n",
    "        model=model, \n",
    "        features=features, \n",
    "        labels=labels, \n",
    "        methods=methods, \n",
    "        save_path=f\"{path.replace('/logs/','/introspection/')}/\"\n",
    "    )\n",
    "    introspector.run(cutoff=10, percentile=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "method = \"ig\"\n",
    "grads_0 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_0_method_{method}.npy\")\n",
    "grads_1 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_1_method_{method}.npy\")\n",
    "print(grads_0.shape)\n",
    "print(grads_1.shape)\n",
    "\n",
    "\n",
    "target_0 = grads_0.reshape(-1, grads_0.shape[2])\n",
    "target_1 = grads_1.reshape(-1, grads_1.shape[2])\n",
    "\n",
    "print(target_0.shape)\n",
    "print(target_1.shape)\n",
    "\n",
    "test = stats.ttest_ind(target_0, target_1, equal_var=False)\n",
    "# test = stats.mannwhitneyu(target_0, target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"ig\"\n",
    "grads_0 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_0_method_{method}.npy\")\n",
    "grads_1 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_1_method_{method}.npy\")\n",
    "print(grads_0.shape)\n",
    "print(grads_1.shape)\n",
    "\n",
    "target_0 = grads_0.reshape(-1, grads_0.shape[2])\n",
    "target_1 = grads_1.reshape(-1, grads_1.shape[2])\n",
    "\n",
    "mean_A = np.mean(target_0, axis=0)\n",
    "mean_B = np.mean(target_1, axis=0)\n",
    "\n",
    "x = np.arange(53)\n",
    "\n",
    "plt.figure(figsize=(4, 7))\n",
    "plt.barh(\n",
    "    x,\n",
    "    mean_A,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    "    label='0', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.barh(\n",
    "    x,\n",
    "    mean_B,\n",
    "    align=\"center\",\n",
    "    color='red',\n",
    "    label='1', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-0.038, 0.038)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "abs_grad = np.abs(mean_A) + np.abs(mean_B)\n",
    "significatn_components = np.argsort(abs_grad)[-10:]\n",
    "plt.yticks(x, [neuromark[i][0] if i in significatn_components else i for i in x], fontsize=7) \n",
    "\n",
    "plt.savefig(\n",
    "    \"1_all.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = np.mean(target_0, axis=0).copy()\n",
    "B_1 = np.mean(target_1, axis=0).copy()\n",
    "\n",
    "for i in range(53):\n",
    "    A_1[i] = np.sign(mean_A[i]) * np.min((np.abs(mean_A[i]), np.abs(mean_B[i])))\n",
    "    B_1[i] = np.sign(mean_B[i]) * np.min((np.abs(mean_A[i]), np.abs(mean_B[i])))\n",
    "\n",
    "x = np.arange(53)\n",
    "\n",
    "plt.figure(figsize=(4, 7))\n",
    "plt.barh(\n",
    "    x,\n",
    "    A_1,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    "    label='0', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.barh(\n",
    "    x,\n",
    "    B_1,\n",
    "    align=\"center\",\n",
    "    color='red',\n",
    "    label='1', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-0.038, 0.038)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "abs_grad = np.abs(A_1) + np.abs(B_1)\n",
    "significatn_components = np.argsort(abs_grad)[-10:]\n",
    "plt.yticks(x, [neuromark[i][0] if i in significatn_components else i for i in x], fontsize=7) \n",
    "plt.savefig(\n",
    "    \"2_sym.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_2 = np.mean(target_0, axis=0).copy()\n",
    "B_2 = np.mean(target_1, axis=0).copy()\n",
    "\n",
    "for i in range(53):\n",
    "    A_2[i] = mean_A[i] - A_1[i]\n",
    "    B_2[i] = mean_B[i] - B_1[i]\n",
    "\n",
    "x = np.arange(53)\n",
    "\n",
    "plt.figure(figsize=(4, 7))\n",
    "plt.barh(\n",
    "    x,\n",
    "    A_2,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    "    label='0', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.barh(\n",
    "    x,\n",
    "    B_2,\n",
    "    align=\"center\",\n",
    "    color='red',\n",
    "    label='1', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-0.038, 0.038)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "abs_grad = np.abs(A_2) + np.abs(B_2)\n",
    "significatn_components = np.argsort(abs_grad)[-10:]\n",
    "plt.yticks(x, [neuromark[i][0] if i in significatn_components else i for i in x], fontsize=7) \n",
    "plt.savefig(\n",
    "    \"3_asym.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_0, B = target_0.shape\n",
    "A_1, B = target_1.shape\n",
    "# Flatten the arrays to 1D arrays\n",
    "target_0_flat = target_0.flatten()\n",
    "target_1_flat = target_1.flatten()\n",
    "target_flat = np.concatenate([target_0_flat, target_1_flat])\n",
    "\n",
    "# Create arrays for the indicators\n",
    "component_array_0 = np.repeat(np.arange(B), A_0)\n",
    "component_array_1 = np.repeat(np.arange(B), A_1)\n",
    "component_array = np.concatenate([component_array_0, component_array_1])\n",
    "target_array_0 = np.array([0]*(A_0*B))\n",
    "target_array_1 = np.array([1]*(A_1*B))\n",
    "target_array = np.concatenate([target_array_0, target_array_1])\n",
    "\n",
    "print(target_flat.shape)\n",
    "print(component_array.shape)\n",
    "print(target_array.shape)\n",
    "# Create the Pandas DataFrame\n",
    "df = pd.DataFrame({'Data': target_flat,\n",
    "                   'Component': component_array,\n",
    "                   'Target': target_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "fig, ax = plt.subplots(1, 1, figsize=(25, 5))\n",
    "\n",
    "sns.boxplot(df, x=\"Component\", y=\"Data\", hue=\"Target\", showfliers = False, ax=ax)\n",
    "\n",
    "box_pairs = [((i, 0), (i, 1)) for i in range(53)]\n",
    "plt.savefig(\"hi.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = test.statistic\n",
    "plt.bar(\n",
    "    range(pval.shape[0]),\n",
    "    pval,\n",
    "    align=\"center\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.xticks(list(range(0, pval.shape[0], 2)))\n",
    "plt.grid()\n",
    "plt.xlim([0, pval.shape[0]])\n",
    "plt.title(\"T statistics\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = test.pvalue\n",
    "pval = np.log(pval)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "bars = ax.bar(\n",
    "    range(pval.shape[0]),\n",
    "    pval,\n",
    "    align=\"center\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax.set_xticks(list(range(0, pval.shape[0], 2)))\n",
    "ax.grid()\n",
    "ax.set_xlim([0, pval.shape[0]])\n",
    "# ax.set_ylim([0, 0.05])\n",
    "ax.set_title(\"log(p-value)\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "significatn_components = np.where(pval<-40)\n",
    "print(significatn_components[0])\n",
    "print(\"Significatn components (log(p_val) < -40)):\")\n",
    "for i in significatn_components[0]:\n",
    "    print(neuromark[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    \n",
    "    # def run(self, cutoff=1, percentile=0.1):\n",
    "    #     \"\"\"Run introspection, save results\"\"\"\n",
    "    #     targets = torch.unique(self.labels)\n",
    "    #     for target in targets:\n",
    "\n",
    "    #         print(f\"Getting MLP predictions for target {target}\")  \n",
    "    #         filter_array = self.labels == target\n",
    "    #         features = self.features[filter_array]\n",
    "    #         features.requires_grad = False\n",
    "\n",
    "    #         timeseries, predictions = self.model(features, introspection=True)\n",
    "    #         timeseries = timeseries.cpu().detach().numpy()\n",
    "    #         predictions = predictions.cpu().detach().numpy()\n",
    "\n",
    "    #         x = np.arange(timeseries.shape[1])\n",
    "    #         timeseries_0 = timeseries[:, :, 0]\n",
    "    #         timeseries_1 = timeseries[:, :, 1]\n",
    "\n",
    "    #         np.save(f\"{self.save_path}timeseries_target_{target}.npy\", timeseries)\n",
    "    #         window_size = 10\n",
    "\n",
    "    #         fig, ax = plt.subplots(1, cutoff,figsize=(3*cutoff, 2))\n",
    "    #         for i in range(cutoff):\n",
    "    #             smoothed_data_0 = np.convolve(timeseries_0[i], np.ones(window_size)/window_size, mode='same')\n",
    "    #             smoothed_data_1 = np.convolve(timeseries_1[i], np.ones(window_size)/window_size, mode='same')\n",
    "                \n",
    "    #             smooth_line_0 = ax[i].plot(x, smoothed_data_0, label='0', color='blue')\n",
    "    #             smooth_line_1 = ax[i].plot(x, smoothed_data_1, label='1', color='red')\n",
    "    #             line_0 = ax[i].plot(x, timeseries_0[i], '#00009922')\n",
    "    #             line_1 = ax[i].plot(x, timeseries_1[i], '#99000022')\n",
    "\n",
    "    #             # ax[i].set_title(f\"True target = {target}, Predicted target = {predictions[i]}\")\n",
    "    #             ax[i].legend(title=\"Prediction\")\n",
    "                \n",
    "    #             ax[i].set_xticklabels([])\n",
    "    #             ax[i].set_xticks([])\n",
    "    #             ax[i].set_yticklabels([])\n",
    "    #             ax[i].set_yticks([])\n",
    "    #             ax[i].set_xlabel(\"Time\")\n",
    "    #             ax[i].set_ylabel(\"Prediction strength\" if i == 0 else \"\")\n",
    "\n",
    "    #         fig.savefig(\n",
    "    #             f\"{self.save_path}timeseries/prediction_target_{target}.png\",\n",
    "    #             format=\"png\",\n",
    "    #             dpi=300,\n",
    "    #             bbox_inches=\"tight\",\n",
    "    #         )\n",
    "    #         fig.savefig(\n",
    "    #             f\"{self.save_path}timeseries/prediction_target_{target}.svg\",\n",
    "    #             format=\"svg\",\n",
    "    #             bbox_inches=\"tight\",\n",
    "    #         )\n",
    "\n",
    "            \n",
    "    #         for method in self.methods:\n",
    "    #             filter_array = self.labels == target\n",
    "    #             features = self.features[filter_array]\n",
    "    #             features.requires_grad = True\n",
    "\n",
    "    #             grads = self.get_grads(method, features, target)\n",
    "    #             grads = grads.cpu().detach().numpy()\n",
    "\n",
    "    #             np.save(f\"{self.save_path}grads_target_{target}_method_{method}.npy\", grads)\n",
    "\n",
    "    #             smoothed_timeseries = np.array([np.convolve(signal[:, target], np.ones(9)/9, mode='same') for signal in timeseries])\n",
    "    #             smooth_grads = np.log(grads) * smoothed_timeseries[:, :][:, :, np.newaxis]\n",
    "    #             detached_features = features.cpu().detach().numpy()\n",
    "\n",
    "    #             # np.save(f\"{self.save_path}smooth_grads_target_{target}.npy\", grads)\n",
    "                \n",
    "    #             print(f\"\\tPlotting generalized saliency maps using '{method}' with target {target}\")\n",
    "    #             self.plot_colormaps(\n",
    "    #                 grads,\n",
    "    #                 detached_features, \n",
    "    #                 filepath=f\"{self.save_path}{method}/colormap/general_{target}.png\",\n",
    "    #                 color=\"blue\" if target == 0 else \"red\"\n",
    "    #             )\n",
    "\n",
    "    #             self.plot_histograms(\n",
    "    #                 grads, \n",
    "    #                 filepath=f\"{self.save_path}{method}/barchart/general_{target}.png\",\n",
    "    #                 percentile=percentile\n",
    "    #             )\n",
    "\n",
    "    #             print(f\"\\tPlotting single sample saliency maps using '{method}' with target {target}\")\n",
    "    #             for i in range(cutoff):\n",
    "    #                 feature = detached_features[i][np.newaxis, :, :]\n",
    "    #                 grad = grads[i][np.newaxis, :, :]\n",
    "\n",
    "    #                 self.plot_colormaps(\n",
    "    #                     grad,\n",
    "    #                     feature, \n",
    "    #                     filepath=f\"{self.save_path}{method}/colormap/target_{target}_idx_{i:04d}.png\",\n",
    "    #                     color=\"blue\" if target == 0 else \"red\"\n",
    "    #                 )\n",
    "\n",
    "    #                 self.plot_histograms(\n",
    "    #                     grad, \n",
    "    #                     filepath=f\"{self.save_path}{method}/barchart/target_{target}_idx_{i:04d}.png\",\n",
    "    #                     percentile=percentile\n",
    "    #                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Introspector:\n",
    "    \"\"\"Basic introspector\"\"\"\n",
    "\n",
    "    def __init__(self, model, features, labels, methods, save_path) -> None:\n",
    "        self.methods = methods\n",
    "        self.save_path = save_path\n",
    "        self.model = model\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        # os.makedirs(f\"{self.save_path}timeseries\", exist_ok=True)\n",
    "        if \"saliency\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}saliency/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}saliency/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}saliency\", exist_ok=True)\n",
    "        if \"ig\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}ig/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}ig/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}ig\", exist_ok=True)\n",
    "        if \"ignt\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}ignt/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}ignt/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}ignt\", exist_ok=True)\n",
    "\n",
    "    def run(self, cutoff=1, percentile=0.1):\n",
    "        \"\"\"Run introspection, save results\"\"\"\n",
    "        targets = torch.unique(self.labels)\n",
    "        n_classes = targets.shape[0]\n",
    "\n",
    "        print(f\"Getting MLP predictions for {self.save_path}\")\n",
    "        features, timeseries, smooth_timeseries, predictions = (\n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes,\n",
    "        )\n",
    "\n",
    "        window_size = 10\n",
    "        for class_idx in range(n_classes):\n",
    "            filter_array = self.labels == class_idx\n",
    "            features[class_idx] = self.features[filter_array]\n",
    "            features[class_idx].requires_grad = False\n",
    "\n",
    "            timeseries_raw, predictions[class_idx] = self.model(features[class_idx], introspection=True)\n",
    "            timeseries_raw = timeseries_raw.cpu().detach().numpy()\n",
    "            predictions[class_idx] = predictions[class_idx].cpu().detach().numpy()\n",
    "            \n",
    "            # timeseries_raw was calculated for the data related to class_idx;\n",
    "            # yet it has `n_classes` class prediction time series in 3rd dimension\n",
    "            # so timeseries[class_idx] is a list of length n_classes, each element of which\n",
    "            # represents the class prediction time series\n",
    "            timeseries[class_idx] = [timeseries_raw[:, :, j] for j in range(n_classes)]\n",
    "\n",
    "            smooth_timeseries[class_idx] = deepcopy(timeseries[class_idx])\n",
    "            for j in range(n_classes):\n",
    "                for subject in range(smooth_timeseries[class_idx][j].shape[0]):\n",
    "                    smooth_timeseries[class_idx][j][subject] = np.convolve(smooth_timeseries[class_idx][j][subject], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "        all_features = torch.concat(features)\n",
    "        features = [all_features, all_features]\n",
    "\n",
    "        grads = {key: None for key in self.methods}\n",
    "        specific_grads = {key: None for key in self.methods}\n",
    "        for method in self.methods:\n",
    "            print(f\"Processing {method}\")\n",
    "\n",
    "            sizes = [features[j].shape for j in range(n_classes)]\n",
    "            # print(features[0].shape)\n",
    "            # print(features[0].reshape(-1, 1, 53).shape)\n",
    "            # grads[method] = [self.get_grads(method, features[j].reshape(-1, 1, 53), j).cpu().detach().numpy().reshape(*sizes[j]) for j in range(n_classes)]\n",
    "            grads[method] = [self.get_grads(method, features[j], j).cpu().detach().numpy() for j in range(n_classes)]\n",
    "            print(grads[method][0].shape)\n",
    "\n",
    "            np.save(f\"{self.save_path}{method}/grads_0.npy\", grads[method][0])\n",
    "            np.save(f\"{self.save_path}{method}/grads_1.npy\", grads[method][1])\n",
    "\n",
    "            specific_grads[method] = []\n",
    "            for i in range(n_classes):\n",
    "                if i == 0:\n",
    "                    filter_array = np.argwhere(timeseries[i][0] > timeseries[i][1])\n",
    "                else:\n",
    "                    filter_array = np.argwhere(timeseries[i][0] < timeseries[i][1])\n",
    "                \n",
    "                specific_grads[method].append(np.array([grads[method][i][x, y, :] for (x, y) in filter_array]))\n",
    "\n",
    "            # print(specific_grads[method][0].shape)\n",
    "            # print(grads[method][0].shape)\n",
    "            np.save(f\"{self.save_path}{method}/filtered_grads_0.npy\", specific_grads[method][0])\n",
    "            np.save(f\"{self.save_path}{method}/filtered_grads_1.npy\", specific_grads[method][1])\n",
    "\n",
    "        # plot everything\n",
    "        for method in self.methods:\n",
    "            print(f\"Plotting {method}\")\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                print(f\"Plotting time series for true target {i}\")\n",
    "                self.plot_timeseries(\n",
    "                    timeseries=timeseries[i], \n",
    "                    smooth_timeseries=smooth_timeseries[i], \n",
    "                    grads=grads[method][i],\n",
    "                    features=features[i],\n",
    "                    predictions=predictions[i],\n",
    "                    target=i,\n",
    "                    cutoff=cutoff, \n",
    "                    filepath=f\"{self.save_path}{method}/timeseries_target_{i}\"\n",
    "                )\n",
    "\n",
    "            print(f\"Plotting spatial attention\")\n",
    "            self.plot_histograms(\n",
    "                grads=specific_grads[method],\n",
    "                # grads=grads[method],\n",
    "                features=features,\n",
    "                cutoff=cutoff, \n",
    "                filepath=f\"{self.save_path}{method}/spatial\",\n",
    "                percentile=percentile\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def plot_timeseries(self, timeseries, smooth_timeseries, grads, features, predictions, target, filepath, cutoff, use_log=False):\n",
    "        fig, ax = plt.subplots(2, cutoff,figsize=(3*cutoff, 4))\n",
    "\n",
    "        x = np.arange(timeseries[0].shape[1])\n",
    "        for i in range(cutoff):\n",
    "            smooth_line_0 = ax[0][i].plot(x[:140], smooth_timeseries[0][i][:140], label='0', color='blue')\n",
    "            smooth_line_1 = ax[0][i].plot(x[:140], smooth_timeseries[1][i][:140], label='1', color='red')\n",
    "            line_0 = ax[0][i].plot(x[:140], timeseries[0][i][:140], '#00009922')\n",
    "            line_1 = ax[0][i].plot(x[:140], timeseries[1][i][:140], '#99000022')\n",
    "\n",
    "            # ax[i].set_title(f\"True target = {target}, Predicted target = {predictions[i]}\")\n",
    "            ax[0][i].legend(title=\"Prediction\")\n",
    "            \n",
    "            ax[0][i].set_xlim(0, 140)\n",
    "            ax[0][i].set_xticklabels([])\n",
    "            ax[0][i].set_xticks([])\n",
    "            ax[0][i].set_yticklabels([])\n",
    "            ax[0][i].set_yticks([])\n",
    "            ax[0][i].set_xlabel(\"Time\")\n",
    "            ax[0][i].set_ylabel(\"Prediction strength\" if i == 0 else \"\")\n",
    "\n",
    "            # grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            feat = features[i].cpu().detach().numpy()[np.newaxis, :, :]\n",
    "            _, _ = viz.visualize_image_attr(\n",
    "                np.transpose(grad, (2, 1, 0))[:,:140, :],\n",
    "                np.transpose(feat, (2, 1, 0))[:,:140, :],\n",
    "                method=\"heat_map\",\n",
    "                cmap=\"inferno\",\n",
    "                show_colorbar=False,\n",
    "                plt_fig_axis=(fig, ax[1][i]),\n",
    "                use_pyplot=False,\n",
    "            )\n",
    "            ax[0][i].grid()\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        \n",
    "    def plot_histograms(self, grads, features, cutoff, filepath, percentile=0.1):\n",
    "        sns.reset_defaults()\n",
    "        if len(grads[0].shape) == 3:\n",
    "            data_0 = grads[0].reshape(-1, grads[0].shape[2])\n",
    "            data_1 = grads[1].reshape(-1, grads[1].shape[2])\n",
    "            x = np.arange(grads[0].shape[2])\n",
    "        else:\n",
    "            data_0 = grads[0]\n",
    "            data_1 = grads[1]\n",
    "            x = np.arange(grads[0].shape[1])\n",
    "        \n",
    "        corr_0 = np.corrcoef(data_0.T)\n",
    "        corr_1 = np.corrcoef(data_1.T)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(corr_0, cmap=corr_cmap)\n",
    "        cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_fnc_0.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(corr_1, cmap=corr_cmap)\n",
    "        cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_fnc_1.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(np.abs(corr_0-corr_1), cmap=abs_corr_cmap)\n",
    "        # cax.set(clim=(-1.0, 1.0))\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        fig.colorbar(cax)\n",
    "        fig.savefig(\n",
    "            f\"{filepath}_fnc_diff.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        # fig, ax = plt.subplots(1, 4, figsize=(cutoff+3, 7))\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 14))\n",
    "        # fig, ax = plt.subplots(1, 2, figsize=(12, 14))\n",
    "\n",
    "        # data_1 = data_1 * -1.0\n",
    "\n",
    "        df_0 = pd.DataFrame(data_0, columns=[i for i in range(data_0.shape[1])])\n",
    "        df_0['Class'] = 0\n",
    "        df_1 = pd.DataFrame(data_1, columns=[i for i in range(data_1.shape[1])])\n",
    "        df_1['Class'] = 1\n",
    "        df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "        df = pd.melt(df, id_vars=['Class'], var_name='Component', value_name='Data')\n",
    "        sns.boxplot(df, y=\"Component\", x=\"Data\", hue=\"Class\", ax=ax[0], palette=[\"blue\", \"red\"], showfliers = False, orient='h', linewidth=0.3)\n",
    "        ax[0].grid(axis='y', linestyle='--', alpha=0.3, linewidth=0.3)\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[0].set_yticks(x, x, fontsize=6)\n",
    "        ax[0].set_ylim(-3, 55)\n",
    "        ax[0].set_title(\"Gradients\")\n",
    "\n",
    "        # test = stats.mannwhitneyu(np.abs(data_0), np.abs(data_1))\n",
    "        test = stats.ranksums(data_0, data_1)\n",
    "        pvals = test.pvalue\n",
    "        # pvals = pvals*data_0.shape[1]\n",
    "        pvals = stats.false_discovery_control(pvals, method='by')\n",
    "        significant_p_vals = np.argwhere(pvals < 0.05)\n",
    "        # pvals = pvals / pvals.shape[0]\n",
    "        bars = ax[1].barh(\n",
    "            x,\n",
    "            pvals,\n",
    "            align=\"center\",\n",
    "            color='blue',\n",
    "        )\n",
    "        ax[1].set_yticks(x, [neuromark[k][0] if k in significant_p_vals else k for k in x], fontsize=6)\n",
    "        ax[1].set_xlabel(\"P-value\")\n",
    "        ax[1].set_xscale(\"log\")\n",
    "        ax[1].set_title(\"P-values\")\n",
    "\n",
    "        for i, bar in enumerate(bars):\n",
    "            if pvals[i] < 0.05:\n",
    "                bar.set_color('red')\n",
    "        # bar charts: summarizes gradients at each component\n",
    "\n",
    "        data_0 = np.median(data_0, axis=0)\n",
    "        data_1 = np.median(data_1, axis=0)\n",
    "        \n",
    "        min_0, max_0 = np.min(data_0), np.max(data_0)\n",
    "        min_1, max_1 = np.min(data_1), np.max(data_1)\n",
    "        min_v, max_v = min(min_0, min_1), max(max_0, max_1)\n",
    "        if min_v <= 0:\n",
    "            max_v = max(abs(min_v), abs(max_v))\n",
    "            min_v = -1.0*max_v\n",
    "        else:\n",
    "            min_v = 0\n",
    "                \n",
    "        sym_data_0 = np.zeros(x.shape)\n",
    "        sym_data_1 = np.zeros(x.shape)\n",
    "        asym_data_0 = np.zeros(x.shape)\n",
    "        asym_data_1 = np.zeros(x.shape)\n",
    "\n",
    "        for j in x:\n",
    "            sym_data_0[j] = np.sign(data_0[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            sym_data_1[j] = np.sign(data_1[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            asym_data_0[j] = data_0[j] - sym_data_0[j]\n",
    "            asym_data_1[j] = data_1[j] - sym_data_1[j]\n",
    "\n",
    "        abs_grad = np.abs(data_0) + np.abs(data_1)\n",
    "        abs_sym_grad = np.abs(sym_data_0) + np.abs(sym_data_1)\n",
    "        abs_asym_grad = np.abs(asym_data_0) + np.abs(asym_data_1)\n",
    "        significant_comp = np.argsort(abs_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_sym_comp = np.argsort(abs_sym_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_asym_comp = np.argsort(abs_asym_grad)[-int(percentile*x.shape[0]):]\n",
    "\n",
    "        all_significant = np.intersect1d(significant_comp, np.intersect1d(significant_sym_comp, significant_asym_comp))\n",
    "        all_significant = {\n",
    "            \"idx\": all_significant,\n",
    "            \"name\": [neuromark[k][0] for k in all_significant],\n",
    "            \"network\": [neuromark[k][1] for k in all_significant],\n",
    "            \"x\": [neuromark[k][2] for k in all_significant],\n",
    "            \"y\": [neuromark[k][3] for k in all_significant],\n",
    "            \"z\": [neuromark[k][4] for k in all_significant],\n",
    "        }\n",
    "        pd.DataFrame(all_significant).to_csv(f\"{filepath}_significant.csv\")\n",
    "\n",
    "\n",
    "        ax[2].barh(\n",
    "            x,\n",
    "            data_0,\n",
    "            align=\"center\",\n",
    "            color='blue',\n",
    "            label='0', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax[2].barh(\n",
    "            x,\n",
    "            data_1,\n",
    "            align=\"center\",\n",
    "            color='red',\n",
    "            label='1', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax[2].set_xlim(min_v, max_v)\n",
    "        ax[2].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax[2].set_yticks(x, [neuromark[k][0] if k in significant_comp else k for k in x], fontsize=6)\n",
    "        ax[2].set_title(\"Grad medians\")\n",
    "        # ax[2].set_xticks([], [])\n",
    "        \n",
    "        # ax[2].barh(\n",
    "        #     x,\n",
    "        #     sym_data_0,\n",
    "        #     align=\"center\",\n",
    "        #     color='blue',\n",
    "        #     label='0', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[2].barh(\n",
    "        #     x,\n",
    "        #     sym_data_1,\n",
    "        #     align=\"center\",\n",
    "        #     color='red',\n",
    "        #     label='1', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[2].set_xlim(min_v, max_v)\n",
    "        # ax[2].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        # ax[2].set_yticks(x, [neuromark[k][0] if k in significant_sym_comp else k for k in x], fontsize=6)\n",
    "        # # ax[2].set_xticks([], [])\n",
    "\n",
    "        # ax[3].barh(\n",
    "        #     x,\n",
    "        #     asym_data_0,\n",
    "        #     align=\"center\",\n",
    "        #     color='blue',\n",
    "        #     label='0', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[3].barh(\n",
    "        #     x,\n",
    "        #     asym_data_1,\n",
    "        #     align=\"center\",\n",
    "        #     color='red',\n",
    "        #     label='1', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[3].set_xlim(min_v, max_v)\n",
    "        # ax[3].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        # ax[3].set_yticks(x, [neuromark[k][0] if k in significant_asym_comp else k for k in x], fontsize=6)\n",
    "        # # ax[3].set_xticks([], [])\n",
    "\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    \n",
    "    def get_grads(self, method, features, target):\n",
    "        \"\"\"Returns gradients according to method\"\"\"\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        if method == \"saliency\":\n",
    "            saliency = Saliency(self.model)\n",
    "            grads = saliency.attribute(features, target=target, abs=False)\n",
    "        elif method == \"ig\":\n",
    "            ig = IntegratedGradients(self.model, multiply_by_inputs=False)\n",
    "            # ig = IntegratedGradients(self.model, multiply_by_inputs=True)\n",
    "            grads, _ = ig.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                baselines=torch.zeros_like(features),\n",
    "                return_convergence_delta=True,\n",
    "            )\n",
    "        elif method == \"ignt\":\n",
    "            ig = Saliency(self.model)\n",
    "            nt = NoiseTunnel(ig)\n",
    "            grads = nt.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                abs=False,\n",
    "                nt_type=\"vargrad\",\n",
    "                nt_samples=10,\n",
    "                stdevs=0.5,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"'{method}' methods is not recognized\")\n",
    "\n",
    "        return grads\n",
    "\n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "# paths = [\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\"]\n",
    "    \n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-abide_869\"\n",
    "# ]\n",
    "paths = [\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\"]\n",
    "# methods = [\"ig\"]\n",
    "methods = [\"ignt\"]\n",
    "# methods = [\"saliency\"]\n",
    "\n",
    "for path in paths:\n",
    "    model, data, _ = load_model_and_data(path)\n",
    "\n",
    "    dataset = data[\"test\"].dataset\n",
    "    features = [sample[0] for sample in dataset]\n",
    "    labels = [sample[1] for sample in dataset]\n",
    "\n",
    "    load_only_test = True\n",
    "    if not load_only_test:\n",
    "        dataset = data[\"test\"].dataset\n",
    "        for sample in dataset:\n",
    "            features.append(sample[0])\n",
    "            labels.append(sample[1])\n",
    "\n",
    "    features = torch.stack(features)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    introspector = Introspector(\n",
    "        model=model, \n",
    "        features=features, \n",
    "        labels=labels, \n",
    "        methods=methods, \n",
    "        save_path=f\"{path.replace('/logs/','/introspection/')}/\"\n",
    "    )\n",
    "    introspector.run(cutoff=10, percentile=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
