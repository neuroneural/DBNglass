{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel, Saliency, visualization as viz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.data import data_factory, data_postfactory\n",
    "from src.dataloader import dataloader_factory\n",
    "from src.model import model_config_factory, model_factory\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_data(path):\n",
    "    cfg = OmegaConf.load(path+'/general_config.yaml')\n",
    "\n",
    "    df = pd.read_csv(path+'/runs.csv')\n",
    "    best_idx = df[\"test_score\"].idxmax()\n",
    "    k = best_idx // cfg.mode.n_trials\n",
    "    trial = best_idx - cfg.mode.n_trials * k\n",
    "    print(\"k \", k)\n",
    "    print(\"trial \", trial)\n",
    "\n",
    "    original_data = data_factory(cfg)\n",
    "    model_cfg = model_config_factory(cfg, k)\n",
    "    data = data_postfactory(\n",
    "            cfg,\n",
    "            model_cfg,\n",
    "            original_data,\n",
    "        )\n",
    "    dataloaders = dataloader_factory(cfg, data, k=k, trial=trial)\n",
    "\n",
    "    model = model_factory(cfg, model_cfg)\n",
    "\n",
    "    model_logpath = path+f\"/k_{k:02d}/trial_{trial:04d}/best_model.pt\"\n",
    "    checkpoint = torch.load(\n",
    "        model_logpath, map_location=lambda storage, loc: storage\n",
    "    )\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    return model, dataloaders, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuromark = pd.read_csv(\"/Users/pavelpopov/mlp_nn/assets/data/neuromark_regions.csv\")\n",
    "neuromark[\"Index\"]=neuromark[\"Index\"]-1\n",
    "neuromark = {a: (b, c, d, e, f) for a, b, c, d, e, f in zip(neuromark[\"Index\"], neuromark[\"Name\"], neuromark[\"Network\"], neuromark[\"X\"], neuromark[\"Y\"], neuromark[\"Z\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Introspector:\n",
    "    \"\"\"Basic introspector\"\"\"\n",
    "\n",
    "    def __init__(self, model, features, labels, methods, save_path) -> None:\n",
    "        self.methods = methods\n",
    "        self.save_path = save_path\n",
    "        self.model = model\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        # os.makedirs(f\"{self.save_path}timeseries\", exist_ok=True)\n",
    "        if \"saliency\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}saliency/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}saliency/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}saliency\", exist_ok=True)\n",
    "        if \"ig\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}ig/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}ig/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}ig\", exist_ok=True)\n",
    "        if \"ignt\" in self.methods:\n",
    "            # os.makedirs(f\"{self.save_path}ignt/colormap\", exist_ok=True)\n",
    "            # os.makedirs(f\"{self.save_path}ignt/barchart\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.save_path}ignt\", exist_ok=True)\n",
    "\n",
    "    def run(self, cutoff=1, percentile=0.1):\n",
    "        \"\"\"Run introspection, save results\"\"\"\n",
    "        targets = torch.unique(self.labels)\n",
    "        n_classes = targets.shape[0]\n",
    "\n",
    "        print(f\"Getting MLP predictions for {self.save_path}\")\n",
    "        features, timeseries, smooth_timeseries, predictions = (\n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes, \n",
    "            [None]*n_classes,\n",
    "        )\n",
    "\n",
    "        window_size = 10\n",
    "        for class_idx in range(n_classes):\n",
    "            filter_array = self.labels == class_idx\n",
    "            features[class_idx] = self.features[filter_array]\n",
    "            features[class_idx].requires_grad = False\n",
    "\n",
    "            timeseries_raw, predictions[class_idx] = self.model(features[class_idx], introspection=True)\n",
    "            timeseries_raw = timeseries_raw.cpu().detach().numpy()\n",
    "            predictions[class_idx] = predictions[class_idx].cpu().detach().numpy()\n",
    "            \n",
    "            # timeseries_raw was calculated for the data related to class_idx;\n",
    "            # yet it has `n_classes` class prediction time series in 3rd dimension\n",
    "            # so timeseries[class_idx] is a list of length n_classes, each element of which\n",
    "            # represents the class prediction time series\n",
    "            timeseries[class_idx] = [timeseries_raw[:, :, j] for j in range(n_classes)]\n",
    "\n",
    "            smooth_timeseries[class_idx] = deepcopy(timeseries[class_idx])\n",
    "            for j in range(n_classes):\n",
    "                for subject in range(smooth_timeseries[class_idx][j].shape[0]):\n",
    "                    smooth_timeseries[class_idx][j][subject] = np.convolve(smooth_timeseries[class_idx][j][subject], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "        \n",
    "        grads = {key: None for key in self.methods}\n",
    "        specific_grads = {key: None for key in self.methods}\n",
    "        for method in self.methods:\n",
    "            print(f\"Processing {method}\")\n",
    "\n",
    "            sizes = [features[j].shape for j in range(n_classes)]\n",
    "            # print(features[0].shape)\n",
    "            # print(features[0].reshape(-1, 1, 53).shape)\n",
    "            # grads[method] = [self.get_grads(method, features[j].reshape(-1, 1, 53), j).cpu().detach().numpy().reshape(*sizes[j]) for j in range(n_classes)]\n",
    "            grads[method] = [self.get_grads(method, features[j], j).cpu().detach().numpy() for j in range(n_classes)]\n",
    "            print(grads[method][0].shape)\n",
    "\n",
    "            np.save(f\"{self.save_path}{method}/grads_0.npy\", grads[method][0])\n",
    "            np.save(f\"{self.save_path}{method}/grads_1.npy\", grads[method][1])\n",
    "\n",
    "            specific_grads[method] = []\n",
    "            for i in range(n_classes):\n",
    "                if i == 0:\n",
    "                    filter_array = np.argwhere(timeseries[i][0] > timeseries[i][1])\n",
    "                else:\n",
    "                    filter_array = np.argwhere(timeseries[i][0] < timeseries[i][1])\n",
    "                \n",
    "                specific_grads[method].append(np.array([grads[method][i][x, y, :] for (x, y) in filter_array]))\n",
    "\n",
    "            # print(specific_grads[method][0].shape)\n",
    "            # print(grads[method][0].shape)\n",
    "            np.save(f\"{self.save_path}{method}/filtered_grads_0.npy\", specific_grads[method][0])\n",
    "            np.save(f\"{self.save_path}{method}/filtered_grads_1.npy\", specific_grads[method][1])\n",
    "\n",
    "        # plot everything\n",
    "        for method in self.methods:\n",
    "            print(f\"Plotting {method}\")\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                print(f\"Plotting time series for true target {i}\")\n",
    "                self.plot_timeseries(\n",
    "                    timeseries=timeseries[i], \n",
    "                    smooth_timeseries=smooth_timeseries[i], \n",
    "                    grads=grads[method][i],\n",
    "                    features=features[i],\n",
    "                    predictions=predictions[i],\n",
    "                    target=i,\n",
    "                    cutoff=cutoff, \n",
    "                    filepath=f\"{self.save_path}{method}/timeseries_target_{i}\"\n",
    "                )\n",
    "\n",
    "            print(f\"Plotting spatial attention\")\n",
    "            self.plot_histograms(\n",
    "                grads=specific_grads[method],\n",
    "                # grads=grads[method],\n",
    "                features=features,\n",
    "                cutoff=cutoff, \n",
    "                filepath=f\"{self.save_path}{method}/spatial\",\n",
    "                percentile=percentile\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def plot_timeseries(self, timeseries, smooth_timeseries, grads, features, predictions, target, filepath, cutoff, use_log=False):\n",
    "        fig, ax = plt.subplots(2, cutoff,figsize=(3*cutoff, 4))\n",
    "\n",
    "        x = np.arange(timeseries[0].shape[1])\n",
    "        for i in range(cutoff):\n",
    "            smooth_line_0 = ax[0][i].plot(x[:140], smooth_timeseries[0][i][:140], label='0', color='blue')\n",
    "            smooth_line_1 = ax[0][i].plot(x[:140], smooth_timeseries[1][i][:140], label='1', color='red')\n",
    "            line_0 = ax[0][i].plot(x[:140], timeseries[0][i][:140], '#00009922')\n",
    "            line_1 = ax[0][i].plot(x[:140], timeseries[1][i][:140], '#99000022')\n",
    "\n",
    "            # ax[i].set_title(f\"True target = {target}, Predicted target = {predictions[i]}\")\n",
    "            ax[0][i].legend(title=\"Prediction\")\n",
    "            \n",
    "            ax[0][i].set_xlim(0, 140)\n",
    "            ax[0][i].set_xticklabels([])\n",
    "            ax[0][i].set_xticks([])\n",
    "            ax[0][i].set_yticklabels([])\n",
    "            ax[0][i].set_yticks([])\n",
    "            ax[0][i].set_xlabel(\"Time\")\n",
    "            ax[0][i].set_ylabel(\"Prediction strength\" if i == 0 else \"\")\n",
    "\n",
    "            # grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            grad = (grads[i]*smooth_timeseries[target][i][:, np.newaxis])[np.newaxis, :, :]\n",
    "            feat = features[i].cpu().detach().numpy()[np.newaxis, :, :]\n",
    "            _, _ = viz.visualize_image_attr(\n",
    "                np.transpose(grad, (2, 1, 0))[:,:140, :],\n",
    "                np.transpose(feat, (2, 1, 0))[:,:140, :],\n",
    "                method=\"heat_map\",\n",
    "                cmap=\"inferno\",\n",
    "                show_colorbar=False,\n",
    "                plt_fig_axis=(fig, ax[1][i]),\n",
    "                use_pyplot=False,\n",
    "            )\n",
    "            ax[0][i].grid()\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "\n",
    "        \n",
    "    def plot_histograms(self, grads, features, cutoff, filepath, percentile=0.1):\n",
    "        sns.reset_defaults()\n",
    "        if len(grads[0].shape) == 3:\n",
    "            data_0 = grads[0].reshape(-1, grads[0].shape[2])\n",
    "            data_1 = grads[1].reshape(-1, grads[1].shape[2])\n",
    "            x = np.arange(grads[0].shape[2])\n",
    "        else:\n",
    "            data_0 = grads[0]\n",
    "            data_1 = grads[1]\n",
    "            x = np.arange(grads[0].shape[1])\n",
    "        \n",
    "        print(data_0.shape)\n",
    "\n",
    "        # fig, ax = plt.subplots(1, 4, figsize=(cutoff+3, 7))\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 14))\n",
    "        # fig, ax = plt.subplots(1, 2, figsize=(12, 14))\n",
    "\n",
    "\n",
    "        df_0 = pd.DataFrame(data_0, columns=[i for i in range(data_0.shape[1])])\n",
    "        df_0['Class'] = 0\n",
    "        df_1 = pd.DataFrame(data_1, columns=[i for i in range(data_1.shape[1])])\n",
    "        df_1['Class'] = 1\n",
    "        df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "        df = pd.melt(df, id_vars=['Class'], var_name='Component', value_name='Data')\n",
    "        sns.boxplot(df, y=\"Component\", x=\"Data\", hue=\"Class\", ax=ax[0], palette=[\"blue\", \"red\"], showfliers = False, orient='h', linewidth=0.3)\n",
    "        ax[0].grid(axis='y', linestyle='--', alpha=0.3, linewidth=0.3)\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[0].set_yticks(x, x, fontsize=6)\n",
    "        ax[0].set_ylim(-3, 55)\n",
    "        ax[0].set_title(\"Gradients\")\n",
    "\n",
    "        # test = stats.mannwhitneyu(np.abs(data_0), np.abs(data_1))\n",
    "        test = stats.mannwhitneyu(data_0, data_1)\n",
    "        pvals = test.pvalue\n",
    "        pvals = stats.false_discovery_control(pvals, method='by')\n",
    "        significant_p_vals = np.argwhere(pvals < 0.05)\n",
    "        # significant_p_vals = np.argwhere(pvals < 0.05 / pvals.shape[0])\n",
    "        # pvals = pvals / pvals.shape[0]\n",
    "        bars = ax[1].barh(\n",
    "            x,\n",
    "            pvals,\n",
    "            align=\"center\",\n",
    "            color='blue',\n",
    "        )\n",
    "        ax[1].set_yticks(x, [neuromark[k][0] if k in significant_p_vals else k for k in x], fontsize=6)\n",
    "        ax[1].set_xlabel(\"P-value\")\n",
    "        ax[1].set_xscale(\"log\")\n",
    "        ax[1].set_title(\"P-values\")\n",
    "\n",
    "        for i, bar in enumerate(bars):\n",
    "            if pvals[i] < 0.05:\n",
    "                bar.set_color('red')\n",
    "        # bar charts: summarizes gradients at each component\n",
    "\n",
    "        if len(grads[0].shape) == 3:\n",
    "            data_0 = np.median(grads[0], axis=(0, 1))\n",
    "            data_1 = np.median(grads[1], axis=(0, 1))\n",
    "        else:\n",
    "            data_0 = np.median(grads[0], axis=0)\n",
    "            data_1 = np.median(grads[1], axis=0)\n",
    "        \n",
    "        min_0, max_0 = np.min(data_0), np.max(data_0)\n",
    "        min_1, max_1 = np.min(data_1), np.max(data_1)\n",
    "        min_v, max_v = min(min_0, min_1), max(max_0, max_1)\n",
    "        if min_v <= 0:\n",
    "            max_v = max(abs(min_v), abs(max_v))\n",
    "            min_v = -1.0*max_v\n",
    "        else:\n",
    "            min_v = 0\n",
    "                \n",
    "        sym_data_0 = np.zeros(x.shape)\n",
    "        sym_data_1 = np.zeros(x.shape)\n",
    "        asym_data_0 = np.zeros(x.shape)\n",
    "        asym_data_1 = np.zeros(x.shape)\n",
    "\n",
    "        for j in x:\n",
    "            sym_data_0[j] = np.sign(data_0[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            sym_data_1[j] = np.sign(data_1[j]) * np.min((np.abs(data_0[j]), np.abs(data_1[j])))\n",
    "            asym_data_0[j] = data_0[j] - sym_data_0[j]\n",
    "            asym_data_1[j] = data_1[j] - sym_data_1[j]\n",
    "\n",
    "        abs_grad = np.abs(data_0) + np.abs(data_1)\n",
    "        abs_sym_grad = np.abs(sym_data_0) + np.abs(sym_data_1)\n",
    "        abs_asym_grad = np.abs(asym_data_0) + np.abs(asym_data_1)\n",
    "        significant_comp = np.argsort(abs_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_sym_comp = np.argsort(abs_sym_grad)[-int(percentile*x.shape[0]):]\n",
    "        significant_asym_comp = np.argsort(abs_asym_grad)[-int(percentile*x.shape[0]):]\n",
    "\n",
    "        all_significant = np.intersect1d(significant_comp, np.intersect1d(significant_sym_comp, significant_asym_comp))\n",
    "        all_significant = {\n",
    "            \"idx\": all_significant,\n",
    "            \"name\": [neuromark[k][0] for k in all_significant],\n",
    "            \"network\": [neuromark[k][1] for k in all_significant],\n",
    "            \"x\": [neuromark[k][2] for k in all_significant],\n",
    "            \"y\": [neuromark[k][3] for k in all_significant],\n",
    "            \"z\": [neuromark[k][4] for k in all_significant],\n",
    "        }\n",
    "        pd.DataFrame(all_significant).to_csv(f\"{filepath}_significant.csv\")\n",
    "\n",
    "\n",
    "        ax[2].barh(\n",
    "            x,\n",
    "            data_0,\n",
    "            align=\"center\",\n",
    "            color='blue',\n",
    "            label='0', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax[2].barh(\n",
    "            x,\n",
    "            data_1,\n",
    "            align=\"center\",\n",
    "            color='red',\n",
    "            label='1', \n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax[2].set_xlim(min_v, max_v)\n",
    "        ax[2].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax[2].set_yticks(x, [neuromark[k][0] if k in significant_comp else k for k in x], fontsize=6)\n",
    "        ax[2].set_title(\"Grad medians\")\n",
    "        # ax[2].set_xticks([], [])\n",
    "        \n",
    "        # ax[2].barh(\n",
    "        #     x,\n",
    "        #     sym_data_0,\n",
    "        #     align=\"center\",\n",
    "        #     color='blue',\n",
    "        #     label='0', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[2].barh(\n",
    "        #     x,\n",
    "        #     sym_data_1,\n",
    "        #     align=\"center\",\n",
    "        #     color='red',\n",
    "        #     label='1', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[2].set_xlim(min_v, max_v)\n",
    "        # ax[2].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        # ax[2].set_yticks(x, [neuromark[k][0] if k in significant_sym_comp else k for k in x], fontsize=6)\n",
    "        # # ax[2].set_xticks([], [])\n",
    "\n",
    "        # ax[3].barh(\n",
    "        #     x,\n",
    "        #     asym_data_0,\n",
    "        #     align=\"center\",\n",
    "        #     color='blue',\n",
    "        #     label='0', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[3].barh(\n",
    "        #     x,\n",
    "        #     asym_data_1,\n",
    "        #     align=\"center\",\n",
    "        #     color='red',\n",
    "        #     label='1', \n",
    "        #     alpha=0.7,\n",
    "        # )\n",
    "        # ax[3].set_xlim(min_v, max_v)\n",
    "        # ax[3].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        # ax[3].set_yticks(x, [neuromark[k][0] if k in significant_asym_comp else k for k in x], fontsize=6)\n",
    "        # # ax[3].set_xticks([], [])\n",
    "\n",
    "\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.png\",\n",
    "            format=\"png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        fig.savefig(\n",
    "            f\"{filepath}.svg\",\n",
    "            format=\"svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    \n",
    "    def get_grads(self, method, features, target):\n",
    "        \"\"\"Returns gradients according to method\"\"\"\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        if method == \"saliency\":\n",
    "            saliency = Saliency(self.model)\n",
    "            grads = saliency.attribute(features, target=target)\n",
    "        elif method == \"ig\":\n",
    "            ig = IntegratedGradients(self.model, multiply_by_inputs=False)\n",
    "            # ig = IntegratedGradients(self.model, multiply_by_inputs=True)\n",
    "            grads, _ = ig.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                baselines=torch.zeros_like(features),\n",
    "                return_convergence_delta=True,\n",
    "            )\n",
    "        elif method == \"ignt\":\n",
    "            ig = IntegratedGradients(self.model, multiply_by_inputs=False)\n",
    "            nt = NoiseTunnel(ig)\n",
    "            grads, _ = nt.attribute(\n",
    "                inputs=features,\n",
    "                target=target,\n",
    "                baselines=torch.zeros_like(features),\n",
    "                return_convergence_delta=True,\n",
    "                nt_type=\"smoothgrad_sq\",\n",
    "                nt_samples=5,\n",
    "                stdevs=0.2,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"'{method}' methods is not recognized\")\n",
    "\n",
    "        return grads\n",
    "\n",
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "# paths = [\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\"]\n",
    "paths = [\n",
    "    \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-oasis\",\n",
    "    \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\",\n",
    "    \"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-abide_869\"\n",
    "]\n",
    "# paths = [\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\"]\n",
    "methods = [\"ig\"]\n",
    "# methods = [\"ignt\"]\n",
    "# methods = [\"saliency\"]\n",
    "\n",
    "for path in paths:\n",
    "    model, data, _ = load_model_and_data(path)\n",
    "\n",
    "    dataset = data[\"test\"].dataset\n",
    "    features = [sample[0] for sample in dataset]\n",
    "    labels = [sample[1] for sample in dataset]\n",
    "\n",
    "    load_only_test = True\n",
    "    if not load_only_test:\n",
    "        dataset = data[\"test\"].dataset\n",
    "        for sample in dataset:\n",
    "            features.append(sample[0])\n",
    "            labels.append(sample[1])\n",
    "\n",
    "    features = torch.stack(features)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    introspector = Introspector(\n",
    "        model=model, \n",
    "        features=features, \n",
    "        labels=labels, \n",
    "        methods=methods, \n",
    "        save_path=f\"{path.replace('/logs/','/introspection/')}/\"\n",
    "    )\n",
    "    introspector.run(cutoff=10, percentile=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, checkpoint = load_model_and_data(\"./assets/logs/rerun_all2-exp-rearranged_mlp_defHP-fbirn\")\n",
    "\n",
    "dataset = data[\"test\"].dataset\n",
    "all_features = [sample[0] for sample in dataset]\n",
    "all_labels = [sample[1] for sample in dataset]\n",
    "all_features = torch.stack(all_features)\n",
    "all_labels = torch.stack(all_labels)\n",
    "\n",
    "final_weight = checkpoint[\"fc.4.weight\"].numpy()\n",
    "final_bias = checkpoint[\"fc.4.bias\"].numpy()\n",
    "starting_weight = checkpoint[\"fc.0.weight\"].numpy()\n",
    "starting_bias = checkpoint[\"fc.0.bias\"].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_array = all_labels == 0\n",
    "features = all_features[filter_array]\n",
    "features.requires_grad = False\n",
    "model.zero_grad()\n",
    "\n",
    "prediction = model(features[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = prediction[0][0]\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grad = model.fc[4].weight.grad.numpy()\n",
    "init_grad = model.fc[0].weight.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "cutoff = 10\n",
    "\n",
    "for target in range(2):\n",
    "    G = nx.Graph()\n",
    "    plt.figure(figsize=(20,4))\n",
    "    G.add_node(f\"Prediction {target}\", name=f\"Prediction {target}\", layer=2)\n",
    "\n",
    "    important_final = np.argsort(np.abs(final_grad[target]))[-cutoff:]\n",
    "    # print(final_weight.shape)\n",
    "    important_starting = []\n",
    "    for index in important_final:\n",
    "        G.add_node(f\"int {index}\", name=f\"int {index}\", layer=1)\n",
    "        G.add_edge(f\"Prediction {target}\", f\"int {index}\", weight=final_grad[target, index])\n",
    "\n",
    "        important_st = np.argsort(np.abs(init_grad[index]))[-cutoff:]\n",
    "        # for idx in important_st:\n",
    "        #     G.add_node(f\"IC {idx}\", name=f\"IC {idx}\", layer=0)\n",
    "        #     G.add_edge(f\"int {index}\", f\"IC {idx}\", weight=init_grad[index, idx])\n",
    "            \n",
    "        important_starting += [important_st]\n",
    "    \n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\", align='horizontal')\n",
    "    elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0]\n",
    "    esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=elarge, width=1, edge_color=\"r\")\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=esmall, width=1, edge_color=\"b\")\n",
    "    # node labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
    "    # edge weight labels\n",
    "    edge_labels = dict([((u,v,), f\"{d['weight']:.3f}\") for u,v,d in G.edges(data=True)])\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "    ax = plt.gca()\n",
    "    ax.margins(0.08)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.abs(final_weight[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "\n",
    "cutoff = 5\n",
    "\n",
    "for target in range(2):\n",
    "    G = nx.Graph()\n",
    "    plt.figure(figsize=(40,4))\n",
    "    G.add_node(f\"Prediction {target}\", name=f\"Prediction {target}\", layer=2)\n",
    "\n",
    "    important_final = np.argsort(np.abs(final_weight[target]))[-cutoff:]\n",
    "    # important_final = np.argsort(np.abs(final_weight[target]))\n",
    "    # print(final_weight.shape)\n",
    "    important_starting = []\n",
    "    for index in important_final:\n",
    "        G.add_node(f\"int {index}\", name=f\"int {index}\", layer=1)\n",
    "        G.add_edge(f\"Prediction {target}\", f\"int {index}\", weight=final_weight[target, index])\n",
    "\n",
    "        important_st = np.argsort(np.abs(starting_weight[index]))[-cutoff:]\n",
    "        for idx in important_st:\n",
    "            G.add_node(f\"IC {idx}\", name=f\"IC {idx}\", layer=0)\n",
    "            G.add_edge(f\"int {index}\", f\"IC {idx}\", weight=starting_weight[index, idx])\n",
    "            \n",
    "        important_starting += [important_st]\n",
    "    \n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\", align='horizontal')\n",
    "    elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0]\n",
    "    esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=elarge, width=1, edge_color=\"r\")\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=esmall, width=1, edge_color=\"b\")\n",
    "    # node labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
    "    # edge weight labels\n",
    "    edge_labels = dict([((u,v,), f\"{d['weight']:.3f}\") for u,v,d in G.edges(data=True)])\n",
    "    # nx.draw_networkx_edge_labels(G, pos, edge_labels, alpha=0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.margins(0.08)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.abs(starting_weight[0, important_starting[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(final_weight))\n",
    "np.min(np.abs(final_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = [\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-oasis\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "#     \"./assets/logs/rerun_all-exp-mlp_defHP-abide_869\"\n",
    "# ]\n",
    "# methods = [\"saliency\", \"ig\", \"ignt\"]\n",
    "paths = [\n",
    "    \"./assets/logs/rerun_all-exp-mlp_defHP-fbirn\",\n",
    "]\n",
    "methods = [\"ig\"]\n",
    "\n",
    "for path in paths:\n",
    "    model, data, _ = load_model_and_data(path)\n",
    "\n",
    "    dataset = data[\"test\"].dataset\n",
    "    features = [sample[0] for sample in dataset]\n",
    "    labels = [sample[1] for sample in dataset]\n",
    "\n",
    "    load_only_test = True\n",
    "    if not load_only_test:\n",
    "        dataset = data[\"test\"].dataset\n",
    "        for sample in dataset:\n",
    "            features.append(sample[0])\n",
    "            labels.append(sample[1])\n",
    "\n",
    "    features = torch.stack(features)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    introspector = Introspector(\n",
    "        model=model, \n",
    "        features=features, \n",
    "        labels=labels, \n",
    "        methods=methods, \n",
    "        save_path=f\"{path.replace('/logs/','/introspection/')}/\"\n",
    "    )\n",
    "    introspector.run(cutoff=10, percentile=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "method = \"ig\"\n",
    "grads_0 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_0_method_{method}.npy\")\n",
    "grads_1 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_1_method_{method}.npy\")\n",
    "print(grads_0.shape)\n",
    "print(grads_1.shape)\n",
    "\n",
    "\n",
    "target_0 = grads_0.reshape(-1, grads_0.shape[2])\n",
    "target_1 = grads_1.reshape(-1, grads_1.shape[2])\n",
    "\n",
    "print(target_0.shape)\n",
    "print(target_1.shape)\n",
    "\n",
    "test = stats.ttest_ind(target_0, target_1, equal_var=False)\n",
    "# test = stats.mannwhitneyu(target_0, target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"ig\"\n",
    "grads_0 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_0_method_{method}.npy\")\n",
    "grads_1 = np.load(f\"./assets/logs/rerun_all-exp-mlp_defHP-fbirn/introspection/grads_target_1_method_{method}.npy\")\n",
    "print(grads_0.shape)\n",
    "print(grads_1.shape)\n",
    "\n",
    "target_0 = grads_0.reshape(-1, grads_0.shape[2])\n",
    "target_1 = grads_1.reshape(-1, grads_1.shape[2])\n",
    "\n",
    "mean_A = np.mean(target_0, axis=0)\n",
    "mean_B = np.mean(target_1, axis=0)\n",
    "\n",
    "x = np.arange(53)\n",
    "\n",
    "plt.figure(figsize=(4, 7))\n",
    "plt.barh(\n",
    "    x,\n",
    "    mean_A,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    "    label='0', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.barh(\n",
    "    x,\n",
    "    mean_B,\n",
    "    align=\"center\",\n",
    "    color='red',\n",
    "    label='1', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-0.038, 0.038)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "abs_grad = np.abs(mean_A) + np.abs(mean_B)\n",
    "significatn_components = np.argsort(abs_grad)[-10:]\n",
    "plt.yticks(x, [neuromark[i][0] if i in significatn_components else i for i in x], fontsize=7) \n",
    "\n",
    "plt.savefig(\n",
    "    \"1_all.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = np.mean(target_0, axis=0).copy()\n",
    "B_1 = np.mean(target_1, axis=0).copy()\n",
    "\n",
    "for i in range(53):\n",
    "    A_1[i] = np.sign(mean_A[i]) * np.min((np.abs(mean_A[i]), np.abs(mean_B[i])))\n",
    "    B_1[i] = np.sign(mean_B[i]) * np.min((np.abs(mean_A[i]), np.abs(mean_B[i])))\n",
    "\n",
    "x = np.arange(53)\n",
    "\n",
    "plt.figure(figsize=(4, 7))\n",
    "plt.barh(\n",
    "    x,\n",
    "    A_1,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    "    label='0', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.barh(\n",
    "    x,\n",
    "    B_1,\n",
    "    align=\"center\",\n",
    "    color='red',\n",
    "    label='1', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-0.038, 0.038)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "abs_grad = np.abs(A_1) + np.abs(B_1)\n",
    "significatn_components = np.argsort(abs_grad)[-10:]\n",
    "plt.yticks(x, [neuromark[i][0] if i in significatn_components else i for i in x], fontsize=7) \n",
    "plt.savefig(\n",
    "    \"2_sym.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_2 = np.mean(target_0, axis=0).copy()\n",
    "B_2 = np.mean(target_1, axis=0).copy()\n",
    "\n",
    "for i in range(53):\n",
    "    A_2[i] = mean_A[i] - A_1[i]\n",
    "    B_2[i] = mean_B[i] - B_1[i]\n",
    "\n",
    "x = np.arange(53)\n",
    "\n",
    "plt.figure(figsize=(4, 7))\n",
    "plt.barh(\n",
    "    x,\n",
    "    A_2,\n",
    "    align=\"center\",\n",
    "    color='blue',\n",
    "    label='0', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.barh(\n",
    "    x,\n",
    "    B_2,\n",
    "    align=\"center\",\n",
    "    color='red',\n",
    "    label='1', \n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.xlim(-0.038, 0.038)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "abs_grad = np.abs(A_2) + np.abs(B_2)\n",
    "significatn_components = np.argsort(abs_grad)[-10:]\n",
    "plt.yticks(x, [neuromark[i][0] if i in significatn_components else i for i in x], fontsize=7) \n",
    "plt.savefig(\n",
    "    \"3_asym.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_0, B = target_0.shape\n",
    "A_1, B = target_1.shape\n",
    "# Flatten the arrays to 1D arrays\n",
    "target_0_flat = target_0.flatten()\n",
    "target_1_flat = target_1.flatten()\n",
    "target_flat = np.concatenate([target_0_flat, target_1_flat])\n",
    "\n",
    "# Create arrays for the indicators\n",
    "component_array_0 = np.repeat(np.arange(B), A_0)\n",
    "component_array_1 = np.repeat(np.arange(B), A_1)\n",
    "component_array = np.concatenate([component_array_0, component_array_1])\n",
    "target_array_0 = np.array([0]*(A_0*B))\n",
    "target_array_1 = np.array([1]*(A_1*B))\n",
    "target_array = np.concatenate([target_array_0, target_array_1])\n",
    "\n",
    "print(target_flat.shape)\n",
    "print(component_array.shape)\n",
    "print(target_array.shape)\n",
    "# Create the Pandas DataFrame\n",
    "df = pd.DataFrame({'Data': target_flat,\n",
    "                   'Component': component_array,\n",
    "                   'Target': target_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "fig, ax = plt.subplots(1, 1, figsize=(25, 5))\n",
    "\n",
    "sns.boxplot(df, x=\"Component\", y=\"Data\", hue=\"Target\", showfliers = False, ax=ax)\n",
    "\n",
    "box_pairs = [((i, 0), (i, 1)) for i in range(53)]\n",
    "plt.savefig(\"hi.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = test.statistic\n",
    "plt.bar(\n",
    "    range(pval.shape[0]),\n",
    "    pval,\n",
    "    align=\"center\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.xticks(list(range(0, pval.shape[0], 2)))\n",
    "plt.grid()\n",
    "plt.xlim([0, pval.shape[0]])\n",
    "plt.title(\"T statistics\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = test.pvalue\n",
    "pval = np.log(pval)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "bars = ax.bar(\n",
    "    range(pval.shape[0]),\n",
    "    pval,\n",
    "    align=\"center\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax.set_xticks(list(range(0, pval.shape[0], 2)))\n",
    "ax.grid()\n",
    "ax.set_xlim([0, pval.shape[0]])\n",
    "# ax.set_ylim([0, 0.05])\n",
    "ax.set_title(\"log(p-value)\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "significatn_components = np.where(pval<-40)\n",
    "print(significatn_components[0])\n",
    "print(\"Significatn components (log(p_val) < -40)):\")\n",
    "for i in significatn_components[0]:\n",
    "    print(neuromark[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    \n",
    "    # def run(self, cutoff=1, percentile=0.1):\n",
    "    #     \"\"\"Run introspection, save results\"\"\"\n",
    "    #     targets = torch.unique(self.labels)\n",
    "    #     for target in targets:\n",
    "\n",
    "    #         print(f\"Getting MLP predictions for target {target}\")  \n",
    "    #         filter_array = self.labels == target\n",
    "    #         features = self.features[filter_array]\n",
    "    #         features.requires_grad = False\n",
    "\n",
    "    #         timeseries, predictions = self.model(features, introspection=True)\n",
    "    #         timeseries = timeseries.cpu().detach().numpy()\n",
    "    #         predictions = predictions.cpu().detach().numpy()\n",
    "\n",
    "    #         x = np.arange(timeseries.shape[1])\n",
    "    #         timeseries_0 = timeseries[:, :, 0]\n",
    "    #         timeseries_1 = timeseries[:, :, 1]\n",
    "\n",
    "    #         np.save(f\"{self.save_path}timeseries_target_{target}.npy\", timeseries)\n",
    "    #         window_size = 10\n",
    "\n",
    "    #         fig, ax = plt.subplots(1, cutoff,figsize=(3*cutoff, 2))\n",
    "    #         for i in range(cutoff):\n",
    "    #             smoothed_data_0 = np.convolve(timeseries_0[i], np.ones(window_size)/window_size, mode='same')\n",
    "    #             smoothed_data_1 = np.convolve(timeseries_1[i], np.ones(window_size)/window_size, mode='same')\n",
    "                \n",
    "    #             smooth_line_0 = ax[i].plot(x, smoothed_data_0, label='0', color='blue')\n",
    "    #             smooth_line_1 = ax[i].plot(x, smoothed_data_1, label='1', color='red')\n",
    "    #             line_0 = ax[i].plot(x, timeseries_0[i], '#00009922')\n",
    "    #             line_1 = ax[i].plot(x, timeseries_1[i], '#99000022')\n",
    "\n",
    "    #             # ax[i].set_title(f\"True target = {target}, Predicted target = {predictions[i]}\")\n",
    "    #             ax[i].legend(title=\"Prediction\")\n",
    "                \n",
    "    #             ax[i].set_xticklabels([])\n",
    "    #             ax[i].set_xticks([])\n",
    "    #             ax[i].set_yticklabels([])\n",
    "    #             ax[i].set_yticks([])\n",
    "    #             ax[i].set_xlabel(\"Time\")\n",
    "    #             ax[i].set_ylabel(\"Prediction strength\" if i == 0 else \"\")\n",
    "\n",
    "    #         fig.savefig(\n",
    "    #             f\"{self.save_path}timeseries/prediction_target_{target}.png\",\n",
    "    #             format=\"png\",\n",
    "    #             dpi=300,\n",
    "    #             bbox_inches=\"tight\",\n",
    "    #         )\n",
    "    #         fig.savefig(\n",
    "    #             f\"{self.save_path}timeseries/prediction_target_{target}.svg\",\n",
    "    #             format=\"svg\",\n",
    "    #             bbox_inches=\"tight\",\n",
    "    #         )\n",
    "\n",
    "            \n",
    "    #         for method in self.methods:\n",
    "    #             filter_array = self.labels == target\n",
    "    #             features = self.features[filter_array]\n",
    "    #             features.requires_grad = True\n",
    "\n",
    "    #             grads = self.get_grads(method, features, target)\n",
    "    #             grads = grads.cpu().detach().numpy()\n",
    "\n",
    "    #             np.save(f\"{self.save_path}grads_target_{target}_method_{method}.npy\", grads)\n",
    "\n",
    "    #             smoothed_timeseries = np.array([np.convolve(signal[:, target], np.ones(9)/9, mode='same') for signal in timeseries])\n",
    "    #             smooth_grads = np.log(grads) * smoothed_timeseries[:, :][:, :, np.newaxis]\n",
    "    #             detached_features = features.cpu().detach().numpy()\n",
    "\n",
    "    #             # np.save(f\"{self.save_path}smooth_grads_target_{target}.npy\", grads)\n",
    "                \n",
    "    #             print(f\"\\tPlotting generalized saliency maps using '{method}' with target {target}\")\n",
    "    #             self.plot_colormaps(\n",
    "    #                 grads,\n",
    "    #                 detached_features, \n",
    "    #                 filepath=f\"{self.save_path}{method}/colormap/general_{target}.png\",\n",
    "    #                 color=\"blue\" if target == 0 else \"red\"\n",
    "    #             )\n",
    "\n",
    "    #             self.plot_histograms(\n",
    "    #                 grads, \n",
    "    #                 filepath=f\"{self.save_path}{method}/barchart/general_{target}.png\",\n",
    "    #                 percentile=percentile\n",
    "    #             )\n",
    "\n",
    "    #             print(f\"\\tPlotting single sample saliency maps using '{method}' with target {target}\")\n",
    "    #             for i in range(cutoff):\n",
    "    #                 feature = detached_features[i][np.newaxis, :, :]\n",
    "    #                 grad = grads[i][np.newaxis, :, :]\n",
    "\n",
    "    #                 self.plot_colormaps(\n",
    "    #                     grad,\n",
    "    #                     feature, \n",
    "    #                     filepath=f\"{self.save_path}{method}/colormap/target_{target}_idx_{i:04d}.png\",\n",
    "    #                     color=\"blue\" if target == 0 else \"red\"\n",
    "    #                 )\n",
    "\n",
    "    #                 self.plot_histograms(\n",
    "    #                     grad, \n",
    "    #                     filepath=f\"{self.save_path}{method}/barchart/target_{target}_idx_{i:04d}.png\",\n",
    "    #                     percentile=percentile\n",
    "    #                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
